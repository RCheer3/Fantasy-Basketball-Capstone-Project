{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "import psycopg2 as pg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from Player_rank import Player_ranker\n",
    "from sqlalchemy import create_engine\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nHere we're loading in all of the traditional Fantasy basketball stats into SQL, using window function to get a ranking by minutes\\n\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Here we're loading in all of the traditional Fantasy basketball stats into SQL, using window function to get a ranking by minutes\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"IF you have open connections run the following in the psql command prompt:\\n\\nSELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname = 'nba_capstone';\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''IF you have open connections run the following in the psql command prompt:\n",
    "\n",
    "SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname = 'nba_capstone';\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pg2.connect(dbname = 'postgres',host='localhost')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.autocommit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Note for some reason you can not access the database if it is not all lowercase'''\n",
    "\n",
    "cur.execute('DROP DATABASE IF EXISTS nba_capstone;')  # Makes sure there is not already a class_example database and removes is if there is\n",
    "cur.execute('CREATE DATABASE nba_capstone;')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pg2.connect(dbname = 'nba_capstone',host='localhost')\n",
    "conn.autocommit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "        CREATE TABLE NBA_stats (\n",
    "            Season integer, \n",
    "            Player varchar(50), \n",
    "            Pos varchar(10),\n",
    "            Age int,\n",
    "            Tm varchar(15),\n",
    "            G int,\n",
    "            GS int,\n",
    "            MP float,\n",
    "            FG float,\n",
    "            FGA float,\n",
    "            FG_Percentage float,\n",
    "            Threes_Made float,\n",
    "            Threes_Attempted float,\n",
    "            Three_Percentage float,\n",
    "            Twos_Made float,\n",
    "            Twos_Attempted float,\n",
    "            Twos_Percentage float,\n",
    "            eff_FG_Percentage float,\n",
    "            FTM float,\n",
    "            FTA float,\n",
    "            FT_Percentage float,\n",
    "            ORB float,\n",
    "            DRB float,\n",
    "            Rebounds float,\n",
    "            AST float,\n",
    "            STL float,\n",
    "            BLK float,\n",
    "            TOV float,\n",
    "            Fouls float,\n",
    "            Points float\n",
    "        );\n",
    "        '''\n",
    "\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# current_directory_path = os.getcwd()\n",
    "# current_directory_path\n",
    "\n",
    "query = '''\n",
    "        COPY NBA_stats \n",
    "        FROM '/Users/rcheer/Desktop/Galvanize/Capstone/Fantasy-Basketball-Capstone-Project/NBA stats.csv' \n",
    "        DELIMITER ',' \n",
    "        CSV HEADER;\n",
    "        '''\n",
    "\n",
    "\n",
    "\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "        CREATE TABLE nba_advanced (\n",
    "            Season integer, \n",
    "            Player varchar(50), \n",
    "            Pos varchar(10),\n",
    "            Age int,\n",
    "            Tm varchar(15),\n",
    "            G int,\n",
    "            total_MP float,\n",
    "            PER float,\n",
    "            True_Shooting float,\n",
    "            Three_Attempt_Rate float,\n",
    "            FT_rate float,\n",
    "            ORB_Percentage float,\n",
    "            DRB_Percentage float,\n",
    "            Rebound_Percentage float,\n",
    "            Assist_Percentage float,\n",
    "            Steal_Percentage float,\n",
    "            Block_Percentage float,\n",
    "            Turnover_Percentage float,\n",
    "            Usage_Percentage float,\n",
    "            Offensive_WinShares float,\n",
    "            Defensive_WinShares float,\n",
    "            WinShares float,\n",
    "            WinShares_Per48 float,\n",
    "            Offensive_BoxPlusMinus float,\n",
    "            Defensive_BoxPlusMinus float,\n",
    "            BoxPlusMinus float,\n",
    "            Value_overReplacement float\n",
    "        );\n",
    "        '''\n",
    "\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "        COPY nba_advanced \n",
    "        FROM '/Users/rcheer/Desktop/Galvanize/Capstone/Fantasy-Basketball-Capstone-Project/NBA Advanced.csv' \n",
    "        DELIMITER ',' \n",
    "        CSV HEADER;\n",
    "        '''\n",
    "\n",
    "\n",
    "\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "save for later\n",
    "d.points,d.rebounds,d.ast,d.stl,d.blk,d.tov,d.fg_percentage,d.FT_percentage\n",
    "'''\n",
    "\n",
    "\n",
    "query1 = '''\n",
    "            update nba_stats set Tm = 'NOP' where Tm = 'NOH';\n",
    "            update nba_advanced set Tm = 'NOP' where Tm = 'NOH';\n",
    "            update nba_stats set Tm = 'CHA' where Tm = 'CHO';\n",
    "            update nba_advanced set Tm = 'CHA' where Tm = 'CHO';\n",
    "            \n",
    "            DROP TABLE IF EXISTS players;\n",
    "            CREATE TABLE players AS\n",
    "            select season,player,max(G) as Games from NBA_stats where Tm!='TOT' group by season,player;\n",
    "            \n",
    "            DROP TABLE IF EXISTS y_predictions;\n",
    "            CREATE TABLE y_predictions AS\n",
    "            select d.season,d.player,d.pos,d.age,MAX(case when p.player is not null then d.Tm else NULL end) as StartingTeam,SUM(G) as Games,SUM(GS) as GS,\n",
    "            max(MP) as minutes\n",
    "            from NBA_stats d\n",
    "            left join players p\n",
    "                on d.season = p.season\n",
    "                and d.player = p.player\n",
    "                and d.G = p.Games\n",
    "            where d.Tm!='TOT'\n",
    "            group by d.season,d.player,d.pos,d.age;\n",
    "            \n",
    "            update y_predictions set StartingTeam = 'NOP' where startingTeam = 'NOH';\n",
    "            update y_predictions set StartingTeam = 'CHA' where startingTeam = 'CHO';\n",
    "            \n",
    "            DROP TABLE IF EXISTS rank_by_minutes;\n",
    "            CREATE TABLE rank_by_minutes AS\n",
    "            select y.*,n.points,n.rebounds,n.ast,n.stl,n.blk,n.tov,n.threes_made,n.fg,n.fga,n.ftm,n.fta,\n",
    "            case when cast(y.GS as float)/y.Games >0.6 then 1 else 0 end as starter,\n",
    "            row_number() over(partition by n.season order by MP*G desc) as min_rank from NBA_stats n\n",
    "            inner join y_predictions y\n",
    "                ON n.player = y.player\n",
    "                and n.season = y.season\n",
    "                and n.Tm=y.startingTeam;\n",
    "            \n",
    "            select * from rank_by_minutes        \n",
    "                \n",
    "        '''\n",
    "cur.execute(query1)\n",
    "data = cur.fetchall()\n",
    "df = pd.DataFrame(np.array(data))\n",
    "df.columns = ['season','player','position','age','team','gamesPlayed','gamesStarted','minutes'\n",
    "              ,'points','rebounds','assists','steals','blocks','turnovers','threes_made','FGM','FGA','FTM','FTA','starter','min_rank']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['min_rank']=pd.to_numeric(df['min_rank'])\n",
    "df['points']=pd.to_numeric(df['points'])\n",
    "df['rebounds']=pd.to_numeric(df['rebounds'])\n",
    "df['assists']=pd.to_numeric(df['assists'])\n",
    "df['steals']=pd.to_numeric(df['steals'])\n",
    "df['blocks']=pd.to_numeric(df['blocks'])\n",
    "df['turnovers']=pd.to_numeric(df['turnovers'])\n",
    "df['threes_made']=pd.to_numeric(df['threes_made'])\n",
    "df['FGM']=pd.to_numeric(df['FGM'])\n",
    "df['FGA']=pd.to_numeric(df['FGA'])\n",
    "df['FTM']=pd.to_numeric(df['FTM'])\n",
    "df['FTA']=pd.to_numeric(df['FTA'])\n",
    "df['gamesPlayed']=pd.to_numeric(df['gamesPlayed'])\n",
    "df['gamesStarted']=pd.to_numeric(df['gamesStarted'])\n",
    "df['minutes']=pd.to_numeric(df['minutes'])\n",
    "df['age']=pd.to_numeric(df['age'])\n",
    "df['season']=pd.to_numeric(df['season'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>mean_points</th>\n",
       "      <th>mean_rebounds</th>\n",
       "      <th>mean_assists</th>\n",
       "      <th>mean_steals</th>\n",
       "      <th>mean_blocks</th>\n",
       "      <th>mean_turnovers</th>\n",
       "      <th>mean_threes_made</th>\n",
       "      <th>mean_fgm</th>\n",
       "      <th>mean_fga</th>\n",
       "      <th>mean_ftm</th>\n",
       "      <th>mean_fta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008</td>\n",
       "      <td>13.3005</td>\n",
       "      <td>5.1235</td>\n",
       "      <td>2.7870</td>\n",
       "      <td>0.9150</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>1.7035</td>\n",
       "      <td>0.8780</td>\n",
       "      <td>4.9095</td>\n",
       "      <td>10.5850</td>\n",
       "      <td>2.5990</td>\n",
       "      <td>3.3115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009</td>\n",
       "      <td>12.9850</td>\n",
       "      <td>5.1080</td>\n",
       "      <td>2.8490</td>\n",
       "      <td>0.9120</td>\n",
       "      <td>0.5600</td>\n",
       "      <td>1.6965</td>\n",
       "      <td>0.8265</td>\n",
       "      <td>4.8530</td>\n",
       "      <td>10.4370</td>\n",
       "      <td>2.4600</td>\n",
       "      <td>3.1825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010</td>\n",
       "      <td>12.9225</td>\n",
       "      <td>5.0185</td>\n",
       "      <td>2.8135</td>\n",
       "      <td>0.9085</td>\n",
       "      <td>0.5685</td>\n",
       "      <td>1.6790</td>\n",
       "      <td>0.8380</td>\n",
       "      <td>4.7955</td>\n",
       "      <td>10.3525</td>\n",
       "      <td>2.4955</td>\n",
       "      <td>3.2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011</td>\n",
       "      <td>12.1830</td>\n",
       "      <td>4.8915</td>\n",
       "      <td>2.6840</td>\n",
       "      <td>0.9225</td>\n",
       "      <td>0.5725</td>\n",
       "      <td>1.6805</td>\n",
       "      <td>0.8030</td>\n",
       "      <td>4.5925</td>\n",
       "      <td>10.1180</td>\n",
       "      <td>2.1955</td>\n",
       "      <td>2.8590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012</td>\n",
       "      <td>12.5550</td>\n",
       "      <td>5.0520</td>\n",
       "      <td>2.8840</td>\n",
       "      <td>0.9585</td>\n",
       "      <td>0.6125</td>\n",
       "      <td>1.7410</td>\n",
       "      <td>0.8800</td>\n",
       "      <td>4.7285</td>\n",
       "      <td>10.3275</td>\n",
       "      <td>2.2150</td>\n",
       "      <td>2.8920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2013</td>\n",
       "      <td>13.0510</td>\n",
       "      <td>5.1265</td>\n",
       "      <td>2.8310</td>\n",
       "      <td>0.9505</td>\n",
       "      <td>0.5420</td>\n",
       "      <td>1.7375</td>\n",
       "      <td>0.9715</td>\n",
       "      <td>4.8545</td>\n",
       "      <td>10.5425</td>\n",
       "      <td>2.3790</td>\n",
       "      <td>3.1020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2014</td>\n",
       "      <td>12.3805</td>\n",
       "      <td>4.9890</td>\n",
       "      <td>2.7145</td>\n",
       "      <td>0.9230</td>\n",
       "      <td>0.5390</td>\n",
       "      <td>1.6315</td>\n",
       "      <td>0.9600</td>\n",
       "      <td>4.6260</td>\n",
       "      <td>10.1940</td>\n",
       "      <td>2.1725</td>\n",
       "      <td>2.8505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2015</td>\n",
       "      <td>12.8715</td>\n",
       "      <td>5.1935</td>\n",
       "      <td>2.7720</td>\n",
       "      <td>0.9530</td>\n",
       "      <td>0.5900</td>\n",
       "      <td>1.6785</td>\n",
       "      <td>1.0530</td>\n",
       "      <td>4.7645</td>\n",
       "      <td>10.4595</td>\n",
       "      <td>2.2840</td>\n",
       "      <td>2.9900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016</td>\n",
       "      <td>13.3550</td>\n",
       "      <td>5.0420</td>\n",
       "      <td>2.8950</td>\n",
       "      <td>0.9230</td>\n",
       "      <td>0.5340</td>\n",
       "      <td>1.6195</td>\n",
       "      <td>1.2170</td>\n",
       "      <td>4.8980</td>\n",
       "      <td>10.6355</td>\n",
       "      <td>2.3385</td>\n",
       "      <td>2.9850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017</td>\n",
       "      <td>13.2915</td>\n",
       "      <td>5.0950</td>\n",
       "      <td>2.9355</td>\n",
       "      <td>0.9240</td>\n",
       "      <td>0.5585</td>\n",
       "      <td>1.6825</td>\n",
       "      <td>1.3200</td>\n",
       "      <td>4.9175</td>\n",
       "      <td>10.6135</td>\n",
       "      <td>2.1395</td>\n",
       "      <td>2.7465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   season  mean_points  mean_rebounds  mean_assists  mean_steals  mean_blocks  \\\n",
       "0    2008      13.3005         5.1235        2.7870       0.9150       0.5690   \n",
       "1    2009      12.9850         5.1080        2.8490       0.9120       0.5600   \n",
       "2    2010      12.9225         5.0185        2.8135       0.9085       0.5685   \n",
       "3    2011      12.1830         4.8915        2.6840       0.9225       0.5725   \n",
       "4    2012      12.5550         5.0520        2.8840       0.9585       0.6125   \n",
       "5    2013      13.0510         5.1265        2.8310       0.9505       0.5420   \n",
       "6    2014      12.3805         4.9890        2.7145       0.9230       0.5390   \n",
       "7    2015      12.8715         5.1935        2.7720       0.9530       0.5900   \n",
       "8    2016      13.3550         5.0420        2.8950       0.9230       0.5340   \n",
       "9    2017      13.2915         5.0950        2.9355       0.9240       0.5585   \n",
       "\n",
       "   mean_turnovers  mean_threes_made  mean_fgm  mean_fga  mean_ftm  mean_fta  \n",
       "0          1.7035            0.8780    4.9095   10.5850    2.5990    3.3115  \n",
       "1          1.6965            0.8265    4.8530   10.4370    2.4600    3.1825  \n",
       "2          1.6790            0.8380    4.7955   10.3525    2.4955    3.2005  \n",
       "3          1.6805            0.8030    4.5925   10.1180    2.1955    2.8590  \n",
       "4          1.7410            0.8800    4.7285   10.3275    2.2150    2.8920  \n",
       "5          1.7375            0.9715    4.8545   10.5425    2.3790    3.1020  \n",
       "6          1.6315            0.9600    4.6260   10.1940    2.1725    2.8505  \n",
       "7          1.6785            1.0530    4.7645   10.4595    2.2840    2.9900  \n",
       "8          1.6195            1.2170    4.8980   10.6355    2.3385    2.9850  \n",
       "9          1.6825            1.3200    4.9175   10.6135    2.1395    2.7465  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Player_ranker(df)\n",
    "test.get_category_dist()\n",
    "test.cat_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.assign_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_copy = test.value.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>player</th>\n",
       "      <th>value_tot</th>\n",
       "      <th>value_points</th>\n",
       "      <th>value_rebounds</th>\n",
       "      <th>value_assists</th>\n",
       "      <th>value_blocks</th>\n",
       "      <th>value_steals</th>\n",
       "      <th>value_turnovers</th>\n",
       "      <th>value_threes</th>\n",
       "      <th>value_fg</th>\n",
       "      <th>value_ft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008</td>\n",
       "      <td>Andre Iguodala</td>\n",
       "      <td>1.569149</td>\n",
       "      <td>1.022902</td>\n",
       "      <td>0.233549</td>\n",
       "      <td>1.209232</td>\n",
       "      <td>-0.313073</td>\n",
       "      <td>1.720216</td>\n",
       "      <td>-1.392654</td>\n",
       "      <td>0.164932</td>\n",
       "      <td>0.201353</td>\n",
       "      <td>-1.277308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008</td>\n",
       "      <td>Joe Johnson</td>\n",
       "      <td>2.248938</td>\n",
       "      <td>1.506499</td>\n",
       "      <td>-0.293101</td>\n",
       "      <td>1.449827</td>\n",
       "      <td>-0.683573</td>\n",
       "      <td>0.464584</td>\n",
       "      <td>-1.113145</td>\n",
       "      <td>1.381647</td>\n",
       "      <td>-1.036752</td>\n",
       "      <td>0.572951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008</td>\n",
       "      <td>O.J. Mayo</td>\n",
       "      <td>0.491912</td>\n",
       "      <td>0.967102</td>\n",
       "      <td>-0.536170</td>\n",
       "      <td>0.198732</td>\n",
       "      <td>-0.683573</td>\n",
       "      <td>0.464584</td>\n",
       "      <td>-1.532409</td>\n",
       "      <td>1.246456</td>\n",
       "      <td>-0.633993</td>\n",
       "      <td>1.001183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008</td>\n",
       "      <td>Antawn Jamison</td>\n",
       "      <td>3.459954</td>\n",
       "      <td>1.655298</td>\n",
       "      <td>1.529918</td>\n",
       "      <td>-0.426816</td>\n",
       "      <td>-0.498323</td>\n",
       "      <td>0.715710</td>\n",
       "      <td>0.284401</td>\n",
       "      <td>0.705694</td>\n",
       "      <td>0.083254</td>\n",
       "      <td>-0.589183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008</td>\n",
       "      <td>Raymond Felton</td>\n",
       "      <td>-0.135656</td>\n",
       "      <td>0.167306</td>\n",
       "      <td>-0.536170</td>\n",
       "      <td>1.882899</td>\n",
       "      <td>-0.313073</td>\n",
       "      <td>1.469090</td>\n",
       "      <td>-1.532409</td>\n",
       "      <td>-0.240639</td>\n",
       "      <td>-1.364914</td>\n",
       "      <td>0.332253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   season          player  value_tot  value_points  value_rebounds  \\\n",
       "0    2008  Andre Iguodala   1.569149      1.022902        0.233549   \n",
       "1    2008     Joe Johnson   2.248938      1.506499       -0.293101   \n",
       "2    2008       O.J. Mayo   0.491912      0.967102       -0.536170   \n",
       "3    2008  Antawn Jamison   3.459954      1.655298        1.529918   \n",
       "4    2008  Raymond Felton  -0.135656      0.167306       -0.536170   \n",
       "\n",
       "   value_assists  value_blocks  value_steals  value_turnovers  value_threes  \\\n",
       "0       1.209232     -0.313073      1.720216        -1.392654      0.164932   \n",
       "1       1.449827     -0.683573      0.464584        -1.113145      1.381647   \n",
       "2       0.198732     -0.683573      0.464584        -1.532409      1.246456   \n",
       "3      -0.426816     -0.498323      0.715710         0.284401      0.705694   \n",
       "4       1.882899     -0.313073      1.469090        -1.532409     -0.240639   \n",
       "\n",
       "   value_fg  value_ft  \n",
       "0  0.201353 -1.277308  \n",
       "1 -1.036752  0.572951  \n",
       "2 -0.633993  1.001183  \n",
       "3  0.083254 -0.589183  \n",
       "4 -1.364914  0.332253  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy value data back into SQL\n",
    "engine = create_engine(\"postgresql://@localhost/nba_capstone\")\n",
    "\n",
    "value_copy.to_sql(name='value', con=engine, if_exists = 'replace', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "        DROP TABLE IF EXISTS player_value;\n",
    "        CREATE TABLE player_value AS\n",
    "        select ROW_NUMBER() OVER(PARTITION BY season ORDER BY value_tot DESC),* from value;\n",
    "        \n",
    "        DROP TABLE IF EXISTS value;\n",
    "        \n",
    "        select * from player_value;\n",
    "        \n",
    "        '''\n",
    "\n",
    "\n",
    "cur.execute(query)\n",
    "ranking_data = cur.fetchall()\n",
    "df_2 = pd.DataFrame(np.array(ranking_data))\n",
    "cols_value = ['playerrank']\n",
    "for item in (list(value_copy.columns)):\n",
    "    cols_value.append(item)\n",
    "cols_value\n",
    "df_2.columns=cols_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_2.columns:\n",
    "    if i!='player':\n",
    "        df_2[i]=pd.to_numeric(df_2[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_2.columns:\n",
    "    if i!='player':\n",
    "        df_2[i]=pd.to_numeric(df_2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playerrank</th>\n",
       "      <th>season</th>\n",
       "      <th>player</th>\n",
       "      <th>value_tot</th>\n",
       "      <th>value_points</th>\n",
       "      <th>value_rebounds</th>\n",
       "      <th>value_assists</th>\n",
       "      <th>value_blocks</th>\n",
       "      <th>value_steals</th>\n",
       "      <th>value_turnovers</th>\n",
       "      <th>value_threes</th>\n",
       "      <th>value_fg</th>\n",
       "      <th>value_ft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2008</td>\n",
       "      <td>Chris Paul</td>\n",
       "      <td>10.649584</td>\n",
       "      <td>1.766898</td>\n",
       "      <td>0.152526</td>\n",
       "      <td>3.952019</td>\n",
       "      <td>-0.868823</td>\n",
       "      <td>4.733733</td>\n",
       "      <td>-1.811918</td>\n",
       "      <td>-0.105449</td>\n",
       "      <td>1.195183</td>\n",
       "      <td>1.635414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>LeBron James</td>\n",
       "      <td>8.708530</td>\n",
       "      <td>2.808492</td>\n",
       "      <td>1.003268</td>\n",
       "      <td>2.123494</td>\n",
       "      <td>0.983678</td>\n",
       "      <td>1.971343</td>\n",
       "      <td>-1.811918</td>\n",
       "      <td>0.976075</td>\n",
       "      <td>0.888138</td>\n",
       "      <td>-0.234041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2008</td>\n",
       "      <td>Dwyane Wade</td>\n",
       "      <td>8.419529</td>\n",
       "      <td>3.143291</td>\n",
       "      <td>-0.050032</td>\n",
       "      <td>2.267852</td>\n",
       "      <td>1.354178</td>\n",
       "      <td>3.226975</td>\n",
       "      <td>-2.370936</td>\n",
       "      <td>0.300123</td>\n",
       "      <td>1.126183</td>\n",
       "      <td>-0.578103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2008</td>\n",
       "      <td>Danny Granger</td>\n",
       "      <td>6.463654</td>\n",
       "      <td>2.324895</td>\n",
       "      <td>-0.009520</td>\n",
       "      <td>-0.041864</td>\n",
       "      <td>1.539428</td>\n",
       "      <td>0.213457</td>\n",
       "      <td>-1.113145</td>\n",
       "      <td>2.463170</td>\n",
       "      <td>-0.678128</td>\n",
       "      <td>1.765360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2008</td>\n",
       "      <td>Dirk Nowitzki</td>\n",
       "      <td>6.094625</td>\n",
       "      <td>2.343495</td>\n",
       "      <td>1.327360</td>\n",
       "      <td>-0.186221</td>\n",
       "      <td>0.427928</td>\n",
       "      <td>-0.288795</td>\n",
       "      <td>-0.274618</td>\n",
       "      <td>-0.105449</td>\n",
       "      <td>0.611555</td>\n",
       "      <td>2.239370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   playerrank  season         player  value_tot  value_points  value_rebounds  \\\n",
       "0           1    2008     Chris Paul  10.649584      1.766898        0.152526   \n",
       "1           2    2008   LeBron James   8.708530      2.808492        1.003268   \n",
       "2           3    2008    Dwyane Wade   8.419529      3.143291       -0.050032   \n",
       "3           4    2008  Danny Granger   6.463654      2.324895       -0.009520   \n",
       "4           5    2008  Dirk Nowitzki   6.094625      2.343495        1.327360   \n",
       "\n",
       "   value_assists  value_blocks  value_steals  value_turnovers  value_threes  \\\n",
       "0       3.952019     -0.868823      4.733733        -1.811918     -0.105449   \n",
       "1       2.123494      0.983678      1.971343        -1.811918      0.976075   \n",
       "2       2.267852      1.354178      3.226975        -2.370936      0.300123   \n",
       "3      -0.041864      1.539428      0.213457        -1.113145      2.463170   \n",
       "4      -0.186221      0.427928     -0.288795        -0.274618     -0.105449   \n",
       "\n",
       "   value_fg  value_ft  \n",
       "0  1.195183  1.635414  \n",
       "1  0.888138 -0.234041  \n",
       "2  1.126183 -0.578103  \n",
       "3 -0.678128  1.765360  \n",
       "4  0.611555  2.239370  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playerrank</th>\n",
       "      <th>season</th>\n",
       "      <th>player</th>\n",
       "      <th>value_tot</th>\n",
       "      <th>value_points</th>\n",
       "      <th>value_rebounds</th>\n",
       "      <th>value_assists</th>\n",
       "      <th>value_blocks</th>\n",
       "      <th>value_steals</th>\n",
       "      <th>value_turnovers</th>\n",
       "      <th>value_threes</th>\n",
       "      <th>value_fg</th>\n",
       "      <th>value_ft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4219</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>Anthony Davis</td>\n",
       "      <td>13.069292</td>\n",
       "      <td>2.728406</td>\n",
       "      <td>2.327843</td>\n",
       "      <td>-0.323373</td>\n",
       "      <td>4.382346</td>\n",
       "      <td>1.403769</td>\n",
       "      <td>-0.642579</td>\n",
       "      <td>-0.719758</td>\n",
       "      <td>2.572502</td>\n",
       "      <td>1.340136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4220</th>\n",
       "      <td>2</td>\n",
       "      <td>2017</td>\n",
       "      <td>Stephen Curry</td>\n",
       "      <td>10.977767</td>\n",
       "      <td>2.415188</td>\n",
       "      <td>0.001938</td>\n",
       "      <td>1.610250</td>\n",
       "      <td>-0.769567</td>\n",
       "      <td>1.647478</td>\n",
       "      <td>-1.635937</td>\n",
       "      <td>3.343394</td>\n",
       "      <td>1.073741</td>\n",
       "      <td>3.291282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4221</th>\n",
       "      <td>3</td>\n",
       "      <td>2017</td>\n",
       "      <td>Kevin Durant</td>\n",
       "      <td>10.555627</td>\n",
       "      <td>2.415188</td>\n",
       "      <td>0.660945</td>\n",
       "      <td>1.254056</td>\n",
       "      <td>2.665042</td>\n",
       "      <td>-0.545910</td>\n",
       "      <td>-1.635937</td>\n",
       "      <td>1.369863</td>\n",
       "      <td>1.809299</td>\n",
       "      <td>2.563082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4222</th>\n",
       "      <td>4</td>\n",
       "      <td>2017</td>\n",
       "      <td>James Harden</td>\n",
       "      <td>10.522310</td>\n",
       "      <td>3.152171</td>\n",
       "      <td>0.118234</td>\n",
       "      <td>2.984140</td>\n",
       "      <td>0.303748</td>\n",
       "      <td>2.134898</td>\n",
       "      <td>-3.374314</td>\n",
       "      <td>2.762944</td>\n",
       "      <td>-0.589500</td>\n",
       "      <td>3.029989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4223</th>\n",
       "      <td>5</td>\n",
       "      <td>2017</td>\n",
       "      <td>Karl-Anthony Towns</td>\n",
       "      <td>9.046572</td>\n",
       "      <td>1.475533</td>\n",
       "      <td>2.793024</td>\n",
       "      <td>-0.272488</td>\n",
       "      <td>1.806390</td>\n",
       "      <td>-0.302200</td>\n",
       "      <td>-0.270069</td>\n",
       "      <td>0.208962</td>\n",
       "      <td>2.213129</td>\n",
       "      <td>1.394291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4224</th>\n",
       "      <td>6</td>\n",
       "      <td>2017</td>\n",
       "      <td>LeBron James</td>\n",
       "      <td>8.050018</td>\n",
       "      <td>2.617858</td>\n",
       "      <td>1.358716</td>\n",
       "      <td>3.136795</td>\n",
       "      <td>0.733074</td>\n",
       "      <td>1.160059</td>\n",
       "      <td>-3.125975</td>\n",
       "      <td>0.557232</td>\n",
       "      <td>2.935558</td>\n",
       "      <td>-1.323300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4225</th>\n",
       "      <td>7</td>\n",
       "      <td>2017</td>\n",
       "      <td>Giannis Antetokounmpo</td>\n",
       "      <td>7.982516</td>\n",
       "      <td>2.507311</td>\n",
       "      <td>1.901427</td>\n",
       "      <td>0.948747</td>\n",
       "      <td>1.806390</td>\n",
       "      <td>1.403769</td>\n",
       "      <td>-1.635937</td>\n",
       "      <td>-0.835849</td>\n",
       "      <td>2.328774</td>\n",
       "      <td>-0.442116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4226</th>\n",
       "      <td>8</td>\n",
       "      <td>2017</td>\n",
       "      <td>Damian Lillard</td>\n",
       "      <td>7.758411</td>\n",
       "      <td>2.507311</td>\n",
       "      <td>-0.230652</td>\n",
       "      <td>1.864674</td>\n",
       "      <td>-0.340241</td>\n",
       "      <td>0.428929</td>\n",
       "      <td>-1.387598</td>\n",
       "      <td>2.066403</td>\n",
       "      <td>-0.920536</td>\n",
       "      <td>3.770119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4227</th>\n",
       "      <td>9</td>\n",
       "      <td>2017</td>\n",
       "      <td>Chris Paul</td>\n",
       "      <td>7.259998</td>\n",
       "      <td>0.978069</td>\n",
       "      <td>0.118234</td>\n",
       "      <td>2.526177</td>\n",
       "      <td>-0.769567</td>\n",
       "      <td>1.891188</td>\n",
       "      <td>-0.642579</td>\n",
       "      <td>1.369863</td>\n",
       "      <td>-0.176916</td>\n",
       "      <td>1.965529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4228</th>\n",
       "      <td>10</td>\n",
       "      <td>2017</td>\n",
       "      <td>Jimmy Butler</td>\n",
       "      <td>7.194467</td>\n",
       "      <td>1.641355</td>\n",
       "      <td>0.079468</td>\n",
       "      <td>0.999632</td>\n",
       "      <td>-0.340241</td>\n",
       "      <td>2.622318</td>\n",
       "      <td>-0.145900</td>\n",
       "      <td>-0.139308</td>\n",
       "      <td>0.324360</td>\n",
       "      <td>2.152782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4229</th>\n",
       "      <td>11</td>\n",
       "      <td>2017</td>\n",
       "      <td>Victor Oladipo</td>\n",
       "      <td>6.742591</td>\n",
       "      <td>1.807176</td>\n",
       "      <td>0.040703</td>\n",
       "      <td>0.694323</td>\n",
       "      <td>0.518411</td>\n",
       "      <td>3.597157</td>\n",
       "      <td>-1.511767</td>\n",
       "      <td>0.905503</td>\n",
       "      <td>0.389094</td>\n",
       "      <td>0.301991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4230</th>\n",
       "      <td>12</td>\n",
       "      <td>2017</td>\n",
       "      <td>DeMarcus Cousins</td>\n",
       "      <td>6.513379</td>\n",
       "      <td>2.194092</td>\n",
       "      <td>3.025615</td>\n",
       "      <td>1.254056</td>\n",
       "      <td>2.235716</td>\n",
       "      <td>1.647478</td>\n",
       "      <td>-4.119333</td>\n",
       "      <td>1.021593</td>\n",
       "      <td>0.301785</td>\n",
       "      <td>-1.047624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4231</th>\n",
       "      <td>13</td>\n",
       "      <td>2017</td>\n",
       "      <td>Nikola Jokic</td>\n",
       "      <td>6.424866</td>\n",
       "      <td>0.959645</td>\n",
       "      <td>2.172783</td>\n",
       "      <td>1.610250</td>\n",
       "      <td>0.518411</td>\n",
       "      <td>0.672639</td>\n",
       "      <td>-1.387598</td>\n",
       "      <td>0.208962</td>\n",
       "      <td>0.838766</td>\n",
       "      <td>0.831007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4232</th>\n",
       "      <td>14</td>\n",
       "      <td>2017</td>\n",
       "      <td>Kyrie Irving</td>\n",
       "      <td>6.159878</td>\n",
       "      <td>2.046696</td>\n",
       "      <td>-0.502008</td>\n",
       "      <td>1.101402</td>\n",
       "      <td>-0.554904</td>\n",
       "      <td>0.428929</td>\n",
       "      <td>-0.766749</td>\n",
       "      <td>1.718133</td>\n",
       "      <td>0.968233</td>\n",
       "      <td>1.720144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4233</th>\n",
       "      <td>15</td>\n",
       "      <td>2017</td>\n",
       "      <td>Andre Ingram</td>\n",
       "      <td>5.581832</td>\n",
       "      <td>-0.237954</td>\n",
       "      <td>-0.812129</td>\n",
       "      <td>0.287245</td>\n",
       "      <td>2.021053</td>\n",
       "      <td>1.403769</td>\n",
       "      <td>0.226610</td>\n",
       "      <td>1.369863</td>\n",
       "      <td>0.116338</td>\n",
       "      <td>1.207037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234</th>\n",
       "      <td>16</td>\n",
       "      <td>2017</td>\n",
       "      <td>LaMarcus Aldridge</td>\n",
       "      <td>5.370052</td>\n",
       "      <td>1.807176</td>\n",
       "      <td>1.319951</td>\n",
       "      <td>-0.476027</td>\n",
       "      <td>1.377064</td>\n",
       "      <td>-0.789620</td>\n",
       "      <td>0.226610</td>\n",
       "      <td>-1.068029</td>\n",
       "      <td>1.620860</td>\n",
       "      <td>1.352068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4235</th>\n",
       "      <td>17</td>\n",
       "      <td>2017</td>\n",
       "      <td>MarShon Brooks</td>\n",
       "      <td>5.131062</td>\n",
       "      <td>1.254438</td>\n",
       "      <td>-0.812129</td>\n",
       "      <td>0.338130</td>\n",
       "      <td>-0.340241</td>\n",
       "      <td>1.647478</td>\n",
       "      <td>-0.766749</td>\n",
       "      <td>1.602043</td>\n",
       "      <td>1.009007</td>\n",
       "      <td>1.199083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4236</th>\n",
       "      <td>18</td>\n",
       "      <td>2017</td>\n",
       "      <td>Paul George</td>\n",
       "      <td>5.126899</td>\n",
       "      <td>1.586081</td>\n",
       "      <td>0.234529</td>\n",
       "      <td>0.185475</td>\n",
       "      <td>-0.125578</td>\n",
       "      <td>2.622318</td>\n",
       "      <td>-1.263428</td>\n",
       "      <td>2.066403</td>\n",
       "      <td>-1.086400</td>\n",
       "      <td>0.907499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4237</th>\n",
       "      <td>19</td>\n",
       "      <td>2017</td>\n",
       "      <td>Kristaps Porzingis</td>\n",
       "      <td>4.816889</td>\n",
       "      <td>1.733478</td>\n",
       "      <td>0.583415</td>\n",
       "      <td>-0.883106</td>\n",
       "      <td>3.953020</td>\n",
       "      <td>-0.302200</td>\n",
       "      <td>-0.270069</td>\n",
       "      <td>0.673322</td>\n",
       "      <td>-0.888515</td>\n",
       "      <td>0.217545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4238</th>\n",
       "      <td>20</td>\n",
       "      <td>2017</td>\n",
       "      <td>Jrue Holiday</td>\n",
       "      <td>4.397228</td>\n",
       "      <td>1.051768</td>\n",
       "      <td>-0.230652</td>\n",
       "      <td>1.559365</td>\n",
       "      <td>0.518411</td>\n",
       "      <td>1.403769</td>\n",
       "      <td>-1.139258</td>\n",
       "      <td>0.208962</td>\n",
       "      <td>0.875856</td>\n",
       "      <td>0.149007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4239</th>\n",
       "      <td>21</td>\n",
       "      <td>2017</td>\n",
       "      <td>Khris Middleton</td>\n",
       "      <td>4.231094</td>\n",
       "      <td>1.254438</td>\n",
       "      <td>0.040703</td>\n",
       "      <td>0.541669</td>\n",
       "      <td>-0.554904</td>\n",
       "      <td>1.403769</td>\n",
       "      <td>-0.766749</td>\n",
       "      <td>0.557232</td>\n",
       "      <td>0.034790</td>\n",
       "      <td>1.720144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4240</th>\n",
       "      <td>22</td>\n",
       "      <td>2017</td>\n",
       "      <td>Nikola Vucevic</td>\n",
       "      <td>4.156660</td>\n",
       "      <td>0.591153</td>\n",
       "      <td>1.591307</td>\n",
       "      <td>0.236360</td>\n",
       "      <td>1.162400</td>\n",
       "      <td>0.185219</td>\n",
       "      <td>-0.270069</td>\n",
       "      <td>-0.255398</td>\n",
       "      <td>0.356381</td>\n",
       "      <td>0.559307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4241</th>\n",
       "      <td>23</td>\n",
       "      <td>2017</td>\n",
       "      <td>Kyle Lowry</td>\n",
       "      <td>4.141570</td>\n",
       "      <td>0.535879</td>\n",
       "      <td>0.195764</td>\n",
       "      <td>2.017329</td>\n",
       "      <td>-0.769567</td>\n",
       "      <td>0.428929</td>\n",
       "      <td>-0.766749</td>\n",
       "      <td>2.066403</td>\n",
       "      <td>-0.765502</td>\n",
       "      <td>1.199083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4242</th>\n",
       "      <td>24</td>\n",
       "      <td>2017</td>\n",
       "      <td>Otto Porter</td>\n",
       "      <td>4.136911</td>\n",
       "      <td>0.259510</td>\n",
       "      <td>0.505884</td>\n",
       "      <td>-0.476027</td>\n",
       "      <td>-0.125578</td>\n",
       "      <td>1.403769</td>\n",
       "      <td>0.847459</td>\n",
       "      <td>0.557232</td>\n",
       "      <td>0.888985</td>\n",
       "      <td>0.275676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4243</th>\n",
       "      <td>25</td>\n",
       "      <td>2017</td>\n",
       "      <td>Kawhi Leonard</td>\n",
       "      <td>4.001655</td>\n",
       "      <td>0.535879</td>\n",
       "      <td>-0.153122</td>\n",
       "      <td>-0.323373</td>\n",
       "      <td>0.947737</td>\n",
       "      <td>2.622318</td>\n",
       "      <td>-0.145900</td>\n",
       "      <td>-0.139308</td>\n",
       "      <td>0.190517</td>\n",
       "      <td>0.466907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4244</th>\n",
       "      <td>26</td>\n",
       "      <td>2017</td>\n",
       "      <td>Kevin Love</td>\n",
       "      <td>3.988737</td>\n",
       "      <td>0.793824</td>\n",
       "      <td>1.630072</td>\n",
       "      <td>-0.628682</td>\n",
       "      <td>-0.340241</td>\n",
       "      <td>-0.545910</td>\n",
       "      <td>0.226610</td>\n",
       "      <td>1.137683</td>\n",
       "      <td>-0.085231</td>\n",
       "      <td>1.800614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4245</th>\n",
       "      <td>27</td>\n",
       "      <td>2017</td>\n",
       "      <td>Andre Drummond</td>\n",
       "      <td>3.805188</td>\n",
       "      <td>0.314784</td>\n",
       "      <td>4.227332</td>\n",
       "      <td>0.032821</td>\n",
       "      <td>2.235716</td>\n",
       "      <td>1.403769</td>\n",
       "      <td>-1.139258</td>\n",
       "      <td>-1.532389</td>\n",
       "      <td>1.440481</td>\n",
       "      <td>-3.178068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4246</th>\n",
       "      <td>28</td>\n",
       "      <td>2017</td>\n",
       "      <td>Russell Westbrook</td>\n",
       "      <td>3.763264</td>\n",
       "      <td>2.230942</td>\n",
       "      <td>1.940192</td>\n",
       "      <td>3.747412</td>\n",
       "      <td>-0.554904</td>\n",
       "      <td>2.134898</td>\n",
       "      <td>-3.870994</td>\n",
       "      <td>-0.139308</td>\n",
       "      <td>-0.520390</td>\n",
       "      <td>-1.204585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4247</th>\n",
       "      <td>29</td>\n",
       "      <td>2017</td>\n",
       "      <td>Kemba Walker</td>\n",
       "      <td>3.725170</td>\n",
       "      <td>1.622930</td>\n",
       "      <td>-0.773363</td>\n",
       "      <td>1.355826</td>\n",
       "      <td>-0.554904</td>\n",
       "      <td>0.428929</td>\n",
       "      <td>-0.642579</td>\n",
       "      <td>1.834223</td>\n",
       "      <td>-0.897960</td>\n",
       "      <td>1.352068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4248</th>\n",
       "      <td>30</td>\n",
       "      <td>2017</td>\n",
       "      <td>Eric Bledsoe</td>\n",
       "      <td>3.653643</td>\n",
       "      <td>0.830673</td>\n",
       "      <td>-0.463243</td>\n",
       "      <td>1.101402</td>\n",
       "      <td>0.089085</td>\n",
       "      <td>2.622318</td>\n",
       "      <td>-1.511767</td>\n",
       "      <td>0.441142</td>\n",
       "      <td>0.360757</td>\n",
       "      <td>0.183276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4729</th>\n",
       "      <td>511</td>\n",
       "      <td>2017</td>\n",
       "      <td>Darrun Hilliard</td>\n",
       "      <td>-8.567256</td>\n",
       "      <td>-2.246234</td>\n",
       "      <td>-1.781256</td>\n",
       "      <td>-1.086645</td>\n",
       "      <td>-1.198893</td>\n",
       "      <td>-2.008169</td>\n",
       "      <td>1.716648</td>\n",
       "      <td>-1.532389</td>\n",
       "      <td>-0.468564</td>\n",
       "      <td>0.038246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4730</th>\n",
       "      <td>512</td>\n",
       "      <td>2017</td>\n",
       "      <td>Tim Quarterman</td>\n",
       "      <td>-8.645561</td>\n",
       "      <td>-2.209385</td>\n",
       "      <td>-1.587430</td>\n",
       "      <td>-1.341069</td>\n",
       "      <td>-1.198893</td>\n",
       "      <td>-2.251879</td>\n",
       "      <td>1.219968</td>\n",
       "      <td>-1.532389</td>\n",
       "      <td>-0.307768</td>\n",
       "      <td>0.563284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4731</th>\n",
       "      <td>513</td>\n",
       "      <td>2017</td>\n",
       "      <td>Demetrius Jackson</td>\n",
       "      <td>-8.660268</td>\n",
       "      <td>-2.319932</td>\n",
       "      <td>-1.626195</td>\n",
       "      <td>-1.290185</td>\n",
       "      <td>-0.984230</td>\n",
       "      <td>-1.520749</td>\n",
       "      <td>1.095798</td>\n",
       "      <td>-1.532389</td>\n",
       "      <td>-0.482386</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4732</th>\n",
       "      <td>514</td>\n",
       "      <td>2017</td>\n",
       "      <td>Markel Brown</td>\n",
       "      <td>-8.715761</td>\n",
       "      <td>-2.209385</td>\n",
       "      <td>-1.471135</td>\n",
       "      <td>-1.239300</td>\n",
       "      <td>-1.198893</td>\n",
       "      <td>-2.251879</td>\n",
       "      <td>1.468308</td>\n",
       "      <td>-1.184119</td>\n",
       "      <td>-0.629359</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4733</th>\n",
       "      <td>515</td>\n",
       "      <td>2017</td>\n",
       "      <td>Josh Smith</td>\n",
       "      <td>-8.748490</td>\n",
       "      <td>-2.319932</td>\n",
       "      <td>-1.471135</td>\n",
       "      <td>-1.493724</td>\n",
       "      <td>-1.198893</td>\n",
       "      <td>-2.251879</td>\n",
       "      <td>2.089157</td>\n",
       "      <td>-1.532389</td>\n",
       "      <td>-0.569694</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4734</th>\n",
       "      <td>516</td>\n",
       "      <td>2017</td>\n",
       "      <td>Erik McCree</td>\n",
       "      <td>-8.773341</td>\n",
       "      <td>-2.448905</td>\n",
       "      <td>-1.858786</td>\n",
       "      <td>-1.493724</td>\n",
       "      <td>-1.198893</td>\n",
       "      <td>-1.520749</td>\n",
       "      <td>1.716648</td>\n",
       "      <td>-1.532389</td>\n",
       "      <td>-0.436543</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4735</th>\n",
       "      <td>517</td>\n",
       "      <td>2017</td>\n",
       "      <td>Xavier Munford</td>\n",
       "      <td>-8.789856</td>\n",
       "      <td>-2.356782</td>\n",
       "      <td>-1.897551</td>\n",
       "      <td>-1.137530</td>\n",
       "      <td>-1.198893</td>\n",
       "      <td>-1.764459</td>\n",
       "      <td>1.716648</td>\n",
       "      <td>-1.532389</td>\n",
       "      <td>-0.496208</td>\n",
       "      <td>-0.122692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4736</th>\n",
       "      <td>518</td>\n",
       "      <td>2017</td>\n",
       "      <td>Tyler Lydon</td>\n",
       "      <td>-8.811713</td>\n",
       "      <td>-2.448905</td>\n",
       "      <td>-1.975081</td>\n",
       "      <td>-1.493724</td>\n",
       "      <td>-1.198893</td>\n",
       "      <td>-2.251879</td>\n",
       "      <td>2.089157</td>\n",
       "      <td>-1.532389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4737</th>\n",
       "      <td>519</td>\n",
       "      <td>2017</td>\n",
       "      <td>Trey McKinney-Jones</td>\n",
       "      <td>-8.811713</td>\n",
       "      <td>-2.448905</td>\n",
       "      <td>-1.975081</td>\n",
       "      <td>-1.493724</td>\n",
       "      <td>-1.198893</td>\n",
       "      <td>-2.251879</td>\n",
       "      <td>2.089157</td>\n",
       "      <td>-1.532389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4738</th>\n",
       "      <td>520</td>\n",
       "      <td>2017</td>\n",
       "      <td>Xavier Rathan-Mayes</td>\n",
       "      <td>-8.814577</td>\n",
       "      <td>-1.380278</td>\n",
       "      <td>-1.587430</td>\n",
       "      <td>0.338130</td>\n",
       "      <td>0.089085</td>\n",
       "      <td>0.672639</td>\n",
       "      <td>-0.642579</td>\n",
       "      <td>-1.300209</td>\n",
       "      <td>-2.811382</td>\n",
       "      <td>-2.192553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4739</th>\n",
       "      <td>521</td>\n",
       "      <td>2017</td>\n",
       "      <td>Matt Williams</td>\n",
       "      <td>-8.833537</td>\n",
       "      <td>-2.135687</td>\n",
       "      <td>-1.858786</td>\n",
       "      <td>-1.493724</td>\n",
       "      <td>-1.198893</td>\n",
       "      <td>-2.251879</td>\n",
       "      <td>1.716648</td>\n",
       "      <td>-1.184119</td>\n",
       "      <td>-0.427098</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4740</th>\n",
       "      <td>522</td>\n",
       "      <td>2017</td>\n",
       "      <td>Cole Aldrich</td>\n",
       "      <td>-8.856290</td>\n",
       "      <td>-2.338357</td>\n",
       "      <td>-1.703725</td>\n",
       "      <td>-1.442839</td>\n",
       "      <td>-1.198893</td>\n",
       "      <td>-2.008169</td>\n",
       "      <td>2.089157</td>\n",
       "      <td>-1.532389</td>\n",
       "      <td>-0.234282</td>\n",
       "      <td>-0.486792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4741</th>\n",
       "      <td>523</td>\n",
       "      <td>2017</td>\n",
       "      <td>Nicolas Brussino</td>\n",
       "      <td>-8.938136</td>\n",
       "      <td>-2.448905</td>\n",
       "      <td>-1.664960</td>\n",
       "      <td>-1.493724</td>\n",
       "      <td>-1.198893</td>\n",
       "      <td>-2.251879</td>\n",
       "      <td>2.089157</td>\n",
       "      <td>-1.532389</td>\n",
       "      <td>-0.436543</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>524</td>\n",
       "      <td>2017</td>\n",
       "      <td>PJ Dozier</td>\n",
       "      <td>-8.985381</td>\n",
       "      <td>-2.264659</td>\n",
       "      <td>-1.781256</td>\n",
       "      <td>-1.493724</td>\n",
       "      <td>-1.198893</td>\n",
       "      <td>-2.251879</td>\n",
       "      <td>1.468308</td>\n",
       "      <td>-1.532389</td>\n",
       "      <td>0.069110</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4743</th>\n",
       "      <td>525</td>\n",
       "      <td>2017</td>\n",
       "      <td>Omer Asik</td>\n",
       "      <td>-9.030298</td>\n",
       "      <td>-2.209385</td>\n",
       "      <td>-0.967189</td>\n",
       "      <td>-1.442839</td>\n",
       "      <td>-0.984230</td>\n",
       "      <td>-2.008169</td>\n",
       "      <td>1.592478</td>\n",
       "      <td>-1.532389</td>\n",
       "      <td>-0.018199</td>\n",
       "      <td>-1.460376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4744</th>\n",
       "      <td>526</td>\n",
       "      <td>2017</td>\n",
       "      <td>London Perrantes</td>\n",
       "      <td>-9.069214</td>\n",
       "      <td>-2.356782</td>\n",
       "      <td>-1.858786</td>\n",
       "      <td>-1.290185</td>\n",
       "      <td>-0.984230</td>\n",
       "      <td>-2.008169</td>\n",
       "      <td>1.964987</td>\n",
       "      <td>-1.532389</td>\n",
       "      <td>-0.597338</td>\n",
       "      <td>-0.406323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4745</th>\n",
       "      <td>527</td>\n",
       "      <td>2017</td>\n",
       "      <td>Jacob Pullen</td>\n",
       "      <td>-9.101093</td>\n",
       "      <td>-2.319932</td>\n",
       "      <td>-1.975081</td>\n",
       "      <td>-1.493724</td>\n",
       "      <td>-1.198893</td>\n",
       "      <td>-2.251879</td>\n",
       "      <td>1.716648</td>\n",
       "      <td>-1.532389</td>\n",
       "      <td>-0.045843</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4746</th>\n",
       "      <td>528</td>\n",
       "      <td>2017</td>\n",
       "      <td>Charles Cooke</td>\n",
       "      <td>-9.142223</td>\n",
       "      <td>-2.356782</td>\n",
       "      <td>-1.897551</td>\n",
       "      <td>-1.442839</td>\n",
       "      <td>-1.198893</td>\n",
       "      <td>-2.008169</td>\n",
       "      <td>1.964987</td>\n",
       "      <td>-1.416299</td>\n",
       "      <td>-0.583516</td>\n",
       "      <td>-0.203161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747</th>\n",
       "      <td>529</td>\n",
       "      <td>2017</td>\n",
       "      <td>Nate Wolters</td>\n",
       "      <td>-9.152010</td>\n",
       "      <td>-2.375206</td>\n",
       "      <td>-1.820021</td>\n",
       "      <td>-1.391954</td>\n",
       "      <td>-1.198893</td>\n",
       "      <td>-2.251879</td>\n",
       "      <td>2.089157</td>\n",
       "      <td>-1.532389</td>\n",
       "      <td>-0.670825</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4748</th>\n",
       "      <td>530</td>\n",
       "      <td>2017</td>\n",
       "      <td>Derrick Williams</td>\n",
       "      <td>-9.237618</td>\n",
       "      <td>-2.264659</td>\n",
       "      <td>-1.781256</td>\n",
       "      <td>-1.493724</td>\n",
       "      <td>-1.198893</td>\n",
       "      <td>-2.251879</td>\n",
       "      <td>2.089157</td>\n",
       "      <td>-1.532389</td>\n",
       "      <td>-0.803976</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4749</th>\n",
       "      <td>531</td>\n",
       "      <td>2017</td>\n",
       "      <td>Josh McRoberts</td>\n",
       "      <td>-9.248257</td>\n",
       "      <td>-2.448905</td>\n",
       "      <td>-1.975081</td>\n",
       "      <td>-1.493724</td>\n",
       "      <td>-1.198893</td>\n",
       "      <td>-2.251879</td>\n",
       "      <td>2.089157</td>\n",
       "      <td>-1.532389</td>\n",
       "      <td>-0.436543</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4750</th>\n",
       "      <td>532</td>\n",
       "      <td>2017</td>\n",
       "      <td>Nick Collison</td>\n",
       "      <td>-9.288480</td>\n",
       "      <td>-2.061988</td>\n",
       "      <td>-1.471135</td>\n",
       "      <td>-1.341069</td>\n",
       "      <td>-1.198893</td>\n",
       "      <td>-2.251879</td>\n",
       "      <td>1.468308</td>\n",
       "      <td>-1.532389</td>\n",
       "      <td>0.560941</td>\n",
       "      <td>-1.460376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4751</th>\n",
       "      <td>533</td>\n",
       "      <td>2017</td>\n",
       "      <td>Chris Boucher</td>\n",
       "      <td>-9.297149</td>\n",
       "      <td>-2.448905</td>\n",
       "      <td>-1.587430</td>\n",
       "      <td>-1.493724</td>\n",
       "      <td>-1.198893</td>\n",
       "      <td>-2.251879</td>\n",
       "      <td>2.089157</td>\n",
       "      <td>-1.532389</td>\n",
       "      <td>-0.873086</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4752</th>\n",
       "      <td>534</td>\n",
       "      <td>2017</td>\n",
       "      <td>Kyle Singler</td>\n",
       "      <td>-9.322633</td>\n",
       "      <td>-2.098837</td>\n",
       "      <td>-1.664960</td>\n",
       "      <td>-1.391954</td>\n",
       "      <td>-1.198893</td>\n",
       "      <td>-2.008169</td>\n",
       "      <td>1.716648</td>\n",
       "      <td>-1.300209</td>\n",
       "      <td>-0.440920</td>\n",
       "      <td>-0.935338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4753</th>\n",
       "      <td>535</td>\n",
       "      <td>2017</td>\n",
       "      <td>Vander Blue</td>\n",
       "      <td>-9.478457</td>\n",
       "      <td>-2.338357</td>\n",
       "      <td>-1.897551</td>\n",
       "      <td>-1.188415</td>\n",
       "      <td>-1.198893</td>\n",
       "      <td>-1.764459</td>\n",
       "      <td>1.344138</td>\n",
       "      <td>-1.532389</td>\n",
       "      <td>-0.496208</td>\n",
       "      <td>-0.406323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4754</th>\n",
       "      <td>536</td>\n",
       "      <td>2017</td>\n",
       "      <td>Aaron Jackson</td>\n",
       "      <td>-9.982955</td>\n",
       "      <td>-0.974937</td>\n",
       "      <td>-0.812129</td>\n",
       "      <td>-0.984876</td>\n",
       "      <td>-1.198893</td>\n",
       "      <td>-2.251879</td>\n",
       "      <td>0.847459</td>\n",
       "      <td>-0.371488</td>\n",
       "      <td>-2.204598</td>\n",
       "      <td>-2.031614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4755</th>\n",
       "      <td>537</td>\n",
       "      <td>2017</td>\n",
       "      <td>Luis Montero</td>\n",
       "      <td>-10.102304</td>\n",
       "      <td>-2.448905</td>\n",
       "      <td>-1.587430</td>\n",
       "      <td>-1.493724</td>\n",
       "      <td>-1.198893</td>\n",
       "      <td>-2.251879</td>\n",
       "      <td>0.847459</td>\n",
       "      <td>-1.532389</td>\n",
       "      <td>-0.436543</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4756</th>\n",
       "      <td>538</td>\n",
       "      <td>2017</td>\n",
       "      <td>Chinanu Onuaku</td>\n",
       "      <td>-10.337018</td>\n",
       "      <td>-1.711921</td>\n",
       "      <td>-0.424478</td>\n",
       "      <td>-0.984876</td>\n",
       "      <td>-1.198893</td>\n",
       "      <td>-2.251879</td>\n",
       "      <td>-1.635937</td>\n",
       "      <td>-1.532389</td>\n",
       "      <td>-0.596646</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4757</th>\n",
       "      <td>539</td>\n",
       "      <td>2017</td>\n",
       "      <td>Mindaugas Kuzminskas</td>\n",
       "      <td>-10.557886</td>\n",
       "      <td>-2.448905</td>\n",
       "      <td>-1.975081</td>\n",
       "      <td>-1.493724</td>\n",
       "      <td>-1.198893</td>\n",
       "      <td>-2.251879</td>\n",
       "      <td>2.089157</td>\n",
       "      <td>-1.532389</td>\n",
       "      <td>-1.746172</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4758</th>\n",
       "      <td>540</td>\n",
       "      <td>2017</td>\n",
       "      <td>Scotty Hopson</td>\n",
       "      <td>-12.265018</td>\n",
       "      <td>-2.264659</td>\n",
       "      <td>-1.975081</td>\n",
       "      <td>-0.984876</td>\n",
       "      <td>-1.198893</td>\n",
       "      <td>-2.251879</td>\n",
       "      <td>0.847459</td>\n",
       "      <td>-1.532389</td>\n",
       "      <td>-0.873086</td>\n",
       "      <td>-2.031614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>540 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      playerrank  season                 player  value_tot  value_points  \\\n",
       "4219           1    2017          Anthony Davis  13.069292      2.728406   \n",
       "4220           2    2017          Stephen Curry  10.977767      2.415188   \n",
       "4221           3    2017           Kevin Durant  10.555627      2.415188   \n",
       "4222           4    2017           James Harden  10.522310      3.152171   \n",
       "4223           5    2017     Karl-Anthony Towns   9.046572      1.475533   \n",
       "4224           6    2017           LeBron James   8.050018      2.617858   \n",
       "4225           7    2017  Giannis Antetokounmpo   7.982516      2.507311   \n",
       "4226           8    2017         Damian Lillard   7.758411      2.507311   \n",
       "4227           9    2017             Chris Paul   7.259998      0.978069   \n",
       "4228          10    2017           Jimmy Butler   7.194467      1.641355   \n",
       "4229          11    2017         Victor Oladipo   6.742591      1.807176   \n",
       "4230          12    2017       DeMarcus Cousins   6.513379      2.194092   \n",
       "4231          13    2017           Nikola Jokic   6.424866      0.959645   \n",
       "4232          14    2017           Kyrie Irving   6.159878      2.046696   \n",
       "4233          15    2017           Andre Ingram   5.581832     -0.237954   \n",
       "4234          16    2017      LaMarcus Aldridge   5.370052      1.807176   \n",
       "4235          17    2017         MarShon Brooks   5.131062      1.254438   \n",
       "4236          18    2017            Paul George   5.126899      1.586081   \n",
       "4237          19    2017     Kristaps Porzingis   4.816889      1.733478   \n",
       "4238          20    2017           Jrue Holiday   4.397228      1.051768   \n",
       "4239          21    2017        Khris Middleton   4.231094      1.254438   \n",
       "4240          22    2017         Nikola Vucevic   4.156660      0.591153   \n",
       "4241          23    2017             Kyle Lowry   4.141570      0.535879   \n",
       "4242          24    2017            Otto Porter   4.136911      0.259510   \n",
       "4243          25    2017          Kawhi Leonard   4.001655      0.535879   \n",
       "4244          26    2017             Kevin Love   3.988737      0.793824   \n",
       "4245          27    2017         Andre Drummond   3.805188      0.314784   \n",
       "4246          28    2017      Russell Westbrook   3.763264      2.230942   \n",
       "4247          29    2017           Kemba Walker   3.725170      1.622930   \n",
       "4248          30    2017           Eric Bledsoe   3.653643      0.830673   \n",
       "...          ...     ...                    ...        ...           ...   \n",
       "4729         511    2017        Darrun Hilliard  -8.567256     -2.246234   \n",
       "4730         512    2017         Tim Quarterman  -8.645561     -2.209385   \n",
       "4731         513    2017      Demetrius Jackson  -8.660268     -2.319932   \n",
       "4732         514    2017           Markel Brown  -8.715761     -2.209385   \n",
       "4733         515    2017             Josh Smith  -8.748490     -2.319932   \n",
       "4734         516    2017            Erik McCree  -8.773341     -2.448905   \n",
       "4735         517    2017         Xavier Munford  -8.789856     -2.356782   \n",
       "4736         518    2017            Tyler Lydon  -8.811713     -2.448905   \n",
       "4737         519    2017    Trey McKinney-Jones  -8.811713     -2.448905   \n",
       "4738         520    2017    Xavier Rathan-Mayes  -8.814577     -1.380278   \n",
       "4739         521    2017          Matt Williams  -8.833537     -2.135687   \n",
       "4740         522    2017           Cole Aldrich  -8.856290     -2.338357   \n",
       "4741         523    2017       Nicolas Brussino  -8.938136     -2.448905   \n",
       "4742         524    2017              PJ Dozier  -8.985381     -2.264659   \n",
       "4743         525    2017              Omer Asik  -9.030298     -2.209385   \n",
       "4744         526    2017       London Perrantes  -9.069214     -2.356782   \n",
       "4745         527    2017           Jacob Pullen  -9.101093     -2.319932   \n",
       "4746         528    2017          Charles Cooke  -9.142223     -2.356782   \n",
       "4747         529    2017           Nate Wolters  -9.152010     -2.375206   \n",
       "4748         530    2017       Derrick Williams  -9.237618     -2.264659   \n",
       "4749         531    2017         Josh McRoberts  -9.248257     -2.448905   \n",
       "4750         532    2017          Nick Collison  -9.288480     -2.061988   \n",
       "4751         533    2017          Chris Boucher  -9.297149     -2.448905   \n",
       "4752         534    2017           Kyle Singler  -9.322633     -2.098837   \n",
       "4753         535    2017            Vander Blue  -9.478457     -2.338357   \n",
       "4754         536    2017          Aaron Jackson  -9.982955     -0.974937   \n",
       "4755         537    2017           Luis Montero -10.102304     -2.448905   \n",
       "4756         538    2017         Chinanu Onuaku -10.337018     -1.711921   \n",
       "4757         539    2017   Mindaugas Kuzminskas -10.557886     -2.448905   \n",
       "4758         540    2017          Scotty Hopson -12.265018     -2.264659   \n",
       "\n",
       "      value_rebounds  value_assists  value_blocks  value_steals  \\\n",
       "4219        2.327843      -0.323373      4.382346      1.403769   \n",
       "4220        0.001938       1.610250     -0.769567      1.647478   \n",
       "4221        0.660945       1.254056      2.665042     -0.545910   \n",
       "4222        0.118234       2.984140      0.303748      2.134898   \n",
       "4223        2.793024      -0.272488      1.806390     -0.302200   \n",
       "4224        1.358716       3.136795      0.733074      1.160059   \n",
       "4225        1.901427       0.948747      1.806390      1.403769   \n",
       "4226       -0.230652       1.864674     -0.340241      0.428929   \n",
       "4227        0.118234       2.526177     -0.769567      1.891188   \n",
       "4228        0.079468       0.999632     -0.340241      2.622318   \n",
       "4229        0.040703       0.694323      0.518411      3.597157   \n",
       "4230        3.025615       1.254056      2.235716      1.647478   \n",
       "4231        2.172783       1.610250      0.518411      0.672639   \n",
       "4232       -0.502008       1.101402     -0.554904      0.428929   \n",
       "4233       -0.812129       0.287245      2.021053      1.403769   \n",
       "4234        1.319951      -0.476027      1.377064     -0.789620   \n",
       "4235       -0.812129       0.338130     -0.340241      1.647478   \n",
       "4236        0.234529       0.185475     -0.125578      2.622318   \n",
       "4237        0.583415      -0.883106      3.953020     -0.302200   \n",
       "4238       -0.230652       1.559365      0.518411      1.403769   \n",
       "4239        0.040703       0.541669     -0.554904      1.403769   \n",
       "4240        1.591307       0.236360      1.162400      0.185219   \n",
       "4241        0.195764       2.017329     -0.769567      0.428929   \n",
       "4242        0.505884      -0.476027     -0.125578      1.403769   \n",
       "4243       -0.153122      -0.323373      0.947737      2.622318   \n",
       "4244        1.630072      -0.628682     -0.340241     -0.545910   \n",
       "4245        4.227332       0.032821      2.235716      1.403769   \n",
       "4246        1.940192       3.747412     -0.554904      2.134898   \n",
       "4247       -0.773363       1.355826     -0.554904      0.428929   \n",
       "4248       -0.463243       1.101402      0.089085      2.622318   \n",
       "...              ...            ...           ...           ...   \n",
       "4729       -1.781256      -1.086645     -1.198893     -2.008169   \n",
       "4730       -1.587430      -1.341069     -1.198893     -2.251879   \n",
       "4731       -1.626195      -1.290185     -0.984230     -1.520749   \n",
       "4732       -1.471135      -1.239300     -1.198893     -2.251879   \n",
       "4733       -1.471135      -1.493724     -1.198893     -2.251879   \n",
       "4734       -1.858786      -1.493724     -1.198893     -1.520749   \n",
       "4735       -1.897551      -1.137530     -1.198893     -1.764459   \n",
       "4736       -1.975081      -1.493724     -1.198893     -2.251879   \n",
       "4737       -1.975081      -1.493724     -1.198893     -2.251879   \n",
       "4738       -1.587430       0.338130      0.089085      0.672639   \n",
       "4739       -1.858786      -1.493724     -1.198893     -2.251879   \n",
       "4740       -1.703725      -1.442839     -1.198893     -2.008169   \n",
       "4741       -1.664960      -1.493724     -1.198893     -2.251879   \n",
       "4742       -1.781256      -1.493724     -1.198893     -2.251879   \n",
       "4743       -0.967189      -1.442839     -0.984230     -2.008169   \n",
       "4744       -1.858786      -1.290185     -0.984230     -2.008169   \n",
       "4745       -1.975081      -1.493724     -1.198893     -2.251879   \n",
       "4746       -1.897551      -1.442839     -1.198893     -2.008169   \n",
       "4747       -1.820021      -1.391954     -1.198893     -2.251879   \n",
       "4748       -1.781256      -1.493724     -1.198893     -2.251879   \n",
       "4749       -1.975081      -1.493724     -1.198893     -2.251879   \n",
       "4750       -1.471135      -1.341069     -1.198893     -2.251879   \n",
       "4751       -1.587430      -1.493724     -1.198893     -2.251879   \n",
       "4752       -1.664960      -1.391954     -1.198893     -2.008169   \n",
       "4753       -1.897551      -1.188415     -1.198893     -1.764459   \n",
       "4754       -0.812129      -0.984876     -1.198893     -2.251879   \n",
       "4755       -1.587430      -1.493724     -1.198893     -2.251879   \n",
       "4756       -0.424478      -0.984876     -1.198893     -2.251879   \n",
       "4757       -1.975081      -1.493724     -1.198893     -2.251879   \n",
       "4758       -1.975081      -0.984876     -1.198893     -2.251879   \n",
       "\n",
       "      value_turnovers  value_threes  value_fg  value_ft  \n",
       "4219        -0.642579     -0.719758  2.572502  1.340136  \n",
       "4220        -1.635937      3.343394  1.073741  3.291282  \n",
       "4221        -1.635937      1.369863  1.809299  2.563082  \n",
       "4222        -3.374314      2.762944 -0.589500  3.029989  \n",
       "4223        -0.270069      0.208962  2.213129  1.394291  \n",
       "4224        -3.125975      0.557232  2.935558 -1.323300  \n",
       "4225        -1.635937     -0.835849  2.328774 -0.442116  \n",
       "4226        -1.387598      2.066403 -0.920536  3.770119  \n",
       "4227        -0.642579      1.369863 -0.176916  1.965529  \n",
       "4228        -0.145900     -0.139308  0.324360  2.152782  \n",
       "4229        -1.511767      0.905503  0.389094  0.301991  \n",
       "4230        -4.119333      1.021593  0.301785 -1.047624  \n",
       "4231        -1.387598      0.208962  0.838766  0.831007  \n",
       "4232        -0.766749      1.718133  0.968233  1.720144  \n",
       "4233         0.226610      1.369863  0.116338  1.207037  \n",
       "4234         0.226610     -1.068029  1.620860  1.352068  \n",
       "4235        -0.766749      1.602043  1.009007  1.199083  \n",
       "4236        -1.263428      2.066403 -1.086400  0.907499  \n",
       "4237        -0.270069      0.673322 -0.888515  0.217545  \n",
       "4238        -1.139258      0.208962  0.875856  0.149007  \n",
       "4239        -0.766749      0.557232  0.034790  1.720144  \n",
       "4240        -0.270069     -0.255398  0.356381  0.559307  \n",
       "4241        -0.766749      2.066403 -0.765502  1.199083  \n",
       "4242         0.847459      0.557232  0.888985  0.275676  \n",
       "4243        -0.145900     -0.139308  0.190517  0.466907  \n",
       "4244         0.226610      1.137683 -0.085231  1.800614  \n",
       "4245        -1.139258     -1.532389  1.440481 -3.178068  \n",
       "4246        -3.870994     -0.139308 -0.520390 -1.204585  \n",
       "4247        -0.642579      1.834223 -0.897960  1.352068  \n",
       "4248        -1.511767      0.441142  0.360757  0.183276  \n",
       "...               ...           ...       ...       ...  \n",
       "4729         1.716648     -1.532389 -0.468564  0.038246  \n",
       "4730         1.219968     -1.532389 -0.307768  0.563284  \n",
       "4731         1.095798     -1.532389 -0.482386  0.000000  \n",
       "4732         1.468308     -1.184119 -0.629359  0.000000  \n",
       "4733         2.089157     -1.532389 -0.569694  0.000000  \n",
       "4734         1.716648     -1.532389 -0.436543  0.000000  \n",
       "4735         1.716648     -1.532389 -0.496208 -0.122692  \n",
       "4736         2.089157     -1.532389  0.000000  0.000000  \n",
       "4737         2.089157     -1.532389  0.000000  0.000000  \n",
       "4738        -0.642579     -1.300209 -2.811382 -2.192553  \n",
       "4739         1.716648     -1.184119 -0.427098  0.000000  \n",
       "4740         2.089157     -1.532389 -0.234282 -0.486792  \n",
       "4741         2.089157     -1.532389 -0.436543  0.000000  \n",
       "4742         1.468308     -1.532389  0.069110  0.000000  \n",
       "4743         1.592478     -1.532389 -0.018199 -1.460376  \n",
       "4744         1.964987     -1.532389 -0.597338 -0.406323  \n",
       "4745         1.716648     -1.532389 -0.045843  0.000000  \n",
       "4746         1.964987     -1.416299 -0.583516 -0.203161  \n",
       "4747         2.089157     -1.532389 -0.670825  0.000000  \n",
       "4748         2.089157     -1.532389 -0.803976  0.000000  \n",
       "4749         2.089157     -1.532389 -0.436543  0.000000  \n",
       "4750         1.468308     -1.532389  0.560941 -1.460376  \n",
       "4751         2.089157     -1.532389 -0.873086  0.000000  \n",
       "4752         1.716648     -1.300209 -0.440920 -0.935338  \n",
       "4753         1.344138     -1.532389 -0.496208 -0.406323  \n",
       "4754         0.847459     -0.371488 -2.204598 -2.031614  \n",
       "4755         0.847459     -1.532389 -0.436543  0.000000  \n",
       "4756        -1.635937     -1.532389 -0.596646  0.000000  \n",
       "4757         2.089157     -1.532389 -1.746172  0.000000  \n",
       "4758         0.847459     -1.532389 -0.873086 -2.031614  \n",
       "\n",
       "[540 rows x 13 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2[df_2['season']==2017].sort_values(by='playerrank', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Creating all teammate based changes\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "query = '''\n",
    "        DROP TABLE IF EXISTS adv_top10min;\n",
    "        CREATE TEMPORARY TABLE adv_top10min as \n",
    "        select a.*,usage_percentage*total_MP/G as usage_withMins,row_number() over(partition by a.season,tm order by usage_percentage*total_MP/G desc) as usage_rank\n",
    "        from(select *,row_number() over(partition by season,tm order by total_MP desc) as min_rank from nba_advanced) a\n",
    "        inner join y_predictions y\n",
    "                ON a.player = y.player\n",
    "                and a.season = y.season\n",
    "                and a.Tm=y.startingTeam\n",
    "        where min_rank<=10;\n",
    "        \n",
    "        DROP TABLE IF EXISTS Change_Teams;\n",
    "        CREATE TABLE Change_Teams AS\n",
    "        select na.tm as old_team,na2.tm as new_team,na2.player,na2.pos,p.season,na.usage_withMins,\n",
    "        n.points,n.rebounds,n.ast,n.threes_made,na.usage_rank \n",
    "        from player_value p\n",
    "        inner join adv_top10min na\n",
    "            on p.player = na.player\n",
    "            and p.season = na.season+1\n",
    "        inner join adv_top10min na2\n",
    "            on na2.player = na.player\n",
    "            and na2.season = p.season\n",
    "            and na2.tm != na.tm\n",
    "        inner join rank_by_minutes n\n",
    "            ON N.player = na.player\n",
    "            and n.season = na.season;\n",
    "        \n",
    "        \n",
    "        \n",
    "        DROP TABLE IF EXISTS incoming_by_team;\n",
    "        CREATE TABLE incoming_by_team AS\n",
    "        select new_team,season,SUM(case when usage_withmins >1000 then 1 else 0 end) as high_usageplayer_added,\n",
    "        SUM(usage_withmins) as usagemin_added, MAX(usage_withmins) as max_usageadded,\n",
    "        SUM(points) as points_added, MAX(points) as max_pointsadded,SUM(rebounds) as rebounds_added,\n",
    "        MAX(rebounds) as max_reboundsadded, SUM(ast) as ast_added, MAX(ast) as max_astadded,\n",
    "        SUM(threes_made) as threes_added, MAX(threes_made) as max_threesadded \n",
    "        from change_teams\n",
    "        group by new_team,season;\n",
    "        \n",
    "        DROP TABLE IF EXISTS outgoing_by_team;\n",
    "        CREATE TABLE outgoing_by_team AS\n",
    "        select old_team,season,SUM(case when usage_withmins >1000 then 1 else 0 end) as high_usageplayer_dropped,\n",
    "        SUM(usage_withmins) as usagemin_dropped, MAX(usage_withmins) as max_usagedropped,\n",
    "        SUM(points) as points_dropped, MAX(points) as max_pointsdropped,SUM(rebounds) as rebounds_dropped,\n",
    "        MAX(rebounds) as max_reboundsdropped, SUM(ast) as ast_dropped, MAX(ast) as max_astdropped,\n",
    "        SUM(threes_made) as threes_dropped, MAX(threes_made) as max_threesdropped\n",
    "        from change_teams\n",
    "        group by old_team,season;\n",
    "        \n",
    "        DROP TABLE IF EXISTS Team_Changes;\n",
    "        CREATE TABLE Team_Changes AS\n",
    "        select c.new_team as team, c.season,c.high_usageplayer_added,o.usagemin_dropped-c.usagemin_added as usagemin_opened,\n",
    "        c.max_usageadded,o.high_usageplayer_dropped,o.max_usagedropped,\n",
    "        o.points_dropped-c.points_added as points_opened,max_pointsdropped,max_pointsadded,\n",
    "        o.rebounds_dropped-c.rebounds_added as rebounds_opened,max_reboundsdropped,max_reboundsadded,\n",
    "        o.ast_dropped-c.ast_added as ast_opened,max_astdropped,max_astadded,\n",
    "        o.threes_dropped-c.threes_added as threes_opened,max_threesdropped,max_threesadded\n",
    "        from incoming_by_team c\n",
    "        inner join outgoing_by_team o\n",
    "            ON o.old_team = c.new_team\n",
    "            and o.season = c.season;\n",
    "            \n",
    "        DROP TABLE IF EXISTS Team_maxes;\n",
    "        CREATE TABLE Team_maxes AS\n",
    "        select R.season,R.startingTeam,MAX(R2.points) as pts, max(r2.rebounds) as reb, max(r2.ast) as ast,\n",
    "        MAX(R2.Tov) as TO,MAX(r2.FGA) as shot_attempts, cast(NULL as float) as max_usage \n",
    "        from rank_by_minutes R\n",
    "        inner join rank_by_minutes R2\n",
    "            ON R2.startingTeam = R.startingTeam\n",
    "            and R2.season+1 = R.season\n",
    "        group by R.season,R.startingTeam;\n",
    "        \n",
    "        \n",
    "        update Team_maxes T\n",
    "        set max_usage = usage\n",
    "        from\n",
    "        (select T.season,T.startingTeam,T.pts,t.reb,t.ast,t.TO,MAX(a.usage_percentage) as usage from Team_maxes T\n",
    "        inner join adv_top10min a\n",
    "            ON a.season+1 = t.season\n",
    "            and a.tm = t.startingteam\n",
    "        group by T.season,T.startingTeam,T.pts,t.reb,t.ast,t.TO) A\n",
    "        WHERE A.season = T.season and  A.startingTeam = T.startingTeam;\n",
    "        \n",
    "        \n",
    "        \n",
    "        '''\n",
    "\n",
    "\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Get player based data\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "query = '''\n",
    "        DROP TABLE IF EXISTS player_stats;\n",
    "        CREATE TABLE player_stats AS\n",
    "        select r.*,r2.starter as starter_ly,r3.points-r2.points as change_points_ly,r2.points as points_ly\n",
    "        ,r3.rebounds-r2.rebounds as change_reb_ly, r2.rebounds as rebounds_ly,\n",
    "        r3.ast-r2.ast as change_ast_ly, r2.ast as ast_ly,r3.stl-r2.stl as change_stl_ly,r2.stl as stl_ly,\n",
    "        r3.blk-r2.blk as change_blk_ly, r2.blk as blk_ly,r3.tov-r2.tov as change_tov_ly,r2.tov as tov_ly\n",
    "        from rank_by_minutes r\n",
    "        left join rank_by_minutes r2\n",
    "            on r.player = r2.player\n",
    "            and r.season = r2.season+1\n",
    "        left join rank_by_minutes r3\n",
    "            ON r3.player = r2.player\n",
    "            and r3.season+1 = r2.season;\n",
    "        \n",
    "        DROP TABLE IF EXISTS player_advstats;\n",
    "        CREATE TABLE player_advstats AS\n",
    "        select y.player,y.season,y.startingteam,a.per as per_ly, a2.per-a.per as change_per,a.three_attempt_rate as threeAR_ly,\n",
    "        a2.three_attempt_rate-a.three_attempt_rate as change_3AR, a.rebound_percentage as reb_perc_ly, a2.rebound_percentage-a.rebound_percentage as change_reb_perc\n",
    "        ,a.assist_percentage as ast_perc_ly, a2.assist_percentage-a.assist_percentage as change_assist_perc\n",
    "        ,a.steal_percentage as stl_perc_ly, a2.steal_percentage-a.steal_percentage as change_stl_perc_ly\n",
    "        ,a.block_percentage as blk_perc_ly, a2.block_percentage-a.block_percentage as change_blk_perc_ly\n",
    "        ,a.turnover_percentage as TO_perc_ly, a2.turnover_percentage-a.turnover_percentage as change_turnover_perc_ly,\n",
    "        rank() over(partition by y.season,y.startingTeam order by a.usage_percentage) as usagerank,\n",
    "        rank() over(partition by y.season,a.tm order by a.usage_percentage) as usagerank_ly,\n",
    "        a.offensive_winshares,\n",
    "        a.defensive_winshares,a.winshares,a.winshares_per48,a.offensive_boxplusminus,a.defensive_boxplusminus,\n",
    "        a.boxplusminus,a.value_overreplacement        \n",
    "        from y_predictions y\n",
    "        left join nba_advanced a\n",
    "            ON a.player = y.player\n",
    "            and a.season+1 = y.season\n",
    "        left join nba_advanced a2\n",
    "            ON a2.player = a.player\n",
    "            and a2.season+1 = a.season;\n",
    "    \n",
    "        \n",
    "        DROP TABLE IF EXISTS player_careerstats;\n",
    "        CREATE TABLE player_careerstats AS\n",
    "        select r.player,r.season,SUM(case when r2.player is not null then 1 else 0 end) as YearsPro, avg(r2.points) as career_points\n",
    "        ,avg(r2.rebounds) as career_rebounds,avg(r2.ast) as career_ast, avg(r2.stl) as career_stl, avg(r2.blk) as career_blk\n",
    "        ,avg(r2.tov) as career_TO, avg(r2.threes_made) as career_threesmade,avg(r2.ftm) as career_ftm,avg(r2.fta) as career_fta\n",
    "        ,avg(r2.fga) as career_fga, avg(r2.fg) as career_fgm\n",
    "        from rank_by_minutes r\n",
    "        inner join rank_by_minutes r2\n",
    "            ON r.player = r2.player\n",
    "            and r.season > r2.season\n",
    "        group by r.player,r.season;\n",
    "        \n",
    "        DROP TABLE IF EXISTS points_pred;\n",
    "        CREATE TABLE points_pred(\n",
    "        season int, --these come from player_stats\n",
    "        player varchar(50),\n",
    "        Age int,\n",
    "        team varchar(50),\n",
    "        points float, -- these come from player_stats\n",
    "        points_ly float,\n",
    "        change_points_ly float,\n",
    "        \n",
    "        starter_change int, -- these come from team_changes\n",
    "        high_usageplayer_added int,\n",
    "        usagemin_opened float,\n",
    "        maxusage_added float,\n",
    "        high_usageplayer_dropped int,\n",
    "        points_opened float,\n",
    "        max_pointsdropped float,\n",
    "        max_pointsadded float,\n",
    "        \n",
    "        three_ar_ly float, -- from player_advstats\n",
    "        change_3ar float,\n",
    "        per_ly float,\n",
    "        change_per float,\n",
    "        usagerank float,\n",
    "        usagerank_ly float,\n",
    "        offensive_winshares float,\n",
    "        offensive_boxplusminus float,\n",
    "        boxplusminus float,\n",
    "        value_overreplacement float,\n",
    "        \n",
    "        career_points float,\n",
    "        yearspro int\n",
    "        );\n",
    "        \n",
    "        INSERT INTO points_pred(season,player,age,team,points,points_ly,change_points_ly,starter_change)\n",
    "        SELECT season,player,age,startingteam,points,points_ly,change_points_ly,starter-starter_ly from player_stats;\n",
    "        \n",
    "        update points_pred pp\n",
    "        set high_usageplayer_added = tc.high_usageplayer_added,usagemin_opened=tc.usagemin_opened,\n",
    "        maxusage_added=tc.max_usageadded,high_usageplayer_dropped=tc.high_usageplayer_dropped,points_opened=tc.points_opened,\n",
    "        max_pointsdropped=tc.max_pointsdropped,max_pointsadded=tc.max_pointsadded\n",
    "        from team_changes tc\n",
    "        where tc.team = pp.team and pp.season=tc.season;\n",
    "        \n",
    "        update points_pred pp\n",
    "        set three_ar_ly = pa.threear_ly,change_3ar=pa.change_3ar,per_ly=pa.per_ly,change_per=pa.change_per,\n",
    "        usagerank=pa.usagerank,usagerank_ly=pa.usagerank_ly,offensive_winshares=pa.offensive_winshares,\n",
    "        offensive_boxplusminus=pa.offensive_boxplusminus,boxplusminus=pa.boxplusminus,value_overreplacement=pa.value_overreplacement\n",
    "        from player_advstats pa\n",
    "        where pp.player = pa.player and pp.season = pa.season and pp.team = pa.startingteam;\n",
    "        \n",
    "        update points_pred pp\n",
    "        set career_points = pc.career_points, yearspro = pc.yearspro\n",
    "        from player_careerstats pc\n",
    "        where pp.player = pc.player and pp.season = pc.season;\n",
    "        \n",
    "        \n",
    "        select * from points_pred where season>2009\n",
    "        '''\n",
    "\n",
    "\n",
    "cur.execute(query)\n",
    "data = cur.fetchall()\n",
    "points_df = pd.DataFrame(np.array(data))\n",
    "points_df.columns = ['season','player','age','team','points','points_ly','change_points_ly','starter_change','high_usageplayer_added','usagemin_opened','maxusage_added','high_usageplayer_dropped','points_opened','max_pointsdropped',\n",
    "                    'max_pointsadded','three_ar_ly','change_3ar','per_ly','change_per','usagerank','usagerank_ly','offensive_winshares','offensive_boxplusminus','boxplusminus','value_overreplacement','career_points','yearspro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rookies = points_df[points_df['points_ly'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>player</th>\n",
       "      <th>age</th>\n",
       "      <th>team</th>\n",
       "      <th>points</th>\n",
       "      <th>points_ly</th>\n",
       "      <th>change_points_ly</th>\n",
       "      <th>starter_change</th>\n",
       "      <th>high_usageplayer_added</th>\n",
       "      <th>usagemin_opened</th>\n",
       "      <th>...</th>\n",
       "      <th>per_ly</th>\n",
       "      <th>change_per</th>\n",
       "      <th>usagerank</th>\n",
       "      <th>usagerank_ly</th>\n",
       "      <th>offensive_winshares</th>\n",
       "      <th>offensive_boxplusminus</th>\n",
       "      <th>boxplusminus</th>\n",
       "      <th>value_overreplacement</th>\n",
       "      <th>career_points</th>\n",
       "      <th>yearspro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2010</td>\n",
       "      <td>Blake Griffin</td>\n",
       "      <td>21</td>\n",
       "      <td>LAC</td>\n",
       "      <td>22.5</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>-80.9551</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>2017</td>\n",
       "      <td>Donovan Mitchell</td>\n",
       "      <td>21</td>\n",
       "      <td>UTA</td>\n",
       "      <td>20.5</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>417.038</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>2016</td>\n",
       "      <td>Joel Embiid</td>\n",
       "      <td>22</td>\n",
       "      <td>PHI</td>\n",
       "      <td>20.2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>376.798</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2737</th>\n",
       "      <td>2017</td>\n",
       "      <td>MarShon Brooks</td>\n",
       "      <td>29</td>\n",
       "      <td>MEM</td>\n",
       "      <td>20.1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>710.049</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>8.13333</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>2012</td>\n",
       "      <td>Damian Lillard</td>\n",
       "      <td>22</td>\n",
       "      <td>POR</td>\n",
       "      <td>19</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>2011</td>\n",
       "      <td>Kyrie Irving</td>\n",
       "      <td>19</td>\n",
       "      <td>CLE</td>\n",
       "      <td>18.5</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>368.837</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>2015</td>\n",
       "      <td>Karl-Anthony Towns</td>\n",
       "      <td>20</td>\n",
       "      <td>MIN</td>\n",
       "      <td>18.3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>2015</td>\n",
       "      <td>Jahlil Okafor</td>\n",
       "      <td>20</td>\n",
       "      <td>PHI</td>\n",
       "      <td>17.5</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1223.52</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2014</td>\n",
       "      <td>Andrew Wiggins</td>\n",
       "      <td>19</td>\n",
       "      <td>MIN</td>\n",
       "      <td>16.9</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1160.41</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>2013</td>\n",
       "      <td>Michael Carter-Williams</td>\n",
       "      <td>22</td>\n",
       "      <td>PHI</td>\n",
       "      <td>16.7</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>2010</td>\n",
       "      <td>John Wall</td>\n",
       "      <td>20</td>\n",
       "      <td>WAS</td>\n",
       "      <td>16.4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2051.41</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>2010</td>\n",
       "      <td>Jordan Crawford</td>\n",
       "      <td>22</td>\n",
       "      <td>WAS</td>\n",
       "      <td>16.3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2051.41</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>2017</td>\n",
       "      <td>Kyle Kuzma</td>\n",
       "      <td>22</td>\n",
       "      <td>LAL</td>\n",
       "      <td>16.1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>301.256</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>2013</td>\n",
       "      <td>Derrick Rose</td>\n",
       "      <td>25</td>\n",
       "      <td>CHI</td>\n",
       "      <td>15.9</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1261.04</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>21.1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2017</td>\n",
       "      <td>Ben Simmons</td>\n",
       "      <td>21</td>\n",
       "      <td>PHI</td>\n",
       "      <td>15.8</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>-248.984</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3841</th>\n",
       "      <td>2016</td>\n",
       "      <td>Wilson Chandler</td>\n",
       "      <td>29</td>\n",
       "      <td>DEN</td>\n",
       "      <td>15.7</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>13.7143</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>2017</td>\n",
       "      <td>Dennis Smith</td>\n",
       "      <td>20</td>\n",
       "      <td>DAL</td>\n",
       "      <td>15.2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>2017</td>\n",
       "      <td>Lauri Markkanen</td>\n",
       "      <td>20</td>\n",
       "      <td>CHI</td>\n",
       "      <td>15.2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>3188.93</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>2012</td>\n",
       "      <td>Dion Waiters</td>\n",
       "      <td>21</td>\n",
       "      <td>CLE</td>\n",
       "      <td>14.7</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>670.09</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>2015</td>\n",
       "      <td>Kristaps Porzingis</td>\n",
       "      <td>20</td>\n",
       "      <td>NYK</td>\n",
       "      <td>14.3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>166.672</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>2016</td>\n",
       "      <td>Jordan Crawford</td>\n",
       "      <td>28</td>\n",
       "      <td>NOP</td>\n",
       "      <td>14.1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>95.7075</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>13.15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>2010</td>\n",
       "      <td>DeMarcus Cousins</td>\n",
       "      <td>20</td>\n",
       "      <td>SAC</td>\n",
       "      <td>14.1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>-108.86</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2012</td>\n",
       "      <td>Bradley Beal</td>\n",
       "      <td>19</td>\n",
       "      <td>WAS</td>\n",
       "      <td>13.9</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>-538.243</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>2017</td>\n",
       "      <td>Jayson Tatum</td>\n",
       "      <td>19</td>\n",
       "      <td>BOS</td>\n",
       "      <td>13.9</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>167.141</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>2013</td>\n",
       "      <td>Victor Oladipo</td>\n",
       "      <td>21</td>\n",
       "      <td>ORL</td>\n",
       "      <td>13.8</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>2015</td>\n",
       "      <td>Devin Booker</td>\n",
       "      <td>19</td>\n",
       "      <td>PHO</td>\n",
       "      <td>13.8</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>920.979</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2012</td>\n",
       "      <td>Anthony Davis</td>\n",
       "      <td>19</td>\n",
       "      <td>NOP</td>\n",
       "      <td>13.5</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2890.84</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>2015</td>\n",
       "      <td>D'Angelo Russell</td>\n",
       "      <td>19</td>\n",
       "      <td>LAL</td>\n",
       "      <td>13.2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>799.055</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>2017</td>\n",
       "      <td>Josh Jackson</td>\n",
       "      <td>20</td>\n",
       "      <td>PHO</td>\n",
       "      <td>13.1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>865.843</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1738</th>\n",
       "      <td>2011</td>\n",
       "      <td>Gerald Green</td>\n",
       "      <td>26</td>\n",
       "      <td>NJN</td>\n",
       "      <td>12.9</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>-152.302</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1821</th>\n",
       "      <td>2013</td>\n",
       "      <td>Hamady N'Diaye</td>\n",
       "      <td>27</td>\n",
       "      <td>SAC</td>\n",
       "      <td>0.4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>-80.1364</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.45</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969</th>\n",
       "      <td>2013</td>\n",
       "      <td>Mustafa Shakur</td>\n",
       "      <td>29</td>\n",
       "      <td>OKC</td>\n",
       "      <td>0.3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>2012</td>\n",
       "      <td>Jarvis Varnado</td>\n",
       "      <td>24</td>\n",
       "      <td>MIA</td>\n",
       "      <td>0.3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>2011</td>\n",
       "      <td>Malcolm Thomas</td>\n",
       "      <td>23</td>\n",
       "      <td>SAS</td>\n",
       "      <td>0.3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3825</th>\n",
       "      <td>2012</td>\n",
       "      <td>Will Conroy</td>\n",
       "      <td>30</td>\n",
       "      <td>MIN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1150.36</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>2013</td>\n",
       "      <td>Erik Murphy</td>\n",
       "      <td>23</td>\n",
       "      <td>CHI</td>\n",
       "      <td>0.3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1261.04</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>2013</td>\n",
       "      <td>Chris Smith</td>\n",
       "      <td>26</td>\n",
       "      <td>NYK</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>2017</td>\n",
       "      <td>Tyler Lydon</td>\n",
       "      <td>21</td>\n",
       "      <td>DEN</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>-911.481</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2016</td>\n",
       "      <td>Danuel House</td>\n",
       "      <td>23</td>\n",
       "      <td>WAS</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>644.698</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2012</td>\n",
       "      <td>Darius Johnson-Odom</td>\n",
       "      <td>23</td>\n",
       "      <td>LAL</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>-2893.01</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2017</td>\n",
       "      <td>Chris Boucher</td>\n",
       "      <td>25</td>\n",
       "      <td>GSW</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>-217.758</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3848</th>\n",
       "      <td>2017</td>\n",
       "      <td>Xavier Silas</td>\n",
       "      <td>30</td>\n",
       "      <td>BOS</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>167.141</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>2014</td>\n",
       "      <td>David Wear</td>\n",
       "      <td>24</td>\n",
       "      <td>SAC</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>209.469</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>2015</td>\n",
       "      <td>Sam Dekker</td>\n",
       "      <td>21</td>\n",
       "      <td>HOU</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>2017</td>\n",
       "      <td>Trey McKinney-Jones</td>\n",
       "      <td>27</td>\n",
       "      <td>IND</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>-505.421</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>2013</td>\n",
       "      <td>Elias Harris</td>\n",
       "      <td>24</td>\n",
       "      <td>LAL</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>-119.777</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2017</td>\n",
       "      <td>Ben Moore</td>\n",
       "      <td>22</td>\n",
       "      <td>IND</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>-505.421</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>2017</td>\n",
       "      <td>Erik McCree</td>\n",
       "      <td>24</td>\n",
       "      <td>UTA</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>417.038</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>2010</td>\n",
       "      <td>Gani Lawal</td>\n",
       "      <td>22</td>\n",
       "      <td>PHO</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>826.877</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>2010</td>\n",
       "      <td>Hassan Whiteside</td>\n",
       "      <td>21</td>\n",
       "      <td>SAC</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>-108.86</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>2015</td>\n",
       "      <td>J.J. O'Brien</td>\n",
       "      <td>23</td>\n",
       "      <td>UTA</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2016</td>\n",
       "      <td>Ben Bentil</td>\n",
       "      <td>21</td>\n",
       "      <td>DAL</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1043.97</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2718</th>\n",
       "      <td>2014</td>\n",
       "      <td>Malcolm Lee</td>\n",
       "      <td>24</td>\n",
       "      <td>PHI</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2108.4</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2680</th>\n",
       "      <td>2017</td>\n",
       "      <td>Luis Montero</td>\n",
       "      <td>24</td>\n",
       "      <td>DET</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>79.8685</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>2014</td>\n",
       "      <td>Kalin Lucas</td>\n",
       "      <td>25</td>\n",
       "      <td>MEM</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>-1002.27</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>2011</td>\n",
       "      <td>Keith Benson</td>\n",
       "      <td>23</td>\n",
       "      <td>GSW</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>-537.717</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>2013</td>\n",
       "      <td>Seth Curry</td>\n",
       "      <td>23</td>\n",
       "      <td>MEM</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>-103.154</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>2016</td>\n",
       "      <td>Patricio Garino</td>\n",
       "      <td>23</td>\n",
       "      <td>ORL</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>541.145</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>2013</td>\n",
       "      <td>Royce White</td>\n",
       "      <td>22</td>\n",
       "      <td>SAC</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>-80.1364</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>2014</td>\n",
       "      <td>Jerrelle Benimon</td>\n",
       "      <td>23</td>\n",
       "      <td>UTA</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>569.067</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>791 rows  27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     season                   player age team points points_ly  \\\n",
       "58     2010            Blake Griffin  21  LAC   22.5      None   \n",
       "180    2017         Donovan Mitchell  21  UTA   20.5      None   \n",
       "322    2016              Joel Embiid  22  PHI   20.2      None   \n",
       "2737   2017           MarShon Brooks  29  MEM   20.1      None   \n",
       "121    2012           Damian Lillard  22  POR     19      None   \n",
       "403    2011             Kyrie Irving  19  CLE   18.5      None   \n",
       "373    2015       Karl-Anthony Towns  20  MIN   18.3      None   \n",
       "270    2015            Jahlil Okafor  20  PHI   17.5      None   \n",
       "33     2014           Andrew Wiggins  19  MIN   16.9      None   \n",
       "463    2013  Michael Carter-Williams  22  PHI   16.7      None   \n",
       "329    2010                John Wall  20  WAS   16.4      None   \n",
       "341    2010          Jordan Crawford  22  WAS   16.3      None   \n",
       "399    2017               Kyle Kuzma  22  LAL   16.1      None   \n",
       "1479   2013             Derrick Rose  25  CHI   15.9      None   \n",
       "39     2017              Ben Simmons  21  PHI   15.8      None   \n",
       "3841   2016          Wilson Chandler  29  DEN   15.7      None   \n",
       "156    2017             Dennis Smith  20  DAL   15.2      None   \n",
       "413    2017          Lauri Markkanen  20  CHI   15.2      None   \n",
       "174    2012             Dion Waiters  21  CLE   14.7      None   \n",
       "396    2015       Kristaps Porzingis  20  NYK   14.3      None   \n",
       "2300   2016          Jordan Crawford  28  NOP   14.1      None   \n",
       "149    2010         DeMarcus Cousins  20  SAC   14.1      None   \n",
       "63     2012             Bradley Beal  19  WAS   13.9      None   \n",
       "298    2017             Jayson Tatum  19  BOS   13.9      None   \n",
       "649    2013           Victor Oladipo  21  ORL   13.8      None   \n",
       "164    2015             Devin Booker  19  PHO   13.8      None   \n",
       "38     2012            Anthony Davis  19  NOP   13.5      None   \n",
       "115    2015         D'Angelo Russell  19  LAL   13.2      None   \n",
       "354    2017             Josh Jackson  20  PHO   13.1      None   \n",
       "1738   2011             Gerald Green  26  NJN   12.9      None   \n",
       "...     ...                      ...  ..  ...    ...       ...   \n",
       "1821   2013           Hamady N'Diaye  27  SAC    0.4      None   \n",
       "2969   2013           Mustafa Shakur  29  OKC    0.3      None   \n",
       "294    2012           Jarvis Varnado  24  MIA    0.3      None   \n",
       "434    2011           Malcolm Thomas  23  SAS    0.3      None   \n",
       "3825   2012              Will Conroy  30  MIN    0.3      None   \n",
       "208    2013              Erik Murphy  23  CHI    0.3      None   \n",
       "101    2013              Chris Smith  26  NYK      0      None   \n",
       "639    2017              Tyler Lydon  21  DEN      0      None   \n",
       "132    2016             Danuel House  23  WAS      0      None   \n",
       "135    2012      Darius Johnson-Odom  23  LAL      0      None   \n",
       "95     2017            Chris Boucher  25  GSW      0      None   \n",
       "3848   2017             Xavier Silas  30  BOS      0      None   \n",
       "143    2014               David Wear  24  SAC      0      None   \n",
       "565    2015               Sam Dekker  21  HOU      0      None   \n",
       "629    2017      Trey McKinney-Jones  27  IND      0      None   \n",
       "198    2013             Elias Harris  24  LAL      0      None   \n",
       "55     2017                Ben Moore  22  IND      0      None   \n",
       "207    2017              Erik McCree  24  UTA      0      None   \n",
       "221    2010               Gani Lawal  22  PHO      0      None   \n",
       "243    2010         Hassan Whiteside  21  SAC      0      None   \n",
       "259    2015             J.J. O'Brien  23  UTA      0      None   \n",
       "52     2016               Ben Bentil  21  DAL      0      None   \n",
       "2718   2014              Malcolm Lee  24  PHI      0      None   \n",
       "2680   2017             Luis Montero  24  DET      0      None   \n",
       "372    2014              Kalin Lucas  25  MEM      0      None   \n",
       "377    2011             Keith Benson  23  GSW      0      None   \n",
       "573    2013               Seth Curry  23  MEM      0      None   \n",
       "520    2016          Patricio Garino  23  ORL      0      None   \n",
       "558    2013              Royce White  22  SAC      0      None   \n",
       "313    2014         Jerrelle Benimon  23  UTA      0      None   \n",
       "\n",
       "     change_points_ly starter_change high_usageplayer_added usagemin_opened  \\\n",
       "58               None           None                      0        -80.9551   \n",
       "180              None           None                      0         417.038   \n",
       "322              None           None                      0         376.798   \n",
       "2737             None           None                      0         710.049   \n",
       "121              None           None                   None            None   \n",
       "403              None           None                      0         368.837   \n",
       "373              None           None                   None            None   \n",
       "270              None           None                      0         1223.52   \n",
       "33               None           None                      0         1160.41   \n",
       "463              None           None                   None            None   \n",
       "329              None           None                      0         2051.41   \n",
       "341              None           None                      0         2051.41   \n",
       "399              None           None                      0         301.256   \n",
       "1479             None           None                      0         1261.04   \n",
       "39               None           None                      0        -248.984   \n",
       "3841             None           None                   None            None   \n",
       "156              None           None                   None            None   \n",
       "413              None           None                      0         3188.93   \n",
       "174              None           None                      0          670.09   \n",
       "396              None           None                      0         166.672   \n",
       "2300             None           None                      0         95.7075   \n",
       "149              None           None                      0         -108.86   \n",
       "63               None           None                      0        -538.243   \n",
       "298              None           None                      1         167.141   \n",
       "649              None           None                   None            None   \n",
       "164              None           None                      0         920.979   \n",
       "38               None           None                      0         2890.84   \n",
       "115              None           None                      0         799.055   \n",
       "354              None           None                      0         865.843   \n",
       "1738             None           None                      0        -152.302   \n",
       "...               ...            ...                    ...             ...   \n",
       "1821             None           None                      0        -80.1364   \n",
       "2969             None           None                   None            None   \n",
       "294              None           None                   None            None   \n",
       "434              None           None                   None            None   \n",
       "3825             None           None                      0         1150.36   \n",
       "208              None           None                      0         1261.04   \n",
       "101              None           None                   None            None   \n",
       "639              None           None                      0        -911.481   \n",
       "132              None           None                      0         644.698   \n",
       "135              None           None                      1        -2893.01   \n",
       "95               None           None                      0        -217.758   \n",
       "3848             None           None                      1         167.141   \n",
       "143              None           None                      0         209.469   \n",
       "565              None           None                   None            None   \n",
       "629              None           None                      0        -505.421   \n",
       "198              None           None                      0        -119.777   \n",
       "55               None           None                      0        -505.421   \n",
       "207              None           None                      0         417.038   \n",
       "221              None           None                      0         826.877   \n",
       "243              None           None                      0         -108.86   \n",
       "259              None           None                   None            None   \n",
       "52               None           None                      0         1043.97   \n",
       "2718             None           None                      0          2108.4   \n",
       "2680             None           None                      0         79.8685   \n",
       "372              None           None                      0        -1002.27   \n",
       "377              None           None                      0        -537.717   \n",
       "573              None           None                      0        -103.154   \n",
       "520              None           None                      0         541.145   \n",
       "558              None           None                      0        -80.1364   \n",
       "313              None           None                      0         569.067   \n",
       "\n",
       "       ...    per_ly change_per usagerank usagerank_ly offensive_winshares  \\\n",
       "58     ...      None       None        13            1                None   \n",
       "180    ...      None       None        15            1                None   \n",
       "322    ...      None       None        15            1                None   \n",
       "2737   ...      None       None        21            1                None   \n",
       "121    ...      None       None        17            1                None   \n",
       "403    ...      None       None        27            1                None   \n",
       "373    ...      None       None        29            1                None   \n",
       "270    ...      None       None        18            1                None   \n",
       "33     ...      None       None        21            1                None   \n",
       "463    ...      None       None        17            1                None   \n",
       "329    ...      None       None        24            1                None   \n",
       "341    ...      None       None        24            1                None   \n",
       "399    ...      None       None        22            1                None   \n",
       "1479   ...      None       None        10            1                None   \n",
       "39     ...      None       None        16            1                None   \n",
       "3841   ...      None       None        23            1                None   \n",
       "156    ...      None       None        18            1                None   \n",
       "413    ...      None       None        17            1                None   \n",
       "174    ...      None       None        21            1                None   \n",
       "396    ...      None       None        26            1                None   \n",
       "2300   ...      None       None        27            1                None   \n",
       "149    ...      None       None        19            1                None   \n",
       "63     ...      None       None        21            1                None   \n",
       "298    ...      None       None        12            1                None   \n",
       "649    ...      None       None        20            1                None   \n",
       "164    ...      None       None        17            1                None   \n",
       "38     ...      None       None        11            1                None   \n",
       "115    ...      None       None        13            1                None   \n",
       "354    ...      None       None        13            1                None   \n",
       "1738   ...      None       None        29            1                None   \n",
       "...    ...       ...        ...       ...          ...                 ...   \n",
       "1821   ...      None       None        15            1                None   \n",
       "2969   ...      None       None        24            1                None   \n",
       "294    ...      None       None        20            1                None   \n",
       "434    ...      None       None        14            1                None   \n",
       "3825   ...      None       None        25            1                None   \n",
       "208    ...      None       None        10            1                None   \n",
       "101    ...      None       None        21            1                None   \n",
       "639    ...      None       None        17            1                None   \n",
       "132    ...      None       None        22            1                None   \n",
       "135    ...      None       None        19            1                None   \n",
       "95     ...      None       None        21            1                None   \n",
       "3848   ...      None       None        12            1                None   \n",
       "143    ...      None       None        28            1                None   \n",
       "565    ...      None       None        29            1                None   \n",
       "629    ...      None       None        26            1                None   \n",
       "198    ...      None       None        17            1                None   \n",
       "55     ...      None       None        26            1                None   \n",
       "207    ...      None       None        15            1                None   \n",
       "221    ...      None       None        16            1                None   \n",
       "243    ...      None       None        19            1                None   \n",
       "259    ...      None       None        17            1                None   \n",
       "52     ...      None       None        14            1                None   \n",
       "2718   ...      None       None        22            1                None   \n",
       "2680   ...      None       None        18            1                None   \n",
       "372    ...      None       None        27            1                None   \n",
       "377    ...      None       None        25            1                None   \n",
       "573    ...      None       None        19            1                None   \n",
       "520    ...      None       None        30            1                None   \n",
       "558    ...      None       None        15            1                None   \n",
       "313    ...      None       None        13            1                None   \n",
       "\n",
       "     offensive_boxplusminus boxplusminus value_overreplacement career_points  \\\n",
       "58                     None         None                  None          None   \n",
       "180                    None         None                  None          None   \n",
       "322                    None         None                  None          None   \n",
       "2737                   None         None                  None       8.13333   \n",
       "121                    None         None                  None          None   \n",
       "403                    None         None                  None          None   \n",
       "373                    None         None                  None          None   \n",
       "270                    None         None                  None          None   \n",
       "33                     None         None                  None          None   \n",
       "463                    None         None                  None          None   \n",
       "329                    None         None                  None          None   \n",
       "341                    None         None                  None          None   \n",
       "399                    None         None                  None          None   \n",
       "1479                   None         None                  None          21.1   \n",
       "39                     None         None                  None          None   \n",
       "3841                   None         None                  None       13.7143   \n",
       "156                    None         None                  None          None   \n",
       "413                    None         None                  None          None   \n",
       "174                    None         None                  None          None   \n",
       "396                    None         None                  None          None   \n",
       "2300                   None         None                  None         13.15   \n",
       "149                    None         None                  None          None   \n",
       "63                     None         None                  None          None   \n",
       "298                    None         None                  None          None   \n",
       "649                    None         None                  None          None   \n",
       "164                    None         None                  None          None   \n",
       "38                     None         None                  None          None   \n",
       "115                    None         None                  None          None   \n",
       "354                    None         None                  None          None   \n",
       "1738                   None         None                  None           5.2   \n",
       "...                     ...          ...                   ...           ...   \n",
       "1821                   None         None                  None          0.45   \n",
       "2969                   None         None                  None           2.3   \n",
       "294                    None         None                  None          None   \n",
       "434                    None         None                  None          None   \n",
       "3825                   None         None                  None           1.2   \n",
       "208                    None         None                  None          None   \n",
       "101                    None         None                  None          None   \n",
       "639                    None         None                  None          None   \n",
       "132                    None         None                  None          None   \n",
       "135                    None         None                  None          None   \n",
       "95                     None         None                  None          None   \n",
       "3848                   None         None                  None           5.5   \n",
       "143                    None         None                  None          None   \n",
       "565                    None         None                  None          None   \n",
       "629                    None         None                  None          None   \n",
       "198                    None         None                  None          None   \n",
       "55                     None         None                  None          None   \n",
       "207                    None         None                  None          None   \n",
       "221                    None         None                  None          None   \n",
       "243                    None         None                  None          None   \n",
       "259                    None         None                  None          None   \n",
       "52                     None         None                  None          None   \n",
       "2718                   None         None                  None           4.1   \n",
       "2680                   None         None                  None           1.2   \n",
       "372                    None         None                  None          None   \n",
       "377                    None         None                  None          None   \n",
       "573                    None         None                  None          None   \n",
       "520                    None         None                  None          None   \n",
       "558                    None         None                  None          None   \n",
       "313                    None         None                  None          None   \n",
       "\n",
       "     yearspro  \n",
       "58       None  \n",
       "180      None  \n",
       "322      None  \n",
       "2737        3  \n",
       "121      None  \n",
       "403      None  \n",
       "373      None  \n",
       "270      None  \n",
       "33       None  \n",
       "463      None  \n",
       "329      None  \n",
       "341      None  \n",
       "399      None  \n",
       "1479        4  \n",
       "39       None  \n",
       "3841        7  \n",
       "156      None  \n",
       "413      None  \n",
       "174      None  \n",
       "396      None  \n",
       "2300        4  \n",
       "149      None  \n",
       "63       None  \n",
       "298      None  \n",
       "649      None  \n",
       "164      None  \n",
       "38       None  \n",
       "115      None  \n",
       "354      None  \n",
       "1738        1  \n",
       "...       ...  \n",
       "1821        2  \n",
       "2969        1  \n",
       "294      None  \n",
       "434      None  \n",
       "3825        1  \n",
       "208      None  \n",
       "101      None  \n",
       "639      None  \n",
       "132      None  \n",
       "135      None  \n",
       "95       None  \n",
       "3848        1  \n",
       "143      None  \n",
       "565      None  \n",
       "629      None  \n",
       "198      None  \n",
       "55       None  \n",
       "207      None  \n",
       "221      None  \n",
       "243      None  \n",
       "259      None  \n",
       "52       None  \n",
       "2718        2  \n",
       "2680        1  \n",
       "372      None  \n",
       "377      None  \n",
       "573      None  \n",
       "520      None  \n",
       "558      None  \n",
       "313      None  \n",
       "\n",
       "[791 rows x 27 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rookies.sort_values(by='points',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = points_df[points_df['points_ly'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "for i in players.columns:\n",
    "    if i not in(['player','team']):\n",
    "        players[i]=pd.to_numeric(players[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = players.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = players['points']\n",
    "X = players.drop(['points','player','season','team'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function connection.close>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.close()\n",
    "conn.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression error: 0.7664521068339786\n",
      "GradientBoost error: 0.7665097955425865\n",
      "RandomForest error: 0.7018651757258578\n"
     ]
    }
   ],
   "source": [
    "r2_lr = np.mean(cross_val_score(LinearRegression(),X_train,y_train,cv=10,n_jobs=-1))\n",
    "print('Linear Regression error: {}'.format(r2_lr))\n",
    "\n",
    "r2_gb = np.mean(cross_val_score(GradientBoostingRegressor(learning_rate=0.01,n_estimators=1000,max_depth = 3,max_features=0.5),X_train,y_train,cv=5,n_jobs=-1))\n",
    "print('GradientBoost error: {}'.format(r2_gb))\n",
    "\n",
    "r2_rf = np.mean(cross_val_score(RandomForestRegressor(n_estimators=1000,max_depth = 3,max_features=0.5),X_train,y_train,cv=5,n_jobs=-1))\n",
    "print('RandomForest error: {}'.format(r2_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "[CV] learning_rate=0.01, max_depth=3, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=3, max_features=0.1, n_estimators=1000, score=0.7495924120019375, total=   0.3s\n",
      "[CV] learning_rate=0.01, max_depth=3, max_features=0.1, n_estimators=1000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=0.01, max_depth=3, max_features=0.1, n_estimators=1000, score=0.7476594944544669, total=   0.3s\n",
      "[CV] learning_rate=0.01, max_depth=3, max_features=0.1, n_estimators=1000 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  learning_rate=0.01, max_depth=3, max_features=0.1, n_estimators=1000, score=0.7943399658158287, total=   0.3s\n",
      "[CV] learning_rate=0.01, max_depth=3, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=3, max_features=0.1, n_estimators=1000, score=0.7190431458804185, total=   0.3s\n",
      "[CV] learning_rate=0.01, max_depth=3, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=3, max_features=0.1, n_estimators=1000, score=0.7222216933630867, total=   0.3s\n",
      "[CV] learning_rate=0.01, max_depth=3, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=3, max_features=0.1, n_estimators=2000, score=0.7627053638805009, total=   0.6s\n",
      "[CV] learning_rate=0.01, max_depth=3, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=3, max_features=0.1, n_estimators=2000, score=0.7502206778411044, total=   0.6s\n",
      "[CV] learning_rate=0.01, max_depth=3, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=3, max_features=0.1, n_estimators=2000, score=0.8045909017875303, total=   0.6s\n",
      "[CV] learning_rate=0.01, max_depth=3, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=3, max_features=0.1, n_estimators=2000, score=0.7293798119305249, total=   0.6s\n",
      "[CV] learning_rate=0.01, max_depth=3, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=3, max_features=0.1, n_estimators=2000, score=0.7300588658432952, total=   0.6s\n",
      "[CV] learning_rate=0.01, max_depth=3, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=3, max_features=0.3, n_estimators=1000, score=0.7724619977012069, total=   0.5s\n",
      "[CV] learning_rate=0.01, max_depth=3, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=3, max_features=0.3, n_estimators=1000, score=0.7526974885743176, total=   0.5s\n",
      "[CV] learning_rate=0.01, max_depth=3, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=3, max_features=0.3, n_estimators=1000, score=0.810172956048633, total=   0.5s\n",
      "[CV] learning_rate=0.01, max_depth=3, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=3, max_features=0.3, n_estimators=1000, score=0.7464345262995764, total=   0.5s\n",
      "[CV] learning_rate=0.01, max_depth=3, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=3, max_features=0.3, n_estimators=1000, score=0.7359319991432365, total=   0.5s\n",
      "[CV] learning_rate=0.01, max_depth=3, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=3, max_features=0.3, n_estimators=2000, score=0.7714118315177116, total=   0.9s\n",
      "[CV] learning_rate=0.01, max_depth=3, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=3, max_features=0.3, n_estimators=2000, score=0.7484088434769929, total=   0.9s\n",
      "[CV] learning_rate=0.01, max_depth=3, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=3, max_features=0.3, n_estimators=2000, score=0.809351530051144, total=   0.9s\n",
      "[CV] learning_rate=0.01, max_depth=3, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=3, max_features=0.3, n_estimators=2000, score=0.7396221339062757, total=   0.9s\n",
      "[CV] learning_rate=0.01, max_depth=3, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=3, max_features=0.3, n_estimators=2000, score=0.7360800496568037, total=   1.0s\n",
      "[CV] learning_rate=0.01, max_depth=3, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=3, max_features=0.5, n_estimators=1000, score=0.7782412972206614, total=   0.7s\n",
      "[CV] learning_rate=0.01, max_depth=3, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=3, max_features=0.5, n_estimators=1000, score=0.7534142959553877, total=   0.7s\n",
      "[CV] learning_rate=0.01, max_depth=3, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=3, max_features=0.5, n_estimators=1000, score=0.8076397951828361, total=   0.7s\n",
      "[CV] learning_rate=0.01, max_depth=3, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=3, max_features=0.5, n_estimators=1000, score=0.7526210846094401, total=   0.8s\n",
      "[CV] learning_rate=0.01, max_depth=3, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=3, max_features=0.5, n_estimators=1000, score=0.739283943797602, total=   0.8s\n",
      "[CV] learning_rate=0.01, max_depth=3, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=3, max_features=0.5, n_estimators=2000, score=0.7718861303857902, total=   1.4s\n",
      "[CV] learning_rate=0.01, max_depth=3, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=3, max_features=0.5, n_estimators=2000, score=0.7502934336936846, total=   1.3s\n",
      "[CV] learning_rate=0.01, max_depth=3, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=3, max_features=0.5, n_estimators=2000, score=0.8069350259476348, total=   1.3s\n",
      "[CV] learning_rate=0.01, max_depth=3, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=3, max_features=0.5, n_estimators=2000, score=0.7376255382516397, total=   1.4s\n",
      "[CV] learning_rate=0.01, max_depth=3, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=3, max_features=0.5, n_estimators=2000, score=0.737654204563374, total=   1.4s\n",
      "[CV] learning_rate=0.01, max_depth=5, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=5, max_features=0.1, n_estimators=1000, score=0.7626771339183323, total=   0.5s\n",
      "[CV] learning_rate=0.01, max_depth=5, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=5, max_features=0.1, n_estimators=1000, score=0.7465049831143591, total=   0.5s\n",
      "[CV] learning_rate=0.01, max_depth=5, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=5, max_features=0.1, n_estimators=1000, score=0.8018587343434184, total=   0.5s\n",
      "[CV] learning_rate=0.01, max_depth=5, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=5, max_features=0.1, n_estimators=1000, score=0.7303094382099727, total=   0.5s\n",
      "[CV] learning_rate=0.01, max_depth=5, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=5, max_features=0.1, n_estimators=1000, score=0.7286900382983863, total=   0.5s\n",
      "[CV] learning_rate=0.01, max_depth=5, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=5, max_features=0.1, n_estimators=2000, score=0.7601056582996399, total=   1.0s\n",
      "[CV] learning_rate=0.01, max_depth=5, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=5, max_features=0.1, n_estimators=2000, score=0.7428013575185071, total=   0.9s\n",
      "[CV] learning_rate=0.01, max_depth=5, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=5, max_features=0.1, n_estimators=2000, score=0.8003830497626752, total=   0.9s\n",
      "[CV] learning_rate=0.01, max_depth=5, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=5, max_features=0.1, n_estimators=2000, score=0.7290513105397022, total=   0.9s\n",
      "[CV] learning_rate=0.01, max_depth=5, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=5, max_features=0.1, n_estimators=2000, score=0.7293391964796927, total=   0.9s\n",
      "[CV] learning_rate=0.01, max_depth=5, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=5, max_features=0.3, n_estimators=1000, score=0.7770393060223986, total=   1.0s\n",
      "[CV] learning_rate=0.01, max_depth=5, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=5, max_features=0.3, n_estimators=1000, score=0.7515317599698752, total=   1.0s\n",
      "[CV] learning_rate=0.01, max_depth=5, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=5, max_features=0.3, n_estimators=1000, score=0.8057791616339796, total=   1.2s\n",
      "[CV] learning_rate=0.01, max_depth=5, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=5, max_features=0.3, n_estimators=1000, score=0.7403035333377772, total=   0.9s\n",
      "[CV] learning_rate=0.01, max_depth=5, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=5, max_features=0.3, n_estimators=1000, score=0.7380020790437171, total=   0.9s\n",
      "[CV] learning_rate=0.01, max_depth=5, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=5, max_features=0.3, n_estimators=2000, score=0.7661440685290489, total=   1.7s\n",
      "[CV] learning_rate=0.01, max_depth=5, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=5, max_features=0.3, n_estimators=2000, score=0.7396205227844399, total=   1.7s\n",
      "[CV] learning_rate=0.01, max_depth=5, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=5, max_features=0.3, n_estimators=2000, score=0.802302081648347, total=   1.7s\n",
      "[CV] learning_rate=0.01, max_depth=5, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=5, max_features=0.3, n_estimators=2000, score=0.7295044347022633, total=   1.8s\n",
      "[CV] learning_rate=0.01, max_depth=5, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=5, max_features=0.3, n_estimators=2000, score=0.7316890912641884, total=   1.8s\n",
      "[CV] learning_rate=0.01, max_depth=5, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=5, max_features=0.5, n_estimators=1000, score=0.7715801807194493, total=   1.4s\n",
      "[CV] learning_rate=0.01, max_depth=5, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=5, max_features=0.5, n_estimators=1000, score=0.7457471806092637, total=   1.4s\n",
      "[CV] learning_rate=0.01, max_depth=5, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=5, max_features=0.5, n_estimators=1000, score=0.8033399817639613, total=   1.4s\n",
      "[CV] learning_rate=0.01, max_depth=5, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=5, max_features=0.5, n_estimators=1000, score=0.744422152544153, total=   1.4s\n",
      "[CV] learning_rate=0.01, max_depth=5, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=5, max_features=0.5, n_estimators=1000, score=0.7393631477275849, total=   1.4s\n",
      "[CV] learning_rate=0.01, max_depth=5, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=5, max_features=0.5, n_estimators=2000, score=0.7603685317760063, total=   2.7s\n",
      "[CV] learning_rate=0.01, max_depth=5, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=5, max_features=0.5, n_estimators=2000, score=0.7374521750249801, total=   2.7s\n",
      "[CV] learning_rate=0.01, max_depth=5, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=5, max_features=0.5, n_estimators=2000, score=0.7991477867900119, total=   2.8s\n",
      "[CV] learning_rate=0.01, max_depth=5, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=5, max_features=0.5, n_estimators=2000, score=0.7310660946972016, total=   2.7s\n",
      "[CV] learning_rate=0.01, max_depth=5, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=5, max_features=0.5, n_estimators=2000, score=0.73103055543112, total=   2.7s\n",
      "[CV] learning_rate=0.01, max_depth=7, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=7, max_features=0.1, n_estimators=1000, score=0.7562111426304283, total=   0.9s\n",
      "[CV] learning_rate=0.01, max_depth=7, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=7, max_features=0.1, n_estimators=1000, score=0.7480430230577373, total=   0.8s\n",
      "[CV] learning_rate=0.01, max_depth=7, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=7, max_features=0.1, n_estimators=1000, score=0.7967011623928267, total=   0.8s\n",
      "[CV] learning_rate=0.01, max_depth=7, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=7, max_features=0.1, n_estimators=1000, score=0.7257278038368107, total=   0.8s\n",
      "[CV] learning_rate=0.01, max_depth=7, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=7, max_features=0.1, n_estimators=1000, score=0.7256501733403956, total=   0.8s\n",
      "[CV] learning_rate=0.01, max_depth=7, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=7, max_features=0.1, n_estimators=2000, score=0.7461836284262331, total=   1.5s\n",
      "[CV] learning_rate=0.01, max_depth=7, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=7, max_features=0.1, n_estimators=2000, score=0.7426505957356664, total=   1.5s\n",
      "[CV] learning_rate=0.01, max_depth=7, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=7, max_features=0.1, n_estimators=2000, score=0.7943318467730048, total=   1.5s\n",
      "[CV] learning_rate=0.01, max_depth=7, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=7, max_features=0.1, n_estimators=2000, score=0.7163064966131981, total=   1.5s\n",
      "[CV] learning_rate=0.01, max_depth=7, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=7, max_features=0.1, n_estimators=2000, score=0.7268958591275507, total=   1.6s\n",
      "[CV] learning_rate=0.01, max_depth=7, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=7, max_features=0.3, n_estimators=1000, score=0.7642667464108575, total=   1.6s\n",
      "[CV] learning_rate=0.01, max_depth=7, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=7, max_features=0.3, n_estimators=1000, score=0.7472279720786172, total=   1.6s\n",
      "[CV] learning_rate=0.01, max_depth=7, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=7, max_features=0.3, n_estimators=1000, score=0.8051508686829824, total=   1.6s\n",
      "[CV] learning_rate=0.01, max_depth=7, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=7, max_features=0.3, n_estimators=1000, score=0.730024428630686, total=   1.6s\n",
      "[CV] learning_rate=0.01, max_depth=7, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=7, max_features=0.3, n_estimators=1000, score=0.7321632082347167, total=   1.6s\n",
      "[CV] learning_rate=0.01, max_depth=7, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=7, max_features=0.3, n_estimators=2000, score=0.7570609920410137, total=   3.2s\n",
      "[CV] learning_rate=0.01, max_depth=7, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=7, max_features=0.3, n_estimators=2000, score=0.7422406018737014, total=   3.1s\n",
      "[CV] learning_rate=0.01, max_depth=7, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=7, max_features=0.3, n_estimators=2000, score=0.8025975188997343, total=   3.1s\n",
      "[CV] learning_rate=0.01, max_depth=7, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=7, max_features=0.3, n_estimators=2000, score=0.7321555092651384, total=   3.1s\n",
      "[CV] learning_rate=0.01, max_depth=7, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=7, max_features=0.3, n_estimators=2000, score=0.7271797914893012, total=   3.1s\n",
      "[CV] learning_rate=0.01, max_depth=7, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=7, max_features=0.5, n_estimators=1000, score=0.7624550271802718, total=   2.8s\n",
      "[CV] learning_rate=0.01, max_depth=7, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=7, max_features=0.5, n_estimators=1000, score=0.7422698693042158, total=   2.8s\n",
      "[CV] learning_rate=0.01, max_depth=7, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=7, max_features=0.5, n_estimators=1000, score=0.80269818476494, total=   2.6s\n",
      "[CV] learning_rate=0.01, max_depth=7, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=7, max_features=0.5, n_estimators=1000, score=0.7366475381642718, total=   2.7s\n",
      "[CV] learning_rate=0.01, max_depth=7, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=7, max_features=0.5, n_estimators=1000, score=0.7304025880552752, total=   2.6s\n",
      "[CV] learning_rate=0.01, max_depth=7, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=7, max_features=0.5, n_estimators=2000, score=0.7579714310310804, total=   5.0s\n",
      "[CV] learning_rate=0.01, max_depth=7, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=7, max_features=0.5, n_estimators=2000, score=0.7381936611530922, total=   5.0s\n",
      "[CV] learning_rate=0.01, max_depth=7, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=7, max_features=0.5, n_estimators=2000, score=0.7997123433740743, total=   4.9s\n",
      "[CV] learning_rate=0.01, max_depth=7, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=7, max_features=0.5, n_estimators=2000, score=0.729932346775635, total=   4.9s\n",
      "[CV] learning_rate=0.01, max_depth=7, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=7, max_features=0.5, n_estimators=2000, score=0.721586215979279, total=   4.9s\n",
      "[CV] learning_rate=0.01, max_depth=10, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=10, max_features=0.1, n_estimators=1000, score=0.737095505582872, total=   1.6s\n",
      "[CV] learning_rate=0.01, max_depth=10, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=10, max_features=0.1, n_estimators=1000, score=0.7391002109576411, total=   1.6s\n",
      "[CV] learning_rate=0.01, max_depth=10, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=10, max_features=0.1, n_estimators=1000, score=0.7821762588610093, total=   1.6s\n",
      "[CV] learning_rate=0.01, max_depth=10, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=10, max_features=0.1, n_estimators=1000, score=0.7153281788280829, total=   1.8s\n",
      "[CV] learning_rate=0.01, max_depth=10, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=10, max_features=0.1, n_estimators=1000, score=0.7123294115378661, total=   1.6s\n",
      "[CV] learning_rate=0.01, max_depth=10, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=10, max_features=0.1, n_estimators=2000, score=0.7394600909167903, total=   3.2s\n",
      "[CV] learning_rate=0.01, max_depth=10, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=10, max_features=0.1, n_estimators=2000, score=0.7397868483744654, total=   3.2s\n",
      "[CV] learning_rate=0.01, max_depth=10, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=10, max_features=0.1, n_estimators=2000, score=0.7868843929850418, total=   3.2s\n",
      "[CV] learning_rate=0.01, max_depth=10, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=10, max_features=0.1, n_estimators=2000, score=0.7230768853387237, total=   3.1s\n",
      "[CV] learning_rate=0.01, max_depth=10, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=10, max_features=0.1, n_estimators=2000, score=0.7163275204374132, total=   3.1s\n",
      "[CV] learning_rate=0.01, max_depth=10, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=10, max_features=0.3, n_estimators=1000, score=0.7546220342493819, total=   3.9s\n",
      "[CV] learning_rate=0.01, max_depth=10, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=10, max_features=0.3, n_estimators=1000, score=0.7452179324783312, total=   3.9s\n",
      "[CV] learning_rate=0.01, max_depth=10, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=10, max_features=0.3, n_estimators=1000, score=0.7971776444927601, total=   3.9s\n",
      "[CV] learning_rate=0.01, max_depth=10, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=10, max_features=0.3, n_estimators=1000, score=0.7299268435231488, total=   3.9s\n",
      "[CV] learning_rate=0.01, max_depth=10, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=10, max_features=0.3, n_estimators=1000, score=0.721997645297602, total=   3.8s\n",
      "[CV] learning_rate=0.01, max_depth=10, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=10, max_features=0.3, n_estimators=2000, score=0.7537602466103849, total=   6.7s\n",
      "[CV] learning_rate=0.01, max_depth=10, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=10, max_features=0.3, n_estimators=2000, score=0.7453613935960759, total=   6.9s\n",
      "[CV] learning_rate=0.01, max_depth=10, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=10, max_features=0.3, n_estimators=2000, score=0.7963422967116403, total=   7.3s\n",
      "[CV] learning_rate=0.01, max_depth=10, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=10, max_features=0.3, n_estimators=2000, score=0.731775920024456, total=   7.0s\n",
      "[CV] learning_rate=0.01, max_depth=10, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=10, max_features=0.3, n_estimators=2000, score=0.7192168588867147, total=   7.3s\n",
      "[CV] learning_rate=0.01, max_depth=10, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=10, max_features=0.5, n_estimators=1000, score=0.7521194216181062, total=   6.8s\n",
      "[CV] learning_rate=0.01, max_depth=10, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=10, max_features=0.5, n_estimators=1000, score=0.73591294700449, total=   6.6s\n",
      "[CV] learning_rate=0.01, max_depth=10, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=10, max_features=0.5, n_estimators=1000, score=0.8000873197537989, total=   6.5s\n",
      "[CV] learning_rate=0.01, max_depth=10, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=10, max_features=0.5, n_estimators=1000, score=0.7281733634094683, total=   6.7s\n",
      "[CV] learning_rate=0.01, max_depth=10, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.01, max_depth=10, max_features=0.5, n_estimators=1000, score=0.7275044178926261, total=   6.4s\n",
      "[CV] learning_rate=0.01, max_depth=10, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=10, max_features=0.5, n_estimators=2000, score=0.7545468244722808, total=  11.2s\n",
      "[CV] learning_rate=0.01, max_depth=10, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=10, max_features=0.5, n_estimators=2000, score=0.7373252340416532, total=  11.3s\n",
      "[CV] learning_rate=0.01, max_depth=10, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=10, max_features=0.5, n_estimators=2000, score=0.8003975458072936, total=  11.2s\n",
      "[CV] learning_rate=0.01, max_depth=10, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=10, max_features=0.5, n_estimators=2000, score=0.7251717171625662, total=  11.6s\n",
      "[CV] learning_rate=0.01, max_depth=10, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.01, max_depth=10, max_features=0.5, n_estimators=2000, score=0.7249564186650879, total=  11.5s\n",
      "[CV] learning_rate=0.05, max_depth=3, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=3, max_features=0.1, n_estimators=1000, score=0.752286846835078, total=   0.3s\n",
      "[CV] learning_rate=0.05, max_depth=3, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=3, max_features=0.1, n_estimators=1000, score=0.7344522867350258, total=   0.3s\n",
      "[CV] learning_rate=0.05, max_depth=3, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=3, max_features=0.1, n_estimators=1000, score=0.8002626878951357, total=   0.3s\n",
      "[CV] learning_rate=0.05, max_depth=3, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=3, max_features=0.1, n_estimators=1000, score=0.703864233587904, total=   0.3s\n",
      "[CV] learning_rate=0.05, max_depth=3, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=3, max_features=0.1, n_estimators=1000, score=0.7235337227035721, total=   0.3s\n",
      "[CV] learning_rate=0.05, max_depth=3, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=3, max_features=0.1, n_estimators=2000, score=0.7393922487573352, total=   0.6s\n",
      "[CV] learning_rate=0.05, max_depth=3, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=3, max_features=0.1, n_estimators=2000, score=0.7125520140158332, total=   0.6s\n",
      "[CV] learning_rate=0.05, max_depth=3, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=3, max_features=0.1, n_estimators=2000, score=0.79302259406663, total=   0.5s\n",
      "[CV] learning_rate=0.05, max_depth=3, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=3, max_features=0.1, n_estimators=2000, score=0.7073971620006783, total=   0.6s\n",
      "[CV] learning_rate=0.05, max_depth=3, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=3, max_features=0.1, n_estimators=2000, score=0.7119012948136328, total=   0.6s\n",
      "[CV] learning_rate=0.05, max_depth=3, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=3, max_features=0.3, n_estimators=1000, score=0.7559099138338337, total=   0.5s\n",
      "[CV] learning_rate=0.05, max_depth=3, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=3, max_features=0.3, n_estimators=1000, score=0.7319007998160488, total=   0.4s\n",
      "[CV] learning_rate=0.05, max_depth=3, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=3, max_features=0.3, n_estimators=1000, score=0.7992056979535583, total=   0.5s\n",
      "[CV] learning_rate=0.05, max_depth=3, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=3, max_features=0.3, n_estimators=1000, score=0.7203937644737937, total=   0.4s\n",
      "[CV] learning_rate=0.05, max_depth=3, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=3, max_features=0.3, n_estimators=1000, score=0.721137441584053, total=   0.5s\n",
      "[CV] learning_rate=0.05, max_depth=3, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=3, max_features=0.3, n_estimators=2000, score=0.7385908644620797, total=   1.0s\n",
      "[CV] learning_rate=0.05, max_depth=3, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=3, max_features=0.3, n_estimators=2000, score=0.7092442173441982, total=   0.9s\n",
      "[CV] learning_rate=0.05, max_depth=3, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=3, max_features=0.3, n_estimators=2000, score=0.7851112072120722, total=   1.0s\n",
      "[CV] learning_rate=0.05, max_depth=3, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=3, max_features=0.3, n_estimators=2000, score=0.6894879510843741, total=   0.9s\n",
      "[CV] learning_rate=0.05, max_depth=3, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=3, max_features=0.3, n_estimators=2000, score=0.7074554670306207, total=   0.9s\n",
      "[CV] learning_rate=0.05, max_depth=3, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=3, max_features=0.5, n_estimators=1000, score=0.7466439072911673, total=   0.7s\n",
      "[CV] learning_rate=0.05, max_depth=3, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=3, max_features=0.5, n_estimators=1000, score=0.7257824680827822, total=   0.7s\n",
      "[CV] learning_rate=0.05, max_depth=3, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=3, max_features=0.5, n_estimators=1000, score=0.8005423190440591, total=   0.7s\n",
      "[CV] learning_rate=0.05, max_depth=3, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=3, max_features=0.5, n_estimators=1000, score=0.7147742701837227, total=   0.7s\n",
      "[CV] learning_rate=0.05, max_depth=3, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=3, max_features=0.5, n_estimators=1000, score=0.7219265556585865, total=   0.7s\n",
      "[CV] learning_rate=0.05, max_depth=3, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=3, max_features=0.5, n_estimators=2000, score=0.729322513231452, total=   1.4s\n",
      "[CV] learning_rate=0.05, max_depth=3, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=3, max_features=0.5, n_estimators=2000, score=0.7082251376147489, total=   1.4s\n",
      "[CV] learning_rate=0.05, max_depth=3, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=3, max_features=0.5, n_estimators=2000, score=0.786368296727049, total=   1.4s\n",
      "[CV] learning_rate=0.05, max_depth=3, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=3, max_features=0.5, n_estimators=2000, score=0.6927225064421316, total=   1.4s\n",
      "[CV] learning_rate=0.05, max_depth=3, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=3, max_features=0.5, n_estimators=2000, score=0.7021674528402333, total=   1.4s\n",
      "[CV] learning_rate=0.05, max_depth=5, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=5, max_features=0.1, n_estimators=1000, score=0.735987099698128, total=   0.5s\n",
      "[CV] learning_rate=0.05, max_depth=5, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=5, max_features=0.1, n_estimators=1000, score=0.7163199673523517, total=   0.5s\n",
      "[CV] learning_rate=0.05, max_depth=5, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=5, max_features=0.1, n_estimators=1000, score=0.7837932451898368, total=   0.5s\n",
      "[CV] learning_rate=0.05, max_depth=5, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=5, max_features=0.1, n_estimators=1000, score=0.7048473971269863, total=   0.5s\n",
      "[CV] learning_rate=0.05, max_depth=5, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=5, max_features=0.1, n_estimators=1000, score=0.7286024372293955, total=   0.5s\n",
      "[CV] learning_rate=0.05, max_depth=5, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=5, max_features=0.1, n_estimators=2000, score=0.7434888704202895, total=   1.0s\n",
      "[CV] learning_rate=0.05, max_depth=5, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=5, max_features=0.1, n_estimators=2000, score=0.7199128185929571, total=   1.0s\n",
      "[CV] learning_rate=0.05, max_depth=5, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=5, max_features=0.1, n_estimators=2000, score=0.7876007854609404, total=   1.0s\n",
      "[CV] learning_rate=0.05, max_depth=5, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=5, max_features=0.1, n_estimators=2000, score=0.6945420460139555, total=   1.0s\n",
      "[CV] learning_rate=0.05, max_depth=5, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=5, max_features=0.1, n_estimators=2000, score=0.7055204482159698, total=   1.0s\n",
      "[CV] learning_rate=0.05, max_depth=5, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=5, max_features=0.3, n_estimators=1000, score=0.7510237368472615, total=   1.0s\n",
      "[CV] learning_rate=0.05, max_depth=5, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=5, max_features=0.3, n_estimators=1000, score=0.7303682814183251, total=   0.9s\n",
      "[CV] learning_rate=0.05, max_depth=5, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=5, max_features=0.3, n_estimators=1000, score=0.7875888975676699, total=   0.9s\n",
      "[CV] learning_rate=0.05, max_depth=5, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=5, max_features=0.3, n_estimators=1000, score=0.7086265574189083, total=   0.9s\n",
      "[CV] learning_rate=0.05, max_depth=5, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=5, max_features=0.3, n_estimators=1000, score=0.7244035036909662, total=   0.9s\n",
      "[CV] learning_rate=0.05, max_depth=5, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=5, max_features=0.3, n_estimators=2000, score=0.7393223717485395, total=   1.8s\n",
      "[CV] learning_rate=0.05, max_depth=5, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=5, max_features=0.3, n_estimators=2000, score=0.7229888788680818, total=   1.8s\n",
      "[CV] learning_rate=0.05, max_depth=5, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=5, max_features=0.3, n_estimators=2000, score=0.7914514342873987, total=   1.9s\n",
      "[CV] learning_rate=0.05, max_depth=5, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=5, max_features=0.3, n_estimators=2000, score=0.7119477121686841, total=   1.8s\n",
      "[CV] learning_rate=0.05, max_depth=5, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=5, max_features=0.3, n_estimators=2000, score=0.7131420118957748, total=   1.8s\n",
      "[CV] learning_rate=0.05, max_depth=5, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=5, max_features=0.5, n_estimators=1000, score=0.7483604041973191, total=   1.4s\n",
      "[CV] learning_rate=0.05, max_depth=5, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=5, max_features=0.5, n_estimators=1000, score=0.722014021511503, total=   1.4s\n",
      "[CV] learning_rate=0.05, max_depth=5, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=5, max_features=0.5, n_estimators=1000, score=0.7873754260611263, total=   1.5s\n",
      "[CV] learning_rate=0.05, max_depth=5, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=5, max_features=0.5, n_estimators=1000, score=0.7146158970835647, total=   1.4s\n",
      "[CV] learning_rate=0.05, max_depth=5, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=5, max_features=0.5, n_estimators=1000, score=0.7223065561849062, total=   1.4s\n",
      "[CV] learning_rate=0.05, max_depth=5, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=5, max_features=0.5, n_estimators=2000, score=0.7445193304153469, total=   2.9s\n",
      "[CV] learning_rate=0.05, max_depth=5, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=5, max_features=0.5, n_estimators=2000, score=0.7268449941649757, total=   2.9s\n",
      "[CV] learning_rate=0.05, max_depth=5, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=5, max_features=0.5, n_estimators=2000, score=0.7910472172522893, total=   2.8s\n",
      "[CV] learning_rate=0.05, max_depth=5, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=5, max_features=0.5, n_estimators=2000, score=0.7076979390667831, total=   2.9s\n",
      "[CV] learning_rate=0.05, max_depth=5, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=5, max_features=0.5, n_estimators=2000, score=0.7155123783998669, total=   3.1s\n",
      "[CV] learning_rate=0.05, max_depth=7, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=7, max_features=0.1, n_estimators=1000, score=0.7508683171448276, total=   0.8s\n",
      "[CV] learning_rate=0.05, max_depth=7, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=7, max_features=0.1, n_estimators=1000, score=0.7369347684284189, total=   0.8s\n",
      "[CV] learning_rate=0.05, max_depth=7, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=7, max_features=0.1, n_estimators=1000, score=0.7868665407329936, total=   0.8s\n",
      "[CV] learning_rate=0.05, max_depth=7, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=7, max_features=0.1, n_estimators=1000, score=0.7051506815535404, total=   0.8s\n",
      "[CV] learning_rate=0.05, max_depth=7, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=7, max_features=0.1, n_estimators=1000, score=0.7168202672140183, total=   0.8s\n",
      "[CV] learning_rate=0.05, max_depth=7, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=7, max_features=0.1, n_estimators=2000, score=0.7403865965978269, total=   1.5s\n",
      "[CV] learning_rate=0.05, max_depth=7, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=7, max_features=0.1, n_estimators=2000, score=0.7394960864825679, total=   1.5s\n",
      "[CV] learning_rate=0.05, max_depth=7, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=7, max_features=0.1, n_estimators=2000, score=0.7866143604997458, total=   1.5s\n",
      "[CV] learning_rate=0.05, max_depth=7, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=7, max_features=0.1, n_estimators=2000, score=0.70787989008121, total=   1.5s\n",
      "[CV] learning_rate=0.05, max_depth=7, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=7, max_features=0.1, n_estimators=2000, score=0.708104263057199, total=   1.5s\n",
      "[CV] learning_rate=0.05, max_depth=7, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=7, max_features=0.3, n_estimators=1000, score=0.7527646705276453, total=   1.6s\n",
      "[CV] learning_rate=0.05, max_depth=7, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=7, max_features=0.3, n_estimators=1000, score=0.7306964803031681, total=   1.6s\n",
      "[CV] learning_rate=0.05, max_depth=7, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=7, max_features=0.3, n_estimators=1000, score=0.7968111012770279, total=   1.6s\n",
      "[CV] learning_rate=0.05, max_depth=7, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=7, max_features=0.3, n_estimators=1000, score=0.7207640091874379, total=   1.6s\n",
      "[CV] learning_rate=0.05, max_depth=7, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=7, max_features=0.3, n_estimators=1000, score=0.7215776096779543, total=   1.6s\n",
      "[CV] learning_rate=0.05, max_depth=7, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=7, max_features=0.3, n_estimators=2000, score=0.7532871973495969, total=   2.8s\n",
      "[CV] learning_rate=0.05, max_depth=7, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=7, max_features=0.3, n_estimators=2000, score=0.7300993993125191, total=   2.8s\n",
      "[CV] learning_rate=0.05, max_depth=7, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=7, max_features=0.3, n_estimators=2000, score=0.7962722790974893, total=   2.8s\n",
      "[CV] learning_rate=0.05, max_depth=7, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=7, max_features=0.3, n_estimators=2000, score=0.7249697944914697, total=   2.8s\n",
      "[CV] learning_rate=0.05, max_depth=7, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=7, max_features=0.3, n_estimators=2000, score=0.7245997112925274, total=   2.8s\n",
      "[CV] learning_rate=0.05, max_depth=7, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=7, max_features=0.5, n_estimators=1000, score=0.7450880758859164, total=   2.4s\n",
      "[CV] learning_rate=0.05, max_depth=7, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=7, max_features=0.5, n_estimators=1000, score=0.7219047273998561, total=   2.4s\n",
      "[CV] learning_rate=0.05, max_depth=7, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=7, max_features=0.5, n_estimators=1000, score=0.7968425632753985, total=   2.5s\n",
      "[CV] learning_rate=0.05, max_depth=7, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=7, max_features=0.5, n_estimators=1000, score=0.7228328199892681, total=   2.5s\n",
      "[CV] learning_rate=0.05, max_depth=7, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=7, max_features=0.5, n_estimators=1000, score=0.7210661583696008, total=   2.5s\n",
      "[CV] learning_rate=0.05, max_depth=7, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=7, max_features=0.5, n_estimators=2000, score=0.7490806201728469, total=   4.3s\n",
      "[CV] learning_rate=0.05, max_depth=7, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=7, max_features=0.5, n_estimators=2000, score=0.7251426220404611, total=   4.4s\n",
      "[CV] learning_rate=0.05, max_depth=7, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=7, max_features=0.5, n_estimators=2000, score=0.7987682411300973, total=   4.4s\n",
      "[CV] learning_rate=0.05, max_depth=7, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=7, max_features=0.5, n_estimators=2000, score=0.7209759908921514, total=   4.6s\n",
      "[CV] learning_rate=0.05, max_depth=7, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=7, max_features=0.5, n_estimators=2000, score=0.7279667359080849, total=   4.7s\n",
      "[CV] learning_rate=0.05, max_depth=10, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=10, max_features=0.1, n_estimators=1000, score=0.7177709865279249, total=   1.0s\n",
      "[CV] learning_rate=0.05, max_depth=10, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=10, max_features=0.1, n_estimators=1000, score=0.7284447966563368, total=   1.0s\n",
      "[CV] learning_rate=0.05, max_depth=10, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=10, max_features=0.1, n_estimators=1000, score=0.7781155952166244, total=   1.0s\n",
      "[CV] learning_rate=0.05, max_depth=10, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=10, max_features=0.1, n_estimators=1000, score=0.7134790895649794, total=   1.0s\n",
      "[CV] learning_rate=0.05, max_depth=10, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=10, max_features=0.1, n_estimators=1000, score=0.7022702697577334, total=   1.0s\n",
      "[CV] learning_rate=0.05, max_depth=10, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=10, max_features=0.1, n_estimators=2000, score=0.7278511082439557, total=   1.1s\n",
      "[CV] learning_rate=0.05, max_depth=10, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=10, max_features=0.1, n_estimators=2000, score=0.7282186855102353, total=   1.1s\n",
      "[CV] learning_rate=0.05, max_depth=10, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=10, max_features=0.1, n_estimators=2000, score=0.771979995001921, total=   1.1s\n",
      "[CV] learning_rate=0.05, max_depth=10, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=10, max_features=0.1, n_estimators=2000, score=0.7168075035305853, total=   1.1s\n",
      "[CV] learning_rate=0.05, max_depth=10, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=10, max_features=0.1, n_estimators=2000, score=0.7107626870256414, total=   1.1s\n",
      "[CV] learning_rate=0.05, max_depth=10, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=10, max_features=0.3, n_estimators=1000, score=0.7485030109277822, total=   2.1s\n",
      "[CV] learning_rate=0.05, max_depth=10, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=10, max_features=0.3, n_estimators=1000, score=0.7434980023515272, total=   2.0s\n",
      "[CV] learning_rate=0.05, max_depth=10, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=10, max_features=0.3, n_estimators=1000, score=0.7896427314684267, total=   2.0s\n",
      "[CV] learning_rate=0.05, max_depth=10, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=10, max_features=0.3, n_estimators=1000, score=0.7194799449885727, total=   2.1s\n",
      "[CV] learning_rate=0.05, max_depth=10, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=10, max_features=0.3, n_estimators=1000, score=0.7233966325426893, total=   2.1s\n",
      "[CV] learning_rate=0.05, max_depth=10, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=10, max_features=0.3, n_estimators=2000, score=0.7397012993381731, total=   2.2s\n",
      "[CV] learning_rate=0.05, max_depth=10, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=10, max_features=0.3, n_estimators=2000, score=0.7369431862597071, total=   2.1s\n",
      "[CV] learning_rate=0.05, max_depth=10, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=10, max_features=0.3, n_estimators=2000, score=0.7890744709242321, total=   2.1s\n",
      "[CV] learning_rate=0.05, max_depth=10, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=10, max_features=0.3, n_estimators=2000, score=0.7174973044650194, total=   2.1s\n",
      "[CV] learning_rate=0.05, max_depth=10, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=10, max_features=0.3, n_estimators=2000, score=0.7198864632305905, total=   2.2s\n",
      "[CV] learning_rate=0.05, max_depth=10, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=10, max_features=0.5, n_estimators=1000, score=0.7494867760789095, total=   3.3s\n",
      "[CV] learning_rate=0.05, max_depth=10, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=10, max_features=0.5, n_estimators=1000, score=0.7325835434169372, total=   3.4s\n",
      "[CV] learning_rate=0.05, max_depth=10, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=10, max_features=0.5, n_estimators=1000, score=0.7991469983980578, total=   3.3s\n",
      "[CV] learning_rate=0.05, max_depth=10, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=10, max_features=0.5, n_estimators=1000, score=0.7226740586690906, total=   3.4s\n",
      "[CV] learning_rate=0.05, max_depth=10, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.05, max_depth=10, max_features=0.5, n_estimators=1000, score=0.7216717811919983, total=   3.4s\n",
      "[CV] learning_rate=0.05, max_depth=10, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=10, max_features=0.5, n_estimators=2000, score=0.7487770435984751, total=   3.5s\n",
      "[CV] learning_rate=0.05, max_depth=10, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=10, max_features=0.5, n_estimators=2000, score=0.7330375830969063, total=   3.5s\n",
      "[CV] learning_rate=0.05, max_depth=10, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=10, max_features=0.5, n_estimators=2000, score=0.7975796905296982, total=   3.4s\n",
      "[CV] learning_rate=0.05, max_depth=10, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=10, max_features=0.5, n_estimators=2000, score=0.7222503677852579, total=   3.4s\n",
      "[CV] learning_rate=0.05, max_depth=10, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.05, max_depth=10, max_features=0.5, n_estimators=2000, score=0.7095453234854497, total=   3.5s\n",
      "[CV] learning_rate=0.1, max_depth=3, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=3, max_features=0.1, n_estimators=1000, score=0.7400927184551788, total=   0.3s\n",
      "[CV] learning_rate=0.1, max_depth=3, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=3, max_features=0.1, n_estimators=1000, score=0.7051763260749899, total=   0.3s\n",
      "[CV] learning_rate=0.1, max_depth=3, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=3, max_features=0.1, n_estimators=1000, score=0.7863782256378044, total=   0.3s\n",
      "[CV] learning_rate=0.1, max_depth=3, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=3, max_features=0.1, n_estimators=1000, score=0.6903610883360561, total=   0.3s\n",
      "[CV] learning_rate=0.1, max_depth=3, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=3, max_features=0.1, n_estimators=1000, score=0.709880163506944, total=   0.3s\n",
      "[CV] learning_rate=0.1, max_depth=3, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=3, max_features=0.1, n_estimators=2000, score=0.7238648057886273, total=   0.6s\n",
      "[CV] learning_rate=0.1, max_depth=3, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=3, max_features=0.1, n_estimators=2000, score=0.7015505815972904, total=   0.6s\n",
      "[CV] learning_rate=0.1, max_depth=3, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=3, max_features=0.1, n_estimators=2000, score=0.771189774211563, total=   0.6s\n",
      "[CV] learning_rate=0.1, max_depth=3, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=3, max_features=0.1, n_estimators=2000, score=0.6774645909887353, total=   0.6s\n",
      "[CV] learning_rate=0.1, max_depth=3, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=3, max_features=0.1, n_estimators=2000, score=0.6983061932376536, total=   0.6s\n",
      "[CV] learning_rate=0.1, max_depth=3, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=3, max_features=0.3, n_estimators=1000, score=0.7346831372833165, total=   0.5s\n",
      "[CV] learning_rate=0.1, max_depth=3, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=3, max_features=0.3, n_estimators=1000, score=0.6977042165981303, total=   0.4s\n",
      "[CV] learning_rate=0.1, max_depth=3, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=3, max_features=0.3, n_estimators=1000, score=0.7876122507583366, total=   0.5s\n",
      "[CV] learning_rate=0.1, max_depth=3, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=3, max_features=0.3, n_estimators=1000, score=0.690826704359533, total=   0.5s\n",
      "[CV] learning_rate=0.1, max_depth=3, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=3, max_features=0.3, n_estimators=1000, score=0.7044378218776257, total=   0.5s\n",
      "[CV] learning_rate=0.1, max_depth=3, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=3, max_features=0.3, n_estimators=2000, score=0.7191456328307622, total=   0.9s\n",
      "[CV] learning_rate=0.1, max_depth=3, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=3, max_features=0.3, n_estimators=2000, score=0.6950523653128976, total=   0.9s\n",
      "[CV] learning_rate=0.1, max_depth=3, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=3, max_features=0.3, n_estimators=2000, score=0.770112856119971, total=   0.9s\n",
      "[CV] learning_rate=0.1, max_depth=3, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=3, max_features=0.3, n_estimators=2000, score=0.6671644457002963, total=   0.9s\n",
      "[CV] learning_rate=0.1, max_depth=3, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=3, max_features=0.3, n_estimators=2000, score=0.6967486575476491, total=   0.9s\n",
      "[CV] learning_rate=0.1, max_depth=3, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=3, max_features=0.5, n_estimators=1000, score=0.7239935260934496, total=   0.7s\n",
      "[CV] learning_rate=0.1, max_depth=3, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=3, max_features=0.5, n_estimators=1000, score=0.6968332954089733, total=   0.7s\n",
      "[CV] learning_rate=0.1, max_depth=3, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=3, max_features=0.5, n_estimators=1000, score=0.7912071424581322, total=   0.7s\n",
      "[CV] learning_rate=0.1, max_depth=3, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=3, max_features=0.5, n_estimators=1000, score=0.6787669355629476, total=   0.7s\n",
      "[CV] learning_rate=0.1, max_depth=3, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=3, max_features=0.5, n_estimators=1000, score=0.7036425895595, total=   0.7s\n",
      "[CV] learning_rate=0.1, max_depth=3, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=3, max_features=0.5, n_estimators=2000, score=0.7107280285375973, total=   1.4s\n",
      "[CV] learning_rate=0.1, max_depth=3, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=3, max_features=0.5, n_estimators=2000, score=0.6906934320836046, total=   1.4s\n",
      "[CV] learning_rate=0.1, max_depth=3, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=3, max_features=0.5, n_estimators=2000, score=0.7684853436032557, total=   1.4s\n",
      "[CV] learning_rate=0.1, max_depth=3, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=3, max_features=0.5, n_estimators=2000, score=0.6733068186415727, total=   1.4s\n",
      "[CV] learning_rate=0.1, max_depth=3, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=3, max_features=0.5, n_estimators=2000, score=0.6881659954174325, total=   1.4s\n",
      "[CV] learning_rate=0.1, max_depth=5, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=5, max_features=0.1, n_estimators=1000, score=0.73062496919834, total=   0.5s\n",
      "[CV] learning_rate=0.1, max_depth=5, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=5, max_features=0.1, n_estimators=1000, score=0.7112315806878475, total=   0.5s\n",
      "[CV] learning_rate=0.1, max_depth=5, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=5, max_features=0.1, n_estimators=1000, score=0.7889885476586568, total=   0.5s\n",
      "[CV] learning_rate=0.1, max_depth=5, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=5, max_features=0.1, n_estimators=1000, score=0.6938845522487078, total=   0.6s\n",
      "[CV] learning_rate=0.1, max_depth=5, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=5, max_features=0.1, n_estimators=1000, score=0.7078837587072513, total=   0.7s\n",
      "[CV] learning_rate=0.1, max_depth=5, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=5, max_features=0.1, n_estimators=2000, score=0.7355003080703729, total=   1.0s\n",
      "[CV] learning_rate=0.1, max_depth=5, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=5, max_features=0.1, n_estimators=2000, score=0.7127437181038732, total=   1.0s\n",
      "[CV] learning_rate=0.1, max_depth=5, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=5, max_features=0.1, n_estimators=2000, score=0.7818553463982422, total=   1.0s\n",
      "[CV] learning_rate=0.1, max_depth=5, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=5, max_features=0.1, n_estimators=2000, score=0.6858339349017537, total=   1.0s\n",
      "[CV] learning_rate=0.1, max_depth=5, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=5, max_features=0.1, n_estimators=2000, score=0.7021546186806401, total=   1.0s\n",
      "[CV] learning_rate=0.1, max_depth=5, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=5, max_features=0.3, n_estimators=1000, score=0.7417695496650021, total=   0.9s\n",
      "[CV] learning_rate=0.1, max_depth=5, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=5, max_features=0.3, n_estimators=1000, score=0.7099484489978046, total=   0.9s\n",
      "[CV] learning_rate=0.1, max_depth=5, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=5, max_features=0.3, n_estimators=1000, score=0.7923173090695046, total=   1.0s\n",
      "[CV] learning_rate=0.1, max_depth=5, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=5, max_features=0.3, n_estimators=1000, score=0.6985217244130694, total=   0.9s\n",
      "[CV] learning_rate=0.1, max_depth=5, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=5, max_features=0.3, n_estimators=1000, score=0.7017796878802254, total=   0.9s\n",
      "[CV] learning_rate=0.1, max_depth=5, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=5, max_features=0.3, n_estimators=2000, score=0.7366849469695127, total=   1.8s\n",
      "[CV] learning_rate=0.1, max_depth=5, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=5, max_features=0.3, n_estimators=2000, score=0.710631893814357, total=   1.9s\n",
      "[CV] learning_rate=0.1, max_depth=5, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=5, max_features=0.3, n_estimators=2000, score=0.7701324326286275, total=   1.9s\n",
      "[CV] learning_rate=0.1, max_depth=5, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=5, max_features=0.3, n_estimators=2000, score=0.6971844661677287, total=   1.9s\n",
      "[CV] learning_rate=0.1, max_depth=5, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=5, max_features=0.3, n_estimators=2000, score=0.710067350050307, total=   1.8s\n",
      "[CV] learning_rate=0.1, max_depth=5, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=5, max_features=0.5, n_estimators=1000, score=0.7378688585097068, total=   1.4s\n",
      "[CV] learning_rate=0.1, max_depth=5, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=5, max_features=0.5, n_estimators=1000, score=0.723908544033475, total=   1.4s\n",
      "[CV] learning_rate=0.1, max_depth=5, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=5, max_features=0.5, n_estimators=1000, score=0.7861375811253967, total=   1.4s\n",
      "[CV] learning_rate=0.1, max_depth=5, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=5, max_features=0.5, n_estimators=1000, score=0.7083898379004534, total=   1.5s\n",
      "[CV] learning_rate=0.1, max_depth=5, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=5, max_features=0.5, n_estimators=1000, score=0.7020346355186942, total=   1.4s\n",
      "[CV] learning_rate=0.1, max_depth=5, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=5, max_features=0.5, n_estimators=2000, score=0.7436745379932068, total=   3.0s\n",
      "[CV] learning_rate=0.1, max_depth=5, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=5, max_features=0.5, n_estimators=2000, score=0.721390374669193, total=   3.1s\n",
      "[CV] learning_rate=0.1, max_depth=5, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=5, max_features=0.5, n_estimators=2000, score=0.7899364619877614, total=   2.9s\n",
      "[CV] learning_rate=0.1, max_depth=5, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=5, max_features=0.5, n_estimators=2000, score=0.7023376993387401, total=   2.8s\n",
      "[CV] learning_rate=0.1, max_depth=5, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=5, max_features=0.5, n_estimators=2000, score=0.7078778010172415, total=   2.9s\n",
      "[CV] learning_rate=0.1, max_depth=7, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=7, max_features=0.1, n_estimators=1000, score=0.7332085221187111, total=   0.8s\n",
      "[CV] learning_rate=0.1, max_depth=7, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=7, max_features=0.1, n_estimators=1000, score=0.7260121075747383, total=   0.8s\n",
      "[CV] learning_rate=0.1, max_depth=7, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=7, max_features=0.1, n_estimators=1000, score=0.7757299189069455, total=   0.7s\n",
      "[CV] learning_rate=0.1, max_depth=7, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=7, max_features=0.1, n_estimators=1000, score=0.7071261929320487, total=   0.7s\n",
      "[CV] learning_rate=0.1, max_depth=7, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=7, max_features=0.1, n_estimators=1000, score=0.7061625505249256, total=   0.8s\n",
      "[CV] learning_rate=0.1, max_depth=7, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=7, max_features=0.1, n_estimators=2000, score=0.7394025958553498, total=   0.9s\n",
      "[CV] learning_rate=0.1, max_depth=7, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=7, max_features=0.1, n_estimators=2000, score=0.7376728682711406, total=   0.9s\n",
      "[CV] learning_rate=0.1, max_depth=7, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=7, max_features=0.1, n_estimators=2000, score=0.7813160199839769, total=   0.9s\n",
      "[CV] learning_rate=0.1, max_depth=7, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=7, max_features=0.1, n_estimators=2000, score=0.699058564582554, total=   0.9s\n",
      "[CV] learning_rate=0.1, max_depth=7, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=7, max_features=0.1, n_estimators=2000, score=0.7095696708273942, total=   0.9s\n",
      "[CV] learning_rate=0.1, max_depth=7, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=7, max_features=0.3, n_estimators=1000, score=0.7501109680043165, total=   1.4s\n",
      "[CV] learning_rate=0.1, max_depth=7, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=7, max_features=0.3, n_estimators=1000, score=0.7320737587546198, total=   1.4s\n",
      "[CV] learning_rate=0.1, max_depth=7, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=7, max_features=0.3, n_estimators=1000, score=0.7871169906244867, total=   1.4s\n",
      "[CV] learning_rate=0.1, max_depth=7, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=7, max_features=0.3, n_estimators=1000, score=0.7234201757710497, total=   1.4s\n",
      "[CV] learning_rate=0.1, max_depth=7, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=7, max_features=0.3, n_estimators=1000, score=0.7194652481909605, total=   1.5s\n",
      "[CV] learning_rate=0.1, max_depth=7, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=7, max_features=0.3, n_estimators=2000, score=0.7418655595748486, total=   1.5s\n",
      "[CV] learning_rate=0.1, max_depth=7, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=7, max_features=0.3, n_estimators=2000, score=0.7244626202046608, total=   1.5s\n",
      "[CV] learning_rate=0.1, max_depth=7, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=7, max_features=0.3, n_estimators=2000, score=0.7834005945928676, total=   1.5s\n",
      "[CV] learning_rate=0.1, max_depth=7, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=7, max_features=0.3, n_estimators=2000, score=0.7241832829411852, total=   1.5s\n",
      "[CV] learning_rate=0.1, max_depth=7, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=7, max_features=0.3, n_estimators=2000, score=0.7227272210830801, total=   1.5s\n",
      "[CV] learning_rate=0.1, max_depth=7, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=7, max_features=0.5, n_estimators=1000, score=0.7453695426305416, total=   2.2s\n",
      "[CV] learning_rate=0.1, max_depth=7, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=7, max_features=0.5, n_estimators=1000, score=0.7308966989776801, total=   2.3s\n",
      "[CV] learning_rate=0.1, max_depth=7, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=7, max_features=0.5, n_estimators=1000, score=0.7894149280812881, total=   2.2s\n",
      "[CV] learning_rate=0.1, max_depth=7, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=7, max_features=0.5, n_estimators=1000, score=0.7252913188700597, total=   2.2s\n",
      "[CV] learning_rate=0.1, max_depth=7, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=7, max_features=0.5, n_estimators=1000, score=0.7107156873553534, total=   2.3s\n",
      "[CV] learning_rate=0.1, max_depth=7, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=7, max_features=0.5, n_estimators=2000, score=0.7499877423621176, total=   2.3s\n",
      "[CV] learning_rate=0.1, max_depth=7, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=7, max_features=0.5, n_estimators=2000, score=0.7295974357531997, total=   2.4s\n",
      "[CV] learning_rate=0.1, max_depth=7, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=7, max_features=0.5, n_estimators=2000, score=0.8033482037577244, total=   2.4s\n",
      "[CV] learning_rate=0.1, max_depth=7, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=7, max_features=0.5, n_estimators=2000, score=0.708724944503413, total=   2.4s\n",
      "[CV] learning_rate=0.1, max_depth=7, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=7, max_features=0.5, n_estimators=2000, score=0.7096245120460201, total=   2.3s\n",
      "[CV] learning_rate=0.1, max_depth=10, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=10, max_features=0.1, n_estimators=1000, score=0.7245899828276123, total=   0.5s\n",
      "[CV] learning_rate=0.1, max_depth=10, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=10, max_features=0.1, n_estimators=1000, score=0.7213787716744793, total=   0.6s\n",
      "[CV] learning_rate=0.1, max_depth=10, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=10, max_features=0.1, n_estimators=1000, score=0.771114043249621, total=   0.6s\n",
      "[CV] learning_rate=0.1, max_depth=10, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=10, max_features=0.1, n_estimators=1000, score=0.6951395793983591, total=   0.6s\n",
      "[CV] learning_rate=0.1, max_depth=10, max_features=0.1, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=10, max_features=0.1, n_estimators=1000, score=0.6866123757502176, total=   0.6s\n",
      "[CV] learning_rate=0.1, max_depth=10, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=10, max_features=0.1, n_estimators=2000, score=0.7083380746262937, total=   0.6s\n",
      "[CV] learning_rate=0.1, max_depth=10, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=10, max_features=0.1, n_estimators=2000, score=0.7218458017159783, total=   0.6s\n",
      "[CV] learning_rate=0.1, max_depth=10, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=10, max_features=0.1, n_estimators=2000, score=0.7729539221198265, total=   0.6s\n",
      "[CV] learning_rate=0.1, max_depth=10, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=10, max_features=0.1, n_estimators=2000, score=0.6786220787087511, total=   0.7s\n",
      "[CV] learning_rate=0.1, max_depth=10, max_features=0.1, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=10, max_features=0.1, n_estimators=2000, score=0.6924059206715347, total=   0.7s\n",
      "[CV] learning_rate=0.1, max_depth=10, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=10, max_features=0.3, n_estimators=1000, score=0.7461429399331911, total=   1.0s\n",
      "[CV] learning_rate=0.1, max_depth=10, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=10, max_features=0.3, n_estimators=1000, score=0.7335854835740997, total=   1.1s\n",
      "[CV] learning_rate=0.1, max_depth=10, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=10, max_features=0.3, n_estimators=1000, score=0.7882061664132022, total=   1.1s\n",
      "[CV] learning_rate=0.1, max_depth=10, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=10, max_features=0.3, n_estimators=1000, score=0.715496985489933, total=   1.2s\n",
      "[CV] learning_rate=0.1, max_depth=10, max_features=0.3, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=10, max_features=0.3, n_estimators=1000, score=0.7125187406617058, total=   1.1s\n",
      "[CV] learning_rate=0.1, max_depth=10, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=10, max_features=0.3, n_estimators=2000, score=0.7351299096149566, total=   1.2s\n",
      "[CV] learning_rate=0.1, max_depth=10, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=10, max_features=0.3, n_estimators=2000, score=0.7332982267329171, total=   1.1s\n",
      "[CV] learning_rate=0.1, max_depth=10, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=10, max_features=0.3, n_estimators=2000, score=0.7847242825214382, total=   1.2s\n",
      "[CV] learning_rate=0.1, max_depth=10, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=10, max_features=0.3, n_estimators=2000, score=0.7182262515857962, total=   1.2s\n",
      "[CV] learning_rate=0.1, max_depth=10, max_features=0.3, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=10, max_features=0.3, n_estimators=2000, score=0.7061579918019316, total=   1.1s\n",
      "[CV] learning_rate=0.1, max_depth=10, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=10, max_features=0.5, n_estimators=1000, score=0.7342673085994973, total=   1.8s\n",
      "[CV] learning_rate=0.1, max_depth=10, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=10, max_features=0.5, n_estimators=1000, score=0.7324242118013593, total=   1.8s\n",
      "[CV] learning_rate=0.1, max_depth=10, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=10, max_features=0.5, n_estimators=1000, score=0.7848016999949579, total=   1.7s\n",
      "[CV] learning_rate=0.1, max_depth=10, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=10, max_features=0.5, n_estimators=1000, score=0.7116218347626846, total=   1.7s\n",
      "[CV] learning_rate=0.1, max_depth=10, max_features=0.5, n_estimators=1000 \n",
      "[CV]  learning_rate=0.1, max_depth=10, max_features=0.5, n_estimators=1000, score=0.7100725360014587, total=   1.7s\n",
      "[CV] learning_rate=0.1, max_depth=10, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=10, max_features=0.5, n_estimators=2000, score=0.7420754701332148, total=   1.8s\n",
      "[CV] learning_rate=0.1, max_depth=10, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=10, max_features=0.5, n_estimators=2000, score=0.7149834059138893, total=   1.8s\n",
      "[CV] learning_rate=0.1, max_depth=10, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=10, max_features=0.5, n_estimators=2000, score=0.7906294839563833, total=   1.7s\n",
      "[CV] learning_rate=0.1, max_depth=10, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=10, max_features=0.5, n_estimators=2000, score=0.7086771799125724, total=   1.8s\n",
      "[CV] learning_rate=0.1, max_depth=10, max_features=0.5, n_estimators=2000 \n",
      "[CV]  learning_rate=0.1, max_depth=10, max_features=0.5, n_estimators=2000, score=0.7012909195205025, total=   1.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 360 out of 360 | elapsed: 11.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'learning_rate': [0.01, 0.05, 0.1], 'n_estimators': [1000, 2000], 'max_depth': [3, 5, 7, 10], 'max_features': [0.1, 0.3, 0.5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=3)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'learning_rate':[0.01,0.05,0.1], 'n_estimators':[1000,2000],'max_depth':[3,5,7,10],'max_features':[0.1,0.3,0.5]}\n",
    "\n",
    "clf = GridSearchCV(GradientBoostingRegressor(), parameters, cv=5,verbose=3)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.01, loss='ls', max_depth=3, max_features=0.5,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=1000, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model, Input\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.stats as scs\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2157, 23)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=16,input_dim= 23,activation='relu'))\n",
    "model.add(Dense(units=8, activation='relu'))\n",
    "model.add(Dense(units=4, activation='relu'))\n",
    "model.add(Dense(units=1,activation='linear'))\n",
    "model.compile(loss='mse', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.2755\n",
      "Epoch 2/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 7.1428\n",
      "Epoch 3/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 7.2697\n",
      "Epoch 4/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 7.0826\n",
      "Epoch 5/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 7.1460\n",
      "Epoch 6/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 7.2798\n",
      "Epoch 7/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 7.2473\n",
      "Epoch 8/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.1895\n",
      "Epoch 9/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.1678\n",
      "Epoch 10/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1386\n",
      "Epoch 11/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1470\n",
      "Epoch 12/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.2838\n",
      "Epoch 13/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.2678\n",
      "Epoch 14/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.2523\n",
      "Epoch 15/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.2097\n",
      "Epoch 16/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.2175\n",
      "Epoch 17/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1101\n",
      "Epoch 18/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0752\n",
      "Epoch 19/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1100\n",
      "Epoch 20/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.2906\n",
      "Epoch 21/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.2660\n",
      "Epoch 22/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1784\n",
      "Epoch 23/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 7.2423\n",
      "Epoch 24/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.0913\n",
      "Epoch 25/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.1427\n",
      "Epoch 26/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 7.3147\n",
      "Epoch 27/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.2610\n",
      "Epoch 28/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.2961\n",
      "Epoch 29/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.2366\n",
      "Epoch 30/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0640\n",
      "Epoch 31/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0993\n",
      "Epoch 32/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.2396\n",
      "Epoch 33/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1963\n",
      "Epoch 34/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1153\n",
      "Epoch 35/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.2589\n",
      "Epoch 36/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1398\n",
      "Epoch 37/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0713\n",
      "Epoch 38/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.2676\n",
      "Epoch 39/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.2048\n",
      "Epoch 40/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1696\n",
      "Epoch 41/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1424\n",
      "Epoch 42/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.2363\n",
      "Epoch 43/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.2533\n",
      "Epoch 44/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1854\n",
      "Epoch 45/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.3793\n",
      "Epoch 46/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.3172\n",
      "Epoch 47/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.3453\n",
      "Epoch 48/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1659\n",
      "Epoch 49/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1432\n",
      "Epoch 50/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.2394\n",
      "Epoch 51/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1657\n",
      "Epoch 52/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1855\n",
      "Epoch 53/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.4013\n",
      "Epoch 54/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.2534\n",
      "Epoch 55/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0893\n",
      "Epoch 56/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1648\n",
      "Epoch 57/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.2607\n",
      "Epoch 58/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0482\n",
      "Epoch 59/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0210\n",
      "Epoch 60/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0851\n",
      "Epoch 61/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.1715\n",
      "Epoch 62/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.2012\n",
      "Epoch 63/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.2562\n",
      "Epoch 64/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.2460\n",
      "Epoch 65/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0543\n",
      "Epoch 66/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.0820\n",
      "Epoch 67/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.2107\n",
      "Epoch 68/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.0496\n",
      "Epoch 69/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.3608\n",
      "Epoch 70/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.2603\n",
      "Epoch 71/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.6466\n",
      "Epoch 72/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 7.3578\n",
      "Epoch 73/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 7.1913\n",
      "Epoch 74/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.2747\n",
      "Epoch 75/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.1152\n",
      "Epoch 76/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.0702\n",
      "Epoch 77/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1199\n",
      "Epoch 78/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.2436\n",
      "Epoch 79/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.2141\n",
      "Epoch 80/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.2919\n",
      "Epoch 81/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1362\n",
      "Epoch 82/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.5098\n",
      "Epoch 83/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1878\n",
      "Epoch 84/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1458\n",
      "Epoch 85/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.0724\n",
      "Epoch 86/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.0447\n",
      "Epoch 87/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.1067\n",
      "Epoch 88/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.2077\n",
      "Epoch 89/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.2375\n",
      "Epoch 90/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.1306\n",
      "Epoch 91/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1275\n",
      "Epoch 92/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.1174\n",
      "Epoch 93/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0690\n",
      "Epoch 94/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1632\n",
      "Epoch 95/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1713\n",
      "Epoch 96/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1432\n",
      "Epoch 97/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1413\n",
      "Epoch 98/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 7.1033\n",
      "Epoch 99/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 7.1850\n",
      "Epoch 100/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0767\n",
      "Epoch 101/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.2567\n",
      "Epoch 102/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1730\n",
      "Epoch 103/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.2541\n",
      "Epoch 104/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.5343\n",
      "Epoch 105/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 7.3984\n",
      "Epoch 106/1000\n",
      "2157/2157 [==============================] - 0s 16us/step - loss: 7.3520\n",
      "Epoch 107/1000\n",
      "2157/2157 [==============================] - 0s 16us/step - loss: 7.0656\n",
      "Epoch 108/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.2480\n",
      "Epoch 109/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0756\n",
      "Epoch 110/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0946\n",
      "Epoch 111/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.2223\n",
      "Epoch 112/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.3052\n",
      "Epoch 113/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.4302\n",
      "Epoch 114/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.1944\n",
      "Epoch 115/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.3574\n",
      "Epoch 116/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.3190\n",
      "Epoch 117/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.2492\n",
      "Epoch 118/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1785\n",
      "Epoch 119/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1142\n",
      "Epoch 120/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.2118\n",
      "Epoch 121/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0434\n",
      "Epoch 122/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1380\n",
      "Epoch 123/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.2268\n",
      "Epoch 124/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0569\n",
      "Epoch 125/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.2407\n",
      "Epoch 126/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.7000\n",
      "Epoch 127/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.3952\n",
      "Epoch 128/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.5379\n",
      "Epoch 129/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.4474\n",
      "Epoch 130/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.2513\n",
      "Epoch 131/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1202\n",
      "Epoch 132/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.3204\n",
      "Epoch 133/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0348\n",
      "Epoch 134/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.1104\n",
      "Epoch 135/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 7.3623\n",
      "Epoch 136/1000\n",
      "2157/2157 [==============================] - 0s 17us/step - loss: 7.5125\n",
      "Epoch 137/1000\n",
      "2157/2157 [==============================] - 0s 16us/step - loss: 7.2844\n",
      "Epoch 138/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.2636\n",
      "Epoch 139/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 7.1080\n",
      "Epoch 140/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.1816\n",
      "Epoch 141/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.5855\n",
      "Epoch 142/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.6231\n",
      "Epoch 143/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.4405\n",
      "Epoch 144/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.3812\n",
      "Epoch 145/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 7.1673\n",
      "Epoch 146/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.0606\n",
      "Epoch 147/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1122\n",
      "Epoch 148/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.5472\n",
      "Epoch 149/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.3000\n",
      "Epoch 150/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.4295\n",
      "Epoch 151/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.1937\n",
      "Epoch 152/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1126\n",
      "Epoch 153/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.4044\n",
      "Epoch 154/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.3793\n",
      "Epoch 155/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.2126\n",
      "Epoch 156/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1362\n",
      "Epoch 157/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1882\n",
      "Epoch 158/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0932\n",
      "Epoch 159/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0486\n",
      "Epoch 160/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.2324\n",
      "Epoch 161/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.2349\n",
      "Epoch 162/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.3342\n",
      "Epoch 163/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.3427\n",
      "Epoch 164/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0998\n",
      "Epoch 165/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0986\n",
      "Epoch 166/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9897\n",
      "Epoch 167/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.1668\n",
      "Epoch 168/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1233\n",
      "Epoch 169/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1223\n",
      "Epoch 170/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1038\n",
      "Epoch 171/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.3424\n",
      "Epoch 172/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.2782\n",
      "Epoch 173/1000\n",
      "2157/2157 [==============================] - 0s 15us/step - loss: 7.1361\n",
      "Epoch 174/1000\n",
      "2157/2157 [==============================] - 0s 16us/step - loss: 7.2184\n",
      "Epoch 175/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 7.3144\n",
      "Epoch 176/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.2306\n",
      "Epoch 177/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.9397\n",
      "Epoch 178/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.0198\n",
      "Epoch 179/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.9610\n",
      "Epoch 180/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1000\n",
      "Epoch 181/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.2908\n",
      "Epoch 182/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9845\n",
      "Epoch 183/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0535\n",
      "Epoch 184/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.0657\n",
      "Epoch 185/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.2646\n",
      "Epoch 186/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0864\n",
      "Epoch 187/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0189\n",
      "Epoch 188/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0532\n",
      "Epoch 189/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.1480\n",
      "Epoch 190/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9621\n",
      "Epoch 191/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.1097\n",
      "Epoch 192/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0545\n",
      "Epoch 193/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.2350\n",
      "Epoch 194/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0793\n",
      "Epoch 195/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0733\n",
      "Epoch 196/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1887\n",
      "Epoch 197/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.1013\n",
      "Epoch 198/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.1177\n",
      "Epoch 199/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9232\n",
      "Epoch 200/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9575\n",
      "Epoch 201/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0907\n",
      "Epoch 202/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1747\n",
      "Epoch 203/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.2321\n",
      "Epoch 204/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1953\n",
      "Epoch 205/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.2674\n",
      "Epoch 206/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.2484\n",
      "Epoch 207/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0689\n",
      "Epoch 208/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.2529\n",
      "Epoch 209/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.3444\n",
      "Epoch 210/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.4893\n",
      "Epoch 211/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1150\n",
      "Epoch 212/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.2936\n",
      "Epoch 213/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.9963\n",
      "Epoch 214/1000\n",
      "2157/2157 [==============================] - 0s 16us/step - loss: 6.9636\n",
      "Epoch 215/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.1314\n",
      "Epoch 216/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.9818\n",
      "Epoch 217/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 6.9108\n",
      "Epoch 218/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1504\n",
      "Epoch 219/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0062\n",
      "Epoch 220/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.0192\n",
      "Epoch 221/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9445\n",
      "Epoch 222/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 6.9466\n",
      "Epoch 223/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 6.9089\n",
      "Epoch 224/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9311\n",
      "Epoch 225/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0767\n",
      "Epoch 226/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9671\n",
      "Epoch 227/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0680\n",
      "Epoch 228/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.2415\n",
      "Epoch 229/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1271\n",
      "Epoch 230/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1724\n",
      "Epoch 231/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.1277\n",
      "Epoch 232/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0994\n",
      "Epoch 233/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0389\n",
      "Epoch 234/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0215\n",
      "Epoch 235/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0872\n",
      "Epoch 236/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9712\n",
      "Epoch 237/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.1743\n",
      "Epoch 238/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.2788\n",
      "Epoch 239/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9493\n",
      "Epoch 240/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9461\n",
      "Epoch 241/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9289\n",
      "Epoch 242/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.1260\n",
      "Epoch 243/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.0948\n",
      "Epoch 244/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0745\n",
      "Epoch 245/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1479\n",
      "Epoch 246/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8792\n",
      "Epoch 247/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0633\n",
      "Epoch 248/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.2320\n",
      "Epoch 249/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0972\n",
      "Epoch 250/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0729\n",
      "Epoch 251/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8986\n",
      "Epoch 252/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0142\n",
      "Epoch 253/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1461\n",
      "Epoch 254/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 7.0175\n",
      "Epoch 255/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1970\n",
      "Epoch 256/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1661\n",
      "Epoch 257/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.1081\n",
      "Epoch 258/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.9397\n",
      "Epoch 259/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0440\n",
      "Epoch 260/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.2702\n",
      "Epoch 261/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.2114\n",
      "Epoch 262/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0360\n",
      "Epoch 263/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0274\n",
      "Epoch 264/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0615\n",
      "Epoch 265/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0208\n",
      "Epoch 266/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1203\n",
      "Epoch 267/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.9903\n",
      "Epoch 268/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1874\n",
      "Epoch 269/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0513\n",
      "Epoch 270/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1783\n",
      "Epoch 271/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.9810\n",
      "Epoch 272/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1329\n",
      "Epoch 273/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.0061\n",
      "Epoch 274/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0258\n",
      "Epoch 275/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.8726\n",
      "Epoch 276/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.4407\n",
      "Epoch 277/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1331\n",
      "Epoch 278/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1023\n",
      "Epoch 279/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0118\n",
      "Epoch 280/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0655\n",
      "Epoch 281/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9601\n",
      "Epoch 282/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0319\n",
      "Epoch 283/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8855\n",
      "Epoch 284/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0594\n",
      "Epoch 285/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9447\n",
      "Epoch 286/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8512\n",
      "Epoch 287/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 6.9243\n",
      "Epoch 288/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 7.1630\n",
      "Epoch 289/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.8595\n",
      "Epoch 290/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.9074\n",
      "Epoch 291/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.3518\n",
      "Epoch 292/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.1441\n",
      "Epoch 293/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 6.9055\n",
      "Epoch 294/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 7.2459\n",
      "Epoch 295/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.9676\n",
      "Epoch 296/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.9869\n",
      "Epoch 297/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0836\n",
      "Epoch 298/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0418\n",
      "Epoch 299/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 7.1484\n",
      "Epoch 300/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.9788\n",
      "Epoch 301/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.9337\n",
      "Epoch 302/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9440\n",
      "Epoch 303/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8930\n",
      "Epoch 304/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0052\n",
      "Epoch 305/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1076\n",
      "Epoch 306/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0511\n",
      "Epoch 307/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9851\n",
      "Epoch 308/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0109\n",
      "Epoch 309/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0413\n",
      "Epoch 310/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8812\n",
      "Epoch 311/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9638\n",
      "Epoch 312/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0025\n",
      "Epoch 313/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.1932\n",
      "Epoch 314/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0983\n",
      "Epoch 315/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0021\n",
      "Epoch 316/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9532\n",
      "Epoch 317/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.1164\n",
      "Epoch 318/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0394\n",
      "Epoch 319/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.0018\n",
      "Epoch 320/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.2343\n",
      "Epoch 321/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1609\n",
      "Epoch 322/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9259\n",
      "Epoch 323/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8419\n",
      "Epoch 324/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9655\n",
      "Epoch 325/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9659\n",
      "Epoch 326/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0281\n",
      "Epoch 327/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.2588\n",
      "Epoch 328/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9697\n",
      "Epoch 329/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.1068\n",
      "Epoch 330/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1745\n",
      "Epoch 331/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.0073\n",
      "Epoch 332/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1463\n",
      "Epoch 333/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.3267\n",
      "Epoch 334/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9484\n",
      "Epoch 335/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.1168\n",
      "Epoch 336/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9943\n",
      "Epoch 337/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.0298\n",
      "Epoch 338/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.9023\n",
      "Epoch 339/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0779\n",
      "Epoch 340/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1528\n",
      "Epoch 341/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1460\n",
      "Epoch 342/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9476\n",
      "Epoch 343/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0300\n",
      "Epoch 344/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1143\n",
      "Epoch 345/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.8434\n",
      "Epoch 346/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0122\n",
      "Epoch 347/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1917\n",
      "Epoch 348/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.1909\n",
      "Epoch 349/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.4439\n",
      "Epoch 350/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.9885\n",
      "Epoch 351/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9124\n",
      "Epoch 352/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0716\n",
      "Epoch 353/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.0739\n",
      "Epoch 354/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.1866\n",
      "Epoch 355/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.2013\n",
      "Epoch 356/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0397\n",
      "Epoch 357/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.1752\n",
      "Epoch 358/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 7.0028\n",
      "Epoch 359/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 6.8829\n",
      "Epoch 360/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9456\n",
      "Epoch 361/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0182\n",
      "Epoch 362/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9091\n",
      "Epoch 363/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.1032\n",
      "Epoch 364/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.2504\n",
      "Epoch 365/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.3266\n",
      "Epoch 366/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.1714\n",
      "Epoch 367/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.0575\n",
      "Epoch 368/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8524\n",
      "Epoch 369/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9937\n",
      "Epoch 370/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 7.0381\n",
      "Epoch 371/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.0048\n",
      "Epoch 372/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 7.1222\n",
      "Epoch 373/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.0854\n",
      "Epoch 374/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8503\n",
      "Epoch 375/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 6.9425\n",
      "Epoch 376/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8822\n",
      "Epoch 377/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 7.0262\n",
      "Epoch 378/1000\n",
      "2157/2157 [==============================] - 0s 17us/step - loss: 6.9333\n",
      "Epoch 379/1000\n",
      "2157/2157 [==============================] - 0s 15us/step - loss: 6.9722\n",
      "Epoch 380/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.9232\n",
      "Epoch 381/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0637\n",
      "Epoch 382/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.3292\n",
      "Epoch 383/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.3183\n",
      "Epoch 384/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.4614\n",
      "Epoch 385/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0252\n",
      "Epoch 386/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9533\n",
      "Epoch 387/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9480\n",
      "Epoch 388/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9605\n",
      "Epoch 389/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.0588\n",
      "Epoch 390/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9655\n",
      "Epoch 391/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9415\n",
      "Epoch 392/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9085\n",
      "Epoch 393/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9089\n",
      "Epoch 394/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0091\n",
      "Epoch 395/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8945\n",
      "Epoch 396/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.0221\n",
      "Epoch 397/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.1711\n",
      "Epoch 398/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9662\n",
      "Epoch 399/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9896\n",
      "Epoch 400/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.2439\n",
      "Epoch 401/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1264\n",
      "Epoch 402/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0529\n",
      "Epoch 403/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.1369\n",
      "Epoch 404/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.3139\n",
      "Epoch 405/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.4317\n",
      "Epoch 406/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.2919\n",
      "Epoch 407/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.1647\n",
      "Epoch 408/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0048\n",
      "Epoch 409/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0047\n",
      "Epoch 410/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0349\n",
      "Epoch 411/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1693\n",
      "Epoch 412/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.1094\n",
      "Epoch 413/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9502\n",
      "Epoch 414/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.1247\n",
      "Epoch 415/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.9674\n",
      "Epoch 416/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8492\n",
      "Epoch 417/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9144\n",
      "Epoch 418/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7841\n",
      "Epoch 419/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8863\n",
      "Epoch 420/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0570\n",
      "Epoch 421/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.7823\n",
      "Epoch 422/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9362\n",
      "Epoch 423/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1125\n",
      "Epoch 424/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9734\n",
      "Epoch 425/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0461\n",
      "Epoch 426/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9573\n",
      "Epoch 427/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8372\n",
      "Epoch 428/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1675\n",
      "Epoch 429/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9260\n",
      "Epoch 430/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.9605\n",
      "Epoch 431/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0713\n",
      "Epoch 432/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0035\n",
      "Epoch 433/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.2595\n",
      "Epoch 434/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0456\n",
      "Epoch 435/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9668\n",
      "Epoch 436/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8052\n",
      "Epoch 437/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.8207\n",
      "Epoch 438/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9233\n",
      "Epoch 439/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8383\n",
      "Epoch 440/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9998\n",
      "Epoch 441/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0667\n",
      "Epoch 442/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.2129\n",
      "Epoch 443/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.1903\n",
      "Epoch 444/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.1760\n",
      "Epoch 445/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.2863\n",
      "Epoch 446/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9126\n",
      "Epoch 447/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8829\n",
      "Epoch 448/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1905\n",
      "Epoch 449/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 7.0653\n",
      "Epoch 450/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.2578\n",
      "Epoch 451/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9720\n",
      "Epoch 452/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8815\n",
      "Epoch 453/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9134\n",
      "Epoch 454/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.2165\n",
      "Epoch 455/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9766\n",
      "Epoch 456/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8077\n",
      "Epoch 457/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9279\n",
      "Epoch 458/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8851\n",
      "Epoch 459/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8479\n",
      "Epoch 460/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.2355\n",
      "Epoch 461/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.5091\n",
      "Epoch 462/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.0031\n",
      "Epoch 463/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9496\n",
      "Epoch 464/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0352\n",
      "Epoch 465/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 6.8632\n",
      "Epoch 466/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.9776\n",
      "Epoch 467/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9007\n",
      "Epoch 468/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9406\n",
      "Epoch 469/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.8100\n",
      "Epoch 470/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9421\n",
      "Epoch 471/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9823\n",
      "Epoch 472/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0073\n",
      "Epoch 473/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9783\n",
      "Epoch 474/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.2338\n",
      "Epoch 475/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.9437\n",
      "Epoch 476/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0781\n",
      "Epoch 477/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.9193\n",
      "Epoch 478/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8397\n",
      "Epoch 479/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.7727\n",
      "Epoch 480/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8362\n",
      "Epoch 481/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.7629\n",
      "Epoch 482/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.8956\n",
      "Epoch 483/1000\n",
      "2157/2157 [==============================] - 0s 21us/step - loss: 7.0398\n",
      "Epoch 484/1000\n",
      "2157/2157 [==============================] - 0s 16us/step - loss: 6.8240\n",
      "Epoch 485/1000\n",
      "2157/2157 [==============================] - 0s 18us/step - loss: 6.8579\n",
      "Epoch 486/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.0225\n",
      "Epoch 487/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0733\n",
      "Epoch 488/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.5082\n",
      "Epoch 489/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9068\n",
      "Epoch 490/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.1199\n",
      "Epoch 491/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0835\n",
      "Epoch 492/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0457\n",
      "Epoch 493/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8784\n",
      "Epoch 494/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8507\n",
      "Epoch 495/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.0398\n",
      "Epoch 496/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.0060\n",
      "Epoch 497/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0019\n",
      "Epoch 498/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.3603\n",
      "Epoch 499/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.4195\n",
      "Epoch 500/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 6.7689\n",
      "Epoch 501/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9024\n",
      "Epoch 502/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.8269\n",
      "Epoch 503/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9824\n",
      "Epoch 504/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.2073\n",
      "Epoch 505/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0550\n",
      "Epoch 506/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 7.0163\n",
      "Epoch 507/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.9539\n",
      "Epoch 508/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9555\n",
      "Epoch 509/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1402\n",
      "Epoch 510/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.1057\n",
      "Epoch 511/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.7736\n",
      "Epoch 512/1000\n",
      "2157/2157 [==============================] - 0s 15us/step - loss: 6.8376\n",
      "Epoch 513/1000\n",
      "2157/2157 [==============================] - 0s 18us/step - loss: 6.9871\n",
      "Epoch 514/1000\n",
      "2157/2157 [==============================] - 0s 16us/step - loss: 7.1750\n",
      "Epoch 515/1000\n",
      "2157/2157 [==============================] - 0s 15us/step - loss: 7.1184\n",
      "Epoch 516/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 7.1288\n",
      "Epoch 517/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.9407\n",
      "Epoch 518/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8408\n",
      "Epoch 519/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9929\n",
      "Epoch 520/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.0749\n",
      "Epoch 521/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0059\n",
      "Epoch 522/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.9254\n",
      "Epoch 523/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9315\n",
      "Epoch 524/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8231\n",
      "Epoch 525/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8175\n",
      "Epoch 526/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9439\n",
      "Epoch 527/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8854\n",
      "Epoch 528/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.8083\n",
      "Epoch 529/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8495\n",
      "Epoch 530/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8238\n",
      "Epoch 531/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8425\n",
      "Epoch 532/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7636\n",
      "Epoch 533/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.8612\n",
      "Epoch 534/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9245\n",
      "Epoch 535/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8995\n",
      "Epoch 536/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9643\n",
      "Epoch 537/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0225\n",
      "Epoch 538/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8797\n",
      "Epoch 539/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.8461\n",
      "Epoch 540/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0301\n",
      "Epoch 541/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.2197\n",
      "Epoch 542/1000\n",
      "2157/2157 [==============================] - 0s 17us/step - loss: 7.0138\n",
      "Epoch 543/1000\n",
      "2157/2157 [==============================] - 0s 16us/step - loss: 6.9289\n",
      "Epoch 544/1000\n",
      "2157/2157 [==============================] - 0s 17us/step - loss: 7.1173\n",
      "Epoch 545/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 7.0549\n",
      "Epoch 546/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.9524\n",
      "Epoch 547/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.2755\n",
      "Epoch 548/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9445\n",
      "Epoch 549/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7079\n",
      "Epoch 550/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.7714\n",
      "Epoch 551/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.8422\n",
      "Epoch 552/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0639\n",
      "Epoch 553/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.9555\n",
      "Epoch 554/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.7636\n",
      "Epoch 555/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8573\n",
      "Epoch 556/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8080\n",
      "Epoch 557/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8883\n",
      "Epoch 558/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8534\n",
      "Epoch 559/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9072\n",
      "Epoch 560/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8117\n",
      "Epoch 561/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7734\n",
      "Epoch 562/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9292\n",
      "Epoch 563/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8419\n",
      "Epoch 564/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.3484\n",
      "Epoch 565/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.5470\n",
      "Epoch 566/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1099\n",
      "Epoch 567/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9553\n",
      "Epoch 568/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.1865\n",
      "Epoch 569/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.1571\n",
      "Epoch 570/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8486\n",
      "Epoch 571/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7336\n",
      "Epoch 572/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8602\n",
      "Epoch 573/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8315\n",
      "Epoch 574/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8888\n",
      "Epoch 575/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.8929\n",
      "Epoch 576/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7882\n",
      "Epoch 577/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8292\n",
      "Epoch 578/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9923\n",
      "Epoch 579/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1824\n",
      "Epoch 580/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9947\n",
      "Epoch 581/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8270\n",
      "Epoch 582/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9020\n",
      "Epoch 583/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8821\n",
      "Epoch 584/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8907\n",
      "Epoch 585/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.8326\n",
      "Epoch 586/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7607\n",
      "Epoch 587/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0083\n",
      "Epoch 588/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9095\n",
      "Epoch 589/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9184\n",
      "Epoch 590/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8982\n",
      "Epoch 591/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8672\n",
      "Epoch 592/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8763\n",
      "Epoch 593/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.8096\n",
      "Epoch 594/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.7914\n",
      "Epoch 595/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8380\n",
      "Epoch 596/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.1096\n",
      "Epoch 597/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8450\n",
      "Epoch 598/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.9738\n",
      "Epoch 599/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8596\n",
      "Epoch 600/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8464\n",
      "Epoch 601/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8822\n",
      "Epoch 602/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9524\n",
      "Epoch 603/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9478\n",
      "Epoch 604/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.0236\n",
      "Epoch 605/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 7.0436\n",
      "Epoch 606/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9479\n",
      "Epoch 607/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0625\n",
      "Epoch 608/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9936\n",
      "Epoch 609/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9065\n",
      "Epoch 610/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 6.8402\n",
      "Epoch 611/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.2872\n",
      "Epoch 612/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9049\n",
      "Epoch 613/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7912\n",
      "Epoch 614/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.0976\n",
      "Epoch 615/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 7.0662\n",
      "Epoch 616/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9936\n",
      "Epoch 617/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.8491\n",
      "Epoch 618/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9072\n",
      "Epoch 619/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9926\n",
      "Epoch 620/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.0382\n",
      "Epoch 621/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8088\n",
      "Epoch 622/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.8013\n",
      "Epoch 623/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8835\n",
      "Epoch 624/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8190\n",
      "Epoch 625/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 6.9454\n",
      "Epoch 626/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9143\n",
      "Epoch 627/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8848\n",
      "Epoch 628/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8075\n",
      "Epoch 629/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 6.7953\n",
      "Epoch 630/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.7304\n",
      "Epoch 631/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.2506\n",
      "Epoch 632/1000\n",
      "2157/2157 [==============================] - 0s 18us/step - loss: 7.0036\n",
      "Epoch 633/1000\n",
      "2157/2157 [==============================] - 0s 16us/step - loss: 6.8120\n",
      "Epoch 634/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1658\n",
      "Epoch 635/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.9329\n",
      "Epoch 636/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.7807\n",
      "Epoch 637/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.8872\n",
      "Epoch 638/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9126\n",
      "Epoch 639/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9899\n",
      "Epoch 640/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.8109\n",
      "Epoch 641/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.8940\n",
      "Epoch 642/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.3363\n",
      "Epoch 643/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.4073\n",
      "Epoch 644/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9120\n",
      "Epoch 645/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0770\n",
      "Epoch 646/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 6.9746\n",
      "Epoch 647/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0678\n",
      "Epoch 648/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.2504\n",
      "Epoch 649/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 6.9446\n",
      "Epoch 650/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8752\n",
      "Epoch 651/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.8693\n",
      "Epoch 652/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8837\n",
      "Epoch 653/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8018\n",
      "Epoch 654/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 6.9663\n",
      "Epoch 655/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9389\n",
      "Epoch 656/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 6.8013\n",
      "Epoch 657/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 6.7645\n",
      "Epoch 658/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 6.8164\n",
      "Epoch 659/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 6.8506\n",
      "Epoch 660/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.0223\n",
      "Epoch 661/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 6.8219\n",
      "Epoch 662/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 7.0625\n",
      "Epoch 663/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 6.8674\n",
      "Epoch 664/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.7230\n",
      "Epoch 665/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.7910\n",
      "Epoch 666/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 6.8000\n",
      "Epoch 667/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 6.8589\n",
      "Epoch 668/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 7.3427\n",
      "Epoch 669/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.0028\n",
      "Epoch 670/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8206\n",
      "Epoch 671/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8214\n",
      "Epoch 672/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.7785\n",
      "Epoch 673/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 6.8088\n",
      "Epoch 674/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8329\n",
      "Epoch 675/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.8703\n",
      "Epoch 676/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.8623\n",
      "Epoch 677/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8452\n",
      "Epoch 678/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8145\n",
      "Epoch 679/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0316\n",
      "Epoch 680/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.4412\n",
      "Epoch 681/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1850\n",
      "Epoch 682/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9264\n",
      "Epoch 683/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9025\n",
      "Epoch 684/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.2135\n",
      "Epoch 685/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.3667\n",
      "Epoch 686/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9676\n",
      "Epoch 687/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.8169\n",
      "Epoch 688/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9068\n",
      "Epoch 689/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.5965\n",
      "Epoch 690/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.3278\n",
      "Epoch 691/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8950\n",
      "Epoch 692/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7884\n",
      "Epoch 693/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9598\n",
      "Epoch 694/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9839\n",
      "Epoch 695/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9985\n",
      "Epoch 696/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.0811\n",
      "Epoch 697/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9047\n",
      "Epoch 698/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.8480\n",
      "Epoch 699/1000\n",
      "2157/2157 [==============================] - 0s 15us/step - loss: 6.7825\n",
      "Epoch 700/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8194\n",
      "Epoch 701/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7882\n",
      "Epoch 702/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9128\n",
      "Epoch 703/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8343\n",
      "Epoch 704/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7341\n",
      "Epoch 705/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8938\n",
      "Epoch 706/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7907\n",
      "Epoch 707/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9959\n",
      "Epoch 708/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9756\n",
      "Epoch 709/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.7504\n",
      "Epoch 710/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.0067\n",
      "Epoch 711/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9606\n",
      "Epoch 712/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.7757\n",
      "Epoch 713/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.7674\n",
      "Epoch 714/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7724\n",
      "Epoch 715/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.1674\n",
      "Epoch 716/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9277\n",
      "Epoch 717/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8264\n",
      "Epoch 718/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8924\n",
      "Epoch 719/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.0791\n",
      "Epoch 720/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8400\n",
      "Epoch 721/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8809\n",
      "Epoch 722/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7872\n",
      "Epoch 723/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 6.8056\n",
      "Epoch 724/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 6.8966\n",
      "Epoch 725/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9302\n",
      "Epoch 726/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1791\n",
      "Epoch 727/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8672\n",
      "Epoch 728/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8129\n",
      "Epoch 729/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 6.8004\n",
      "Epoch 730/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 6.7957\n",
      "Epoch 731/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9124\n",
      "Epoch 732/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.9093\n",
      "Epoch 733/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0617\n",
      "Epoch 734/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8845\n",
      "Epoch 735/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8784\n",
      "Epoch 736/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8865\n",
      "Epoch 737/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8154\n",
      "Epoch 738/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.7458\n",
      "Epoch 739/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8054\n",
      "Epoch 740/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7892\n",
      "Epoch 741/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8651\n",
      "Epoch 742/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8243\n",
      "Epoch 743/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.7472\n",
      "Epoch 744/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.8623\n",
      "Epoch 745/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8658\n",
      "Epoch 746/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0128\n",
      "Epoch 747/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8043\n",
      "Epoch 748/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.7538\n",
      "Epoch 749/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7768\n",
      "Epoch 750/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7525\n",
      "Epoch 751/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9140\n",
      "Epoch 752/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.4000\n",
      "Epoch 753/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.3830\n",
      "Epoch 754/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8752\n",
      "Epoch 755/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8596\n",
      "Epoch 756/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.7195\n",
      "Epoch 757/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.0305\n",
      "Epoch 758/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0566\n",
      "Epoch 759/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 7.2046\n",
      "Epoch 760/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9494\n",
      "Epoch 761/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8098\n",
      "Epoch 762/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1632\n",
      "Epoch 763/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0858\n",
      "Epoch 764/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9291\n",
      "Epoch 765/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8028\n",
      "Epoch 766/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8726\n",
      "Epoch 767/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.8732\n",
      "Epoch 768/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.8198\n",
      "Epoch 769/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.7794\n",
      "Epoch 770/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8621\n",
      "Epoch 771/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.0319\n",
      "Epoch 772/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7649\n",
      "Epoch 773/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.6882\n",
      "Epoch 774/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8298\n",
      "Epoch 775/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.9016\n",
      "Epoch 776/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.7621\n",
      "Epoch 777/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0774\n",
      "Epoch 778/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.2051\n",
      "Epoch 779/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8824\n",
      "Epoch 780/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0300\n",
      "Epoch 781/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9649\n",
      "Epoch 782/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8620\n",
      "Epoch 783/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9334\n",
      "Epoch 784/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 6.7697\n",
      "Epoch 785/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.7516\n",
      "Epoch 786/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.1842\n",
      "Epoch 787/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7843\n",
      "Epoch 788/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.1722\n",
      "Epoch 789/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.8492\n",
      "Epoch 790/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7004\n",
      "Epoch 791/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7050\n",
      "Epoch 792/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.1120\n",
      "Epoch 793/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8192\n",
      "Epoch 794/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.7763\n",
      "Epoch 795/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8036\n",
      "Epoch 796/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.1733\n",
      "Epoch 797/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8943\n",
      "Epoch 798/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9734\n",
      "Epoch 799/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8779\n",
      "Epoch 800/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7697\n",
      "Epoch 801/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.9520\n",
      "Epoch 802/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7608\n",
      "Epoch 803/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7706\n",
      "Epoch 804/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.7224\n",
      "Epoch 805/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 6.7398\n",
      "Epoch 806/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0122\n",
      "Epoch 807/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.7413\n",
      "Epoch 808/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.8660\n",
      "Epoch 809/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7949\n",
      "Epoch 810/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8337\n",
      "Epoch 811/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7550\n",
      "Epoch 812/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7197\n",
      "Epoch 813/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7403\n",
      "Epoch 814/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.7576\n",
      "Epoch 815/1000\n",
      "2157/2157 [==============================] - 0s 15us/step - loss: 6.8764\n",
      "Epoch 816/1000\n",
      "2157/2157 [==============================] - 0s 16us/step - loss: 7.2501\n",
      "Epoch 817/1000\n",
      "2157/2157 [==============================] - 0s 15us/step - loss: 7.2650\n",
      "Epoch 818/1000\n",
      "2157/2157 [==============================] - 0s 16us/step - loss: 6.8274\n",
      "Epoch 819/1000\n",
      "2157/2157 [==============================] - 0s 17us/step - loss: 6.6949\n",
      "Epoch 820/1000\n",
      "2157/2157 [==============================] - 0s 16us/step - loss: 6.8343\n",
      "Epoch 821/1000\n",
      "2157/2157 [==============================] - 0s 16us/step - loss: 6.7740\n",
      "Epoch 822/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.6963\n",
      "Epoch 823/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.6971\n",
      "Epoch 824/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.7368\n",
      "Epoch 825/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 6.8296\n",
      "Epoch 826/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 6.7174\n",
      "Epoch 827/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 7.0181\n",
      "Epoch 828/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8070\n",
      "Epoch 829/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.7242\n",
      "Epoch 830/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 6.9236\n",
      "Epoch 831/1000\n",
      "2157/2157 [==============================] - 0s 15us/step - loss: 6.8124\n",
      "Epoch 832/1000\n",
      "2157/2157 [==============================] - 0s 15us/step - loss: 6.7354\n",
      "Epoch 833/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 6.7439\n",
      "Epoch 834/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.7575\n",
      "Epoch 835/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.8176\n",
      "Epoch 836/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.6723\n",
      "Epoch 837/1000\n",
      "2157/2157 [==============================] - 0s 15us/step - loss: 6.8042\n",
      "Epoch 838/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 6.8326\n",
      "Epoch 839/1000\n",
      "2157/2157 [==============================] - 0s 15us/step - loss: 6.8064\n",
      "Epoch 840/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 6.7193\n",
      "Epoch 841/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.8415\n",
      "Epoch 842/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 6.6800\n",
      "Epoch 843/1000\n",
      "2157/2157 [==============================] - 0s 17us/step - loss: 6.7070\n",
      "Epoch 844/1000\n",
      "2157/2157 [==============================] - 0s 18us/step - loss: 6.8803\n",
      "Epoch 845/1000\n",
      "2157/2157 [==============================] - 0s 18us/step - loss: 6.7972\n",
      "Epoch 846/1000\n",
      "2157/2157 [==============================] - 0s 15us/step - loss: 6.8279\n",
      "Epoch 847/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 6.7350\n",
      "Epoch 848/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 6.6678\n",
      "Epoch 849/1000\n",
      "2157/2157 [==============================] - 0s 15us/step - loss: 6.6315\n",
      "Epoch 850/1000\n",
      "2157/2157 [==============================] - 0s 15us/step - loss: 6.7152\n",
      "Epoch 851/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.8321\n",
      "Epoch 852/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.0388\n",
      "Epoch 853/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.8527\n",
      "Epoch 854/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.8968\n",
      "Epoch 855/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 6.9609\n",
      "Epoch 856/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.9535\n",
      "Epoch 857/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.9183\n",
      "Epoch 858/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.9088\n",
      "Epoch 859/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.6309\n",
      "Epoch 860/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.7932\n",
      "Epoch 861/1000\n",
      "2157/2157 [==============================] - 0s 15us/step - loss: 6.9170\n",
      "Epoch 862/1000\n",
      "2157/2157 [==============================] - 0s 15us/step - loss: 6.7483\n",
      "Epoch 863/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 6.9425\n",
      "Epoch 864/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 7.0277\n",
      "Epoch 865/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.0065\n",
      "Epoch 866/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.8497\n",
      "Epoch 867/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 6.9153\n",
      "Epoch 868/1000\n",
      "2157/2157 [==============================] - 0s 17us/step - loss: 6.9244\n",
      "Epoch 869/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.7675\n",
      "Epoch 870/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9167\n",
      "Epoch 871/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0154\n",
      "Epoch 872/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.7967\n",
      "Epoch 873/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9805\n",
      "Epoch 874/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.0636\n",
      "Epoch 875/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.8837\n",
      "Epoch 876/1000\n",
      "2157/2157 [==============================] - 0s 14us/step - loss: 6.9497\n",
      "Epoch 877/1000\n",
      "2157/2157 [==============================] - 0s 16us/step - loss: 6.7581\n",
      "Epoch 878/1000\n",
      "2157/2157 [==============================] - 0s 16us/step - loss: 6.8463\n",
      "Epoch 879/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.1291\n",
      "Epoch 880/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.1091\n",
      "Epoch 881/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8785\n",
      "Epoch 882/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9319\n",
      "Epoch 883/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8266\n",
      "Epoch 884/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 6.8740\n",
      "Epoch 885/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8835\n",
      "Epoch 886/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 7.0746\n",
      "Epoch 887/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.7655\n",
      "Epoch 888/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.7213\n",
      "Epoch 889/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.7279\n",
      "Epoch 890/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9062\n",
      "Epoch 891/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.2193\n",
      "Epoch 892/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9137\n",
      "Epoch 893/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 7.0096\n",
      "Epoch 894/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 6.8278\n",
      "Epoch 895/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 6.7231\n",
      "Epoch 896/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.6425\n",
      "Epoch 897/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.7494\n",
      "Epoch 898/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.6962\n",
      "Epoch 899/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.6764\n",
      "Epoch 900/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.6533\n",
      "Epoch 901/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8546\n",
      "Epoch 902/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.8683\n",
      "Epoch 903/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.8258\n",
      "Epoch 904/1000\n",
      "2157/2157 [==============================] - 0s 15us/step - loss: 6.8188\n",
      "Epoch 905/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 6.7866\n",
      "Epoch 906/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9969\n",
      "Epoch 907/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.0343\n",
      "Epoch 908/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8505\n",
      "Epoch 909/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 6.9846\n",
      "Epoch 910/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0473\n",
      "Epoch 911/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0486\n",
      "Epoch 912/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.9257\n",
      "Epoch 913/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8284\n",
      "Epoch 914/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7633\n",
      "Epoch 915/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.6983\n",
      "Epoch 916/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.7173\n",
      "Epoch 917/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.6960\n",
      "Epoch 918/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.6417\n",
      "Epoch 919/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8665\n",
      "Epoch 920/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.7574\n",
      "Epoch 921/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.7592\n",
      "Epoch 922/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.9052\n",
      "Epoch 923/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8106\n",
      "Epoch 924/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8122\n",
      "Epoch 925/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.7897\n",
      "Epoch 926/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7386\n",
      "Epoch 927/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.7876\n",
      "Epoch 928/1000\n",
      "2157/2157 [==============================] - 0s 13us/step - loss: 7.1358\n",
      "Epoch 929/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7435\n",
      "Epoch 930/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.6871\n",
      "Epoch 931/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8081\n",
      "Epoch 932/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8672\n",
      "Epoch 933/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8017\n",
      "Epoch 934/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8001\n",
      "Epoch 935/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8981\n",
      "Epoch 936/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8095\n",
      "Epoch 937/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0173\n",
      "Epoch 938/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8829\n",
      "Epoch 939/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8288\n",
      "Epoch 940/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 7.0405\n",
      "Epoch 941/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.7879\n",
      "Epoch 942/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.6577\n",
      "Epoch 943/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8585\n",
      "Epoch 944/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.4141\n",
      "Epoch 945/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8133\n",
      "Epoch 946/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.6891\n",
      "Epoch 947/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.6609\n",
      "Epoch 948/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.6680\n",
      "Epoch 949/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.7155\n",
      "Epoch 950/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8164\n",
      "Epoch 951/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9821\n",
      "Epoch 952/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.7957\n",
      "Epoch 953/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8015\n",
      "Epoch 954/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8428\n",
      "Epoch 955/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7468\n",
      "Epoch 956/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8380\n",
      "Epoch 957/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 6.8598\n",
      "Epoch 958/1000\n",
      "2157/2157 [==============================] - 0s 8us/step - loss: 6.6907\n",
      "Epoch 959/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.7607\n",
      "Epoch 960/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9420\n",
      "Epoch 961/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.7573\n",
      "Epoch 962/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8545\n",
      "Epoch 963/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.1471\n",
      "Epoch 964/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7685\n",
      "Epoch 965/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8350\n",
      "Epoch 966/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8489\n",
      "Epoch 967/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8623\n",
      "Epoch 968/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8219\n",
      "Epoch 969/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 7.0018\n",
      "Epoch 970/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.7712\n",
      "Epoch 971/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.8242\n",
      "Epoch 972/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.7738\n",
      "Epoch 973/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.8116\n",
      "Epoch 974/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8901\n",
      "Epoch 975/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8871\n",
      "Epoch 976/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.6642\n",
      "Epoch 977/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.8276\n",
      "Epoch 978/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.8146\n",
      "Epoch 979/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9059\n",
      "Epoch 980/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 7.0260\n",
      "Epoch 981/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.9343\n",
      "Epoch 982/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9801\n",
      "Epoch 983/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.5165\n",
      "Epoch 984/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 7.2722\n",
      "Epoch 985/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8812\n",
      "Epoch 986/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.6469\n",
      "Epoch 987/1000\n",
      "2157/2157 [==============================] - 0s 12us/step - loss: 6.7726\n",
      "Epoch 988/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7650\n",
      "Epoch 989/1000\n",
      "2157/2157 [==============================] - 0s 11us/step - loss: 6.9222\n",
      "Epoch 990/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7030\n",
      "Epoch 991/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.6973\n",
      "Epoch 992/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8001\n",
      "Epoch 993/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.6642\n",
      "Epoch 994/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.9120\n",
      "Epoch 995/1000\n",
      "2157/2157 [==============================] - 0s 9us/step - loss: 6.8042\n",
      "Epoch 996/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8064\n",
      "Epoch 997/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.8077\n",
      "Epoch 998/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7507\n",
      "Epoch 999/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7233\n",
      "Epoch 1000/1000\n",
      "2157/2157 [==============================] - 0s 10us/step - loss: 6.7335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2913b828>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.array(X_train), np.array(y_train), epochs=1000,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "925"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing.columns=['NN_predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing['actual'] = y_test.reset_index()['points']\n",
    "testing['GBR_predictions']=gbr_predictions\n",
    "testing['LY_scoring']=X_test['points_ly'].reset_index()['points_ly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NN_predictions</th>\n",
       "      <th>actual</th>\n",
       "      <th>GBR_predictions</th>\n",
       "      <th>LY_scoring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.844143</td>\n",
       "      <td>19.6</td>\n",
       "      <td>17.807397</td>\n",
       "      <td>17.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.583460</td>\n",
       "      <td>5.5</td>\n",
       "      <td>3.567365</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.279707</td>\n",
       "      <td>12.6</td>\n",
       "      <td>10.218032</td>\n",
       "      <td>7.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.779441</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.222972</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.738732</td>\n",
       "      <td>16.1</td>\n",
       "      <td>12.001045</td>\n",
       "      <td>9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.897798</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4.102811</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20.338715</td>\n",
       "      <td>17.5</td>\n",
       "      <td>18.313618</td>\n",
       "      <td>18.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.509632</td>\n",
       "      <td>4.6</td>\n",
       "      <td>5.172164</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11.425447</td>\n",
       "      <td>8.5</td>\n",
       "      <td>10.248823</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13.024121</td>\n",
       "      <td>12.3</td>\n",
       "      <td>13.196832</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.423476</td>\n",
       "      <td>10.6</td>\n",
       "      <td>5.194911</td>\n",
       "      <td>6.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.667042</td>\n",
       "      <td>6.4</td>\n",
       "      <td>6.217672</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.136390</td>\n",
       "      <td>12.9</td>\n",
       "      <td>11.728652</td>\n",
       "      <td>12.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.299977</td>\n",
       "      <td>6.3</td>\n",
       "      <td>4.503862</td>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12.733069</td>\n",
       "      <td>16.8</td>\n",
       "      <td>14.685294</td>\n",
       "      <td>12.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12.993856</td>\n",
       "      <td>14.6</td>\n",
       "      <td>13.173897</td>\n",
       "      <td>12.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.785801</td>\n",
       "      <td>21.8</td>\n",
       "      <td>16.863571</td>\n",
       "      <td>17.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.778644</td>\n",
       "      <td>3.3</td>\n",
       "      <td>4.021012</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.841348</td>\n",
       "      <td>8.3</td>\n",
       "      <td>8.508100</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>16.594303</td>\n",
       "      <td>18.2</td>\n",
       "      <td>17.419960</td>\n",
       "      <td>19.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>14.008435</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.803461</td>\n",
       "      <td>13.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7.738123</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.871389</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6.538486</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.869240</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>25.786545</td>\n",
       "      <td>25.3</td>\n",
       "      <td>24.482965</td>\n",
       "      <td>27.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10.681409</td>\n",
       "      <td>12.5</td>\n",
       "      <td>11.420400</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6.938096</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.175035</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6.099506</td>\n",
       "      <td>9.7</td>\n",
       "      <td>9.191550</td>\n",
       "      <td>11.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.789058</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.339868</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>16.489565</td>\n",
       "      <td>16.6</td>\n",
       "      <td>15.815726</td>\n",
       "      <td>17.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>11.266147</td>\n",
       "      <td>11.8</td>\n",
       "      <td>10.420622</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>15.764203</td>\n",
       "      <td>11.5</td>\n",
       "      <td>14.586147</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>7.571964</td>\n",
       "      <td>7.7</td>\n",
       "      <td>8.336951</td>\n",
       "      <td>8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>9.329335</td>\n",
       "      <td>8.2</td>\n",
       "      <td>10.578852</td>\n",
       "      <td>12.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>9.455623</td>\n",
       "      <td>10.1</td>\n",
       "      <td>10.282281</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>10.583059</td>\n",
       "      <td>6.4</td>\n",
       "      <td>10.585843</td>\n",
       "      <td>11.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>4.291042</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.354959</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>5.480347</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.247901</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>6.538922</td>\n",
       "      <td>7.7</td>\n",
       "      <td>5.599776</td>\n",
       "      <td>6.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>4.418157</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.214471</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>9.786401</td>\n",
       "      <td>9.2</td>\n",
       "      <td>10.905218</td>\n",
       "      <td>11.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>6.758137</td>\n",
       "      <td>5.1</td>\n",
       "      <td>5.448385</td>\n",
       "      <td>5.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>14.200366</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.478662</td>\n",
       "      <td>16.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>17.505615</td>\n",
       "      <td>18.1</td>\n",
       "      <td>16.566495</td>\n",
       "      <td>17.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>3.347314</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.942152</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>24.372265</td>\n",
       "      <td>27.8</td>\n",
       "      <td>23.924801</td>\n",
       "      <td>26.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>15.915799</td>\n",
       "      <td>11.3</td>\n",
       "      <td>13.459362</td>\n",
       "      <td>17.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>12.603471</td>\n",
       "      <td>12.7</td>\n",
       "      <td>11.659221</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>16.698238</td>\n",
       "      <td>14.8</td>\n",
       "      <td>15.357630</td>\n",
       "      <td>12.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>2.924151</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.615996</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>10.952954</td>\n",
       "      <td>14.3</td>\n",
       "      <td>13.284978</td>\n",
       "      <td>14.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>4.519367</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3.935196</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>11.621898</td>\n",
       "      <td>10.1</td>\n",
       "      <td>11.930927</td>\n",
       "      <td>13.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>4.391263</td>\n",
       "      <td>5.7</td>\n",
       "      <td>4.243671</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>12.723500</td>\n",
       "      <td>10.4</td>\n",
       "      <td>11.235180</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>17.030533</td>\n",
       "      <td>21.1</td>\n",
       "      <td>18.624130</td>\n",
       "      <td>20.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>9.883333</td>\n",
       "      <td>10.6</td>\n",
       "      <td>8.518753</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>6.282350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.493585</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>9.015827</td>\n",
       "      <td>8.3</td>\n",
       "      <td>6.751289</td>\n",
       "      <td>5.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>9.712861</td>\n",
       "      <td>10.1</td>\n",
       "      <td>11.639393</td>\n",
       "      <td>11.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>8.341134</td>\n",
       "      <td>10.8</td>\n",
       "      <td>7.852629</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>925 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     NN_predictions  actual  GBR_predictions  LY_scoring\n",
       "0         17.844143    19.6        17.807397        17.8\n",
       "1          4.583460     5.5         3.567365         2.9\n",
       "2         10.279707    12.6        10.218032         7.3\n",
       "3          4.779441     5.3         5.222972         3.1\n",
       "4         11.738732    16.1        12.001045         9.4\n",
       "5          4.897798     3.8         4.102811         4.7\n",
       "6         20.338715    17.5        18.313618        18.8\n",
       "7          5.509632     4.6         5.172164         5.9\n",
       "8         11.425447     8.5        10.248823        11.0\n",
       "9         13.024121    12.3        13.196832        14.0\n",
       "10         4.423476    10.6         5.194911         6.3\n",
       "11         5.667042     6.4         6.217672         7.5\n",
       "12        13.136390    12.9        11.728652        12.8\n",
       "13         3.299977     6.3         4.503862         5.9\n",
       "14        12.733069    16.8        14.685294        12.2\n",
       "15        12.993856    14.6        13.173897        12.7\n",
       "16        17.785801    21.8        16.863571        17.8\n",
       "17         3.778644     3.3         4.021012         2.2\n",
       "18         7.841348     8.3         8.508100         7.2\n",
       "19        16.594303    18.2        17.419960        19.5\n",
       "20        14.008435    17.0        13.803461        13.7\n",
       "21         7.738123    12.0         8.871389         9.5\n",
       "22         6.538486     4.0         5.869240         7.1\n",
       "23        25.786545    25.3        24.482965        27.1\n",
       "24        10.681409    12.5        11.420400        11.9\n",
       "25         6.938096    10.0         7.175035         8.1\n",
       "26         6.099506     9.7         9.191550        11.4\n",
       "27         2.789058     2.4         2.339868         2.2\n",
       "28        16.489565    16.6        15.815726        17.4\n",
       "29        11.266147    11.8        10.420622        11.0\n",
       "..              ...     ...              ...         ...\n",
       "895       15.764203    11.5        14.586147        15.0\n",
       "896        7.571964     7.7         8.336951         8.7\n",
       "897        9.329335     8.2        10.578852        12.2\n",
       "898        9.455623    10.1        10.282281         6.4\n",
       "899       10.583059     6.4        10.585843        11.5\n",
       "900        4.291042     1.5         3.354959         1.8\n",
       "901        5.480347     3.4         5.247901         4.5\n",
       "902        6.538922     7.7         5.599776         6.2\n",
       "903        4.418157     4.5         4.214471         3.3\n",
       "904        9.786401     9.2        10.905218        11.4\n",
       "905        6.758137     5.1         5.448385         5.2\n",
       "906       14.200366    15.0        15.478662        16.3\n",
       "907       17.505615    18.1        16.566495        17.1\n",
       "908        3.347314     2.7         3.942152         3.3\n",
       "909       24.372265    27.8        23.924801        26.9\n",
       "910       15.915799    11.3        13.459362        17.5\n",
       "911       12.603471    12.7        11.659221        12.0\n",
       "912       16.698238    14.8        15.357630        12.8\n",
       "913        2.924151     1.5         1.615996         2.4\n",
       "914       10.952954    14.3        13.284978        14.2\n",
       "915        4.519367     1.8         3.935196         2.3\n",
       "916       11.621898    10.1        11.930927        13.5\n",
       "917        4.391263     5.7         4.243671         3.4\n",
       "918       12.723500    10.4        11.235180        10.0\n",
       "919       17.030533    21.1        18.624130        20.4\n",
       "920        9.883333    10.6         8.518753         9.1\n",
       "921        6.282350     0.0         5.493585         2.8\n",
       "922        9.015827     8.3         6.751289         5.7\n",
       "923        9.712861    10.1        11.639393        11.5\n",
       "924        8.341134    10.8         7.852629         9.2\n",
       "\n",
       "[925 rows x 4 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1019    17.8\n",
       "3307     2.9\n",
       "3489     7.3\n",
       "3242     3.1\n",
       "1007     9.4\n",
       "2765     4.7\n",
       "2357    18.8\n",
       "3002     5.9\n",
       "2866    11.0\n",
       "2709    14.0\n",
       "3055     6.3\n",
       "1366     7.5\n",
       "3278    12.8\n",
       "3452     5.9\n",
       "1985    12.2\n",
       "2903    12.7\n",
       "715     17.8\n",
       "754      2.2\n",
       "3450     7.2\n",
       "1322    19.5\n",
       "Name: points_ly, dtype: float64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test['points_ly'][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17.844143 ],\n",
       "       [ 4.58346  ],\n",
       "       [10.279707 ],\n",
       "       [ 4.779441 ],\n",
       "       [11.738732 ],\n",
       "       [ 4.8977976],\n",
       "       [20.338715 ],\n",
       "       [ 5.509632 ],\n",
       "       [11.4254465],\n",
       "       [13.024121 ],\n",
       "       [ 4.4234757],\n",
       "       [ 5.667042 ],\n",
       "       [13.13639  ],\n",
       "       [ 3.2999768],\n",
       "       [12.733069 ],\n",
       "       [12.993856 ],\n",
       "       [17.7858   ],\n",
       "       [ 3.7786436],\n",
       "       [ 7.8413477],\n",
       "       [16.594303 ]], dtype=float32)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(learning_rate=0.01,n_estimators=1000,max_depth = 3,max_features=0.5)\n",
    "gbr.fit(X_train,y_train)\n",
    "gbr_predictions = gbr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     17.877944\n",
       "1      3.451549\n",
       "2     10.258897\n",
       "3      5.205964\n",
       "4     11.953578\n",
       "5      4.218924\n",
       "6     18.235270\n",
       "7      5.317524\n",
       "8     10.236510\n",
       "9     13.238273\n",
       "10     5.274126\n",
       "11     6.293032\n",
       "12    11.519619\n",
       "13     4.620900\n",
       "14    14.934067\n",
       "15    13.272919\n",
       "16    16.929187\n",
       "17     4.078159\n",
       "18     8.473781\n",
       "19    17.592289\n",
       "dtype: float64"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
